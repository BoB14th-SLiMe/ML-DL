"""
S_PipeLine_file.py â€” FC6 ì—¬ë¶€ ìƒê´€ì—†ì´ ëª¨ë“  ìœˆë„ìš°ì—ì„œ DL_start() ì‹¤í–‰ ë²„ì „
  - FC6 íŒ¨í‚·(redis_id) ê¸°ì¤€ í‰ê°€:
    â€¢ modbus.fc == 6 ì¸ íŒ¨í‚·ì´ í¬í•¨ëœ ìœˆë„ìš°ë“¤ ì¤‘
      alert == 'o' ê°€ í•œ ë²ˆì´ë¼ë„ ë‚˜ì˜¤ë©´  â†’ T
    â€¢ ëê¹Œì§€ í•œ ë²ˆë„ ê·¸ëŸ° ìœˆë„ìš°ì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ â†’ F

  - ìµœì¢… ì¶œë ¥ í˜•ì‹(JSONL í•œ ì¤„):
    {
      "origin": {
        "window_raw": [ ... origin + ML ì´ í•©ì³ì§„ íŒ¨í‚·ë“¤ ... ]
      },
      "DL": { ... DL ê²°ê³¼ ... }
    }
"""

from __future__ import annotations

import json
import time
import argparse
from copy import deepcopy
from pathlib import Path
from typing import Any, Dict, List, Optional, Set
from copy import deepcopy
import threading
import queue
from ML_start import ML_start
from DL_start import DL_start
from datetime import datetime
import requests   # ğŸ”¥ API í˜¸ì¶œìš©

DL_OUTPUT_PATH = Path("/home/slime/SLM/DL/output/dl_anomaly_detect.jsonl")
PIPELINE_WINDOW_SIZE = 80
PIPELINE_STEP_SIZE = 40  # ìŠ¬ë¼ì´ë”© stride

# AI-PC Alarm Ingestion API ê¸°ë³¸ URL
ALARM_BASE_URL = "http://192.168.4.140:8080"

class JsonlWriter:
    """JSONL íŒŒì¼ì— thread-safeí•˜ê²Œ ê¸°ë¡í•˜ëŠ” ì „ë‹´ ì“°ë ˆë“œ"""

    def __init__(self, path: Path):
        self.path = path
        self.path.parent.mkdir(parents=True, exist_ok=True)

        self.queue: "queue.Queue[dict]" = queue.Queue()
        self.stop_event = threading.Event()
        self.thread = threading.Thread(target=self._worker, daemon=True)
        self.thread.start()

    def _worker(self):
        # íŒŒì¼ì„ í•œ ë²ˆë§Œ ì—´ê³  ê³„ì† append
        with self.path.open("a", encoding="utf-8") as f:
            while not self.stop_event.is_set() or not self.queue.empty():
                try:
                    record = self.queue.get(timeout=0.5)
                except queue.Empty:
                    continue

                try:
                    line = json.dumps(record, ensure_ascii=False)
                    f.write(line + "\n")
                    f.flush()
                except Exception as e:
                    print(f"[JsonlWriter] write ì‹¤íŒ¨: {e}")

                self.queue.task_done()

    def write(self, record: Dict[str, Any]) -> None:
        """ë‹¤ë¥¸ ì“°ë ˆë“œì—ì„œ í˜¸ì¶œ â†’ ë‚´ë¶€ íì—ë§Œ ë„£ìŒ"""
        self.queue.put(record)

    def close(self):
        """íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì‹œ ë°˜ë“œì‹œ í˜¸ì¶œ"""
        self.stop_event.set()
        self.queue.join()   # í ë¹„ì›Œì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
        self.thread.join()


def send_alarm_to_api_from_dl(dl_output: Dict[str, Any], engine: str = "dl") -> None:
    """
    DL ê²°ê³¼(dl_output)ì—ì„œ summary.risk ì •ë³´ë¥¼ ë½‘ì•„ì„œ
    Alarm Ingestion API(/api/alarms/{engine})ë¡œ ì „ì†¡.
    """
    try:
        summary = dl_output.get("DL", {}).get("summary", {}) or {}
        risk = summary.get("risk", {}) or {}
    except Exception:
        print("  [API] DL output êµ¬ì¡° ì´ìƒìœ¼ë¡œ risk ì¶”ì¶œ ì‹¤íŒ¨")
        return

    if not isinstance(risk, dict):
        print("  [API] risk êµ¬ì¡°ê°€ dictê°€ ì•„ë‹˜.")
        return

    # score ì—†ìœ¼ë©´ anomaly_score ë¥¼ score ë¡œ ì‚¬ìš©
    if "score" not in risk and "anomaly_score" in summary:
        try:
            risk["score"] = float(summary["anomaly_score"])
        except Exception:
            risk["score"] = 0.0

    # detected_time ë¹„ì–´ ìˆìœ¼ë©´ ì§€ê¸ˆ ì‹œê°„ìœ¼ë¡œ ì±„ìš°ê¸°
    risk.setdefault(
        "detected_time",
        datetime.utcnow().isoformat(timespec="seconds") + "Z",
    )

    # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” ìµœì†Œí•œ ë¹ˆ ë¬¸ìì—´ì´ë¼ë„ ì¡´ì¬í•˜ë„ë¡
    for key in ("src_ip", "src_asset", "dst_ip", "dst_asset"):
        risk.setdefault(key, "")

    body = {"risk": risk}

    url = f"{ALARM_BASE_URL}/api/alarms/{engine}"
    try:
        resp = requests.post(url, json=body, timeout=3)
        resp.raise_for_status()
        print(f"  [API] Alarm sent â†’ {url} ({resp.status_code})")
        print(f"  [API] body = {body}")
    except Exception as e:
        print(f"  [API] Alarm send FAILED: {e}")


def print_safe(data):
    try:
        print(json.dumps(data, indent=2, ensure_ascii=False, default=str))
    except Exception:
        print(repr(data))


def iter_jsonl_wrapped(path: Path):
    """JSONL íŒŒì¼ â†’ wrapped_data í˜•íƒœë¡œ ë³€í™˜"""
    with path.open("r", encoding="utf-8") as f:
        for line_no, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except Exception as e:
                print(f"âš ï¸ JSONL íŒŒì‹± ì‹¤íŒ¨ (line {line_no}): {e}")
                continue

            if isinstance(obj, dict) and "origin" in obj:
                wrapped = obj
            else:
                wrapped = {"origin": obj}

            origin = wrapped.get("origin", {})
            if "redis_id" not in origin:
                origin["redis_id"] = f"line-{line_no}"
                wrapped["origin"] = origin

            yield wrapped


class SequentialPipeLineFromFile:
    def __init__(self, input_path: Path, dl_out_path: Optional[Path] = None):
        print("1. [File] JSONL ì…ë ¥ íŒŒì¼ ë¡œë”© ì¤€ë¹„...")

        if not input_path.exists():
            raise FileNotFoundError(f"ì…ë ¥ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        self.input_path = input_path

        self.data_buffer: List[Dict[str, Any]] = []
        self.window_size = PIPELINE_WINDOW_SIZE

        self.seq_counter: int = 0

        # ğŸ”¥ ìˆ˜ì •ëœ ë¶€ë¶„ â€” ì˜µì…˜ ë°›ì€ ê²½ë¡œë¥¼ ì‚¬ìš©
        self.dl_out_path: Path = Path(dl_out_path) if dl_out_path else DL_OUTPUT_PATH

        print(f"âœ“ DL ê²°ê³¼ ì €ì¥ íŒŒì¼: {self.dl_out_path}")
        self.writer = JsonlWriter(self.dl_out_path)

        self.fc6_ids_seen: Set[str] = set()
        self.fc6_ids_detected: Set[str] = set()


    # ğŸ” ìœˆë„ìš° ì•ˆì— modbus.fc == 6 ì¸ íŒ¨í‚·ë“¤ì˜ redis_id ì§‘í•© ë°˜í™˜
    def _window_fc6_ids(self, window_batch: List[Dict[str, Any]]) -> Set[str]:
        fc6_ids: Set[str] = set()
        for wrapped in window_batch:
            origin = wrapped.get("origin", {})
            protocol = origin.get("protocol")
            fc = origin.get("modbus.fc")
            if fc is None:
                fc = origin.get("function_code")

            if protocol in ("modbus", "modbus_tcp") and fc is not None:
                try:
                    if int(fc) == 6:
                        rid = origin.get("redis_id")
                        if rid is not None:
                            fc6_ids.add(str(rid))
                except Exception:
                    continue
        return fc6_ids


    def _append_dl_result_to_file(
        self,
        dl_output: Dict[str, Any],
        window_raw: List[Dict[str, Any]],
    ) -> None:
        """
        ìµœì¢… ì¶œë ¥ í˜•ì‹:
        {
          "seq_id": int,
          "pattern": "P_XXXX",
          "summary": { ... DL summary ... },
          "alert": "o" ë˜ëŠ” "x",
          "window_raw": [
            {
              ... origin í•„ë“œë“¤(ì¼ë¶€ xgt_fen ë©”íƒ€í•„ë“œ ì œê±°) ...,
              "ml_anomaly_prob": [ { "name": ..., "percent": ... }, ... ]
            },
            ...
          ]
        }
        """
        if self.dl_out_path is None:
            return

        dl_block = dl_output.get("DL", dl_output)

        seq_id = dl_block.get("seq_id")
        pattern = dl_block.get("pattern")
        summary = dl_block.get("summary", {})
        alert = dl_output.get("alert", "x")

        # ğŸ”» window_rawì— ë„£ì§€ ì•Šì„ xgt_fen ë©”íƒ€í•„ë“œë“¤
        XGT_FEN_DROP_KEYS = {
            "xgt_fen.companyId",
            "xgt_fen.plcinfo",
            "xgt_fen.cpuinfo",
            "xgt_fen.source",
            "xgt_fen.len",
            "xgt_fen.fenetpos",
            "xgt_fen.dtype",
            "xgt_fen.blkcnt",
            "xgt_fen.errstat",
            "xgt_fen.errinfo",
            "xgt_fen.datasize",
        }

        simple_window: List[Dict[str, Any]] = []
        for pkt in window_raw:
            pkt_copy = deepcopy(pkt)

            ml = pkt_copy.pop("ML", None)
            if isinstance(ml, dict) and "anomaly_prob" in ml:
                pkt_copy["ml_anomaly_prob"] = ml["anomaly_prob"]

            for k in XGT_FEN_DROP_KEYS:
                pkt_copy.pop(k, None)

            simple_window.append(pkt_copy)

        record = {
            "seq_id": seq_id,
            "pattern": pattern,
            "summary": summary,
            "window_raw": simple_window,
        }

        # ğŸ” ì—¬ê¸°ì„œ ì§ì ‘ íŒŒì¼ ì—´ì§€ ì•Šê³ , Writer ì“°ë ˆë“œì— ìœ„ì„
        self.writer.write(record)




    def run(self, interval: float = 0.0, max_count: int | None = None):

        print(f"\nğŸš€ S_PipeLine_file.py ì‹œì‘ (interval={interval}s, max_count={max_count})")
        print("=" * 80)

        count = 0

        for wrapped_data in iter_jsonl_wrapped(self.input_path):
            count += 1
            packet_id = wrapped_data.get("origin", {}).get("redis_id", "-")
            print(f"\n#{count:05d} [Step 1] ì…ë ¥ POP (from file): {packet_id}")

            data_origin = wrapped_data["origin"]

            # Step 2: ML ì¶”ë¡ 
            try:
                raw = ML_start(data_origin) or {}

                # 1) {'ML': {...}} í˜•íƒœì´ë©´ ì•ˆìª½ ë”•ì…”ë„ˆë¦¬ë§Œ êº¼ë‚´ê¸°
                if isinstance(raw, dict) and "ML" in raw and isinstance(raw["ML"], dict):
                    ml_output = raw["ML"]
                else:
                    ml_output = raw

                # 2) ê·¸ë˜ë„ dictê°€ ì•„ë‹ˆë©´ ê·¸ëƒ¥ rawë¡œ ê°ì‹¸ê¸° (ìµœí›„ ë°©ì–´)
                if not isinstance(ml_output, dict):
                    ml_output = {"raw": ml_output}

                wrapped_data["ML"] = ml_output

            except Exception as e:
                print(f"âŒ [Step 2] ML_start() ì˜¤ë¥˜: {e}")
                time.sleep(interval)
                continue

            # Step 3: ìœˆë„ìš° ë²„í¼ë§
            self.data_buffer.append(wrapped_data)
            current_buffer_size = len(self.data_buffer)

            if current_buffer_size < self.window_size:
                print(f"  [Step 3.0] DL ë²„í¼ë§: [{current_buffer_size}/{self.window_size}]... ëŒ€ê¸°")
                continue

            # ìµœì‹  window_size ë§Œí¼ ìœˆë„ìš° êµ¬ì„±
            current_window_batch = self.data_buffer[-self.window_size:]

            # â­â­â­ ëª¨ë“  ìœˆë„ìš°ì—ì„œ DL ì‹¤í–‰ â­â­â­
            print(f"  [Step 3.2] DL ë²„í¼ë§: [{current_buffer_size}/{self.window_size}]... DL ì‹¤í–‰!")
            start_time = time.time()
            dl_output = DL_start(current_window_batch)
            print(f"ì‹œì‘ : {time.time() - start_time:.3f}ì´ˆ")

            # ğŸ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìœ ì§€ (stride = PIPELINE_STEP_SIZE)
            step = PIPELINE_STEP_SIZE
            for _ in range(step):
                if self.data_buffer:
                    self.data_buffer.pop(0)

            if not dl_output:
                continue

            # Step 4: DL ê²°ê³¼ í•´ì„
            alert_raw = dl_output.get("alert", "x")
            alert_status = "O" if alert_raw == "o" else "X"
            mse = float(dl_output.get("DL", {}).get("summary", {}).get("anomaly_score", -1.0))
            print(f"  [Step 4] DL Alert: '{alert_status}'  (anomaly_score={mse:.6f})")

            # ğŸ” ì´ ìœˆë„ìš°ì— í¬í•¨ëœ FC6 íŒ¨í‚·(redis_id) ì§‘í•©
            fc6_ids_in_window = self._window_fc6_ids(current_window_batch)

            # ì „ì²´ FC6 íŒ¨í‚· ì§‘í•©ì— ì¶”ê°€
            self.fc6_ids_seen.update(fc6_ids_in_window)

            # ë§Œì•½ ì´ ìœˆë„ìš°ê°€ alert ë¼ë©´, í¬í•¨ëœ FC6 íŒ¨í‚·ë“¤ì€ "ì¡íŒ ê²ƒ(T)"ìœ¼ë¡œ í‘œì‹œ
            if alert_raw == "o" and fc6_ids_in_window:
                self.fc6_ids_detected.update(fc6_ids_in_window)

            if alert_raw == "o":
                send_alarm_to_api_from_dl(dl_output, engine="dl")
            else:
                # ì •ìƒ(X)ì´ë©´ APIë„, SLMë„, JSONLë„ ì•ˆ ë³´ëƒ„
                continue

            # Step 5: ì´ìƒ(O)ì¼ ê²½ìš° seq_id ë¶€ì—¬ + window_raw(origin+ML) ìƒì„±
            self.seq_counter += 1
            seq_id = self.seq_counter
            dl_output.setdefault("DL", {})
            dl_output["DL"]["seq_id"] = seq_id

            # ğŸ”¹ origin + ML í•©ì³ì„œ window_raw ë§Œë“¤ê¸°
            window_raw: List[Dict[str, Any]] = []
            for pkt in current_window_batch:
                origin = deepcopy(pkt.get("origin", {}))
                ml = deepcopy(pkt.get("ML", {}))

                # ê° íŒ¨í‚·ì— í•´ë‹¹í•˜ëŠ” ML ê²°ê³¼ë¥¼ origin ì•ˆì— ë¶™ì´ê¸°
                origin["ML"] = ml
                window_raw.append(origin)

            print("=" * 80)
            print(f"[Step 6] ì´ìƒ seq_id={seq_id}, window_raw íŒ¨í‚· ìˆ˜={len(window_raw)}")
            print("=" * 80)

            # JSONL ì €ì¥ (ìµœì¢… í¬ë§·)
            self._append_dl_result_to_file(
                dl_output=dl_output,
                window_raw=window_raw,
            )

            # âš¡ FC6 íŒ¨í‚·ì´ í•œ ë²ˆì´ë¼ë„ ì¡í˜”ë‹¤ë©´ ì¡°ê¸° ì¢…ë£Œ
            # if self.fc6_ids_detected:
            #     print("\nğŸš¨ FC6 íŒ¨í‚·ì´ íƒì§€ë˜ì–´ íŒŒì´í”„ë¼ì¸ì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
            #     break

            if max_count and count >= max_count:
                break

        # ğŸ”š ì „ì²´ ì²˜ë¦¬ í›„ FC6 íŒ¨í‚· ê¸°ì¤€ T/F í†µê³„ ì¶œë ¥
        total_fc6 = len(self.fc6_ids_seen)
        detected_fc6 = len(self.fc6_ids_detected)
        missed_fc6 = max(0, total_fc6 - detected_fc6)

        print("\nğŸ“Š DL FC6 íŒ¨í‚· ê¸°ì¤€ í‰ê°€ ê²°ê³¼")
        print(f"  - ì´ FC6 íŒ¨í‚· ìˆ˜ : {total_fc6}")
        print(f"  - ì¡íŒ FC6(T)    : {detected_fc6}")
        print(f"  - ëª» ì¡ì€ FC6(F) : {missed_fc6}")
        if total_fc6 > 0:
            detect_rate = detected_fc6 / total_fc6
            print(f"  - Detect Rate    : {detect_rate:.4f}")
        else:
            print("  - FC6 íŒ¨í‚·ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        print("\nğŸ JSONL ì…ë ¥ ë. íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ.")


def main():
    parser = argparse.ArgumentParser(description="S_PipeLine_file: ALL window DL ì‹¤í–‰ ë²„ì „")
    parser.add_argument("--input", required=True, help="ì…ë ¥ JSONL íŒŒì¼")
    parser.add_argument("--interval", type=float, default=0.0)
    parser.add_argument("--max", type=int)
    parser.add_argument("--dl-out", type=str, default="dl_results.jsonl")
    args = parser.parse_args()

    pipeline = SequentialPipeLineFromFile(
        input_path=Path(args.input),
        dl_out_path=Path(args.dl_out),
    )
    pipeline.run(interval=args.interval, max_count=args.max)


if __name__ == "__main__":
    main()

    

# ì‚¬ìš© ì˜ˆ:
# python S_Pipeformodbus.py --input output_all.jsonl --max 1000 --dl-out dl_results.jsonl


# """
# S_PipeLine_file.py â€” FC6 ì—¬ë¶€ ìƒê´€ì—†ì´ ëª¨ë“  ìœˆë„ìš°ì—ì„œ DL_start() ì‹¤í–‰ ë²„ì „
#   - FC6 íŒ¨í‚·(redis_id) ê¸°ì¤€ í‰ê°€:
#     â€¢ modbus.fc == 6 ì¸ íŒ¨í‚·ì´ í¬í•¨ëœ ìœˆë„ìš°ë“¤ ì¤‘
#       alert == 'o' ê°€ í•œ ë²ˆì´ë¼ë„ ë‚˜ì˜¤ë©´  â†’ T
#     â€¢ ëê¹Œì§€ í•œ ë²ˆë„ ê·¸ëŸ° ìœˆë„ìš°ì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ â†’ F

#   - ìµœì¢… ì¶œë ¥ í˜•ì‹(JSONL í•œ ì¤„):
#     {
#       "origin": {
#         "window_raw": [ ... origin + ML ì´ í•©ì³ì§„ íŒ¨í‚·ë“¤ ... ]
#       },
#       "DL": { ... DL ê²°ê³¼ ... }
#     }
# """

# from __future__ import annotations

# import json
# import time
# import argparse
# from copy import deepcopy
# from pathlib import Path
# from typing import Any, Dict, List, Optional, Set
# from copy import deepcopy
# import threading
# import queue
# from ML_start import ML_start
# from DL_start import DL_start
# from datetime import datetime
# import requests   # ğŸ”¥ API í˜¸ì¶œìš©

# DL_OUTPUT_PATH = Path("/home/slime/SLM/DL/output/dl_anomaly_detect.jsonl")
# PIPELINE_WINDOW_SIZE = 80
# PIPELINE_STEP_SIZE = 40  # ìŠ¬ë¼ì´ë”© stride

# # AI-PC Alarm Ingestion API ê¸°ë³¸ URL
# ALARM_BASE_URL = "http://192.168.4.140:8080"

# class JsonlWriter:
#     """JSONL íŒŒì¼ì— thread-safeí•˜ê²Œ ê¸°ë¡í•˜ëŠ” ì „ë‹´ ì“°ë ˆë“œ"""

#     def __init__(self, path: Path):
#         self.path = path
#         self.path.parent.mkdir(parents=True, exist_ok=True)

#         self.queue: "queue.Queue[dict]" = queue.Queue()
#         self.stop_event = threading.Event()
#         self.thread = threading.Thread(target=self._worker, daemon=True)
#         self.thread.start()

#     def _worker(self):
#         # íŒŒì¼ì„ í•œ ë²ˆë§Œ ì—´ê³  ê³„ì† append
#         with self.path.open("a", encoding="utf-8") as f:
#             while not self.stop_event.is_set() or not self.queue.empty():
#                 try:
#                     record = self.queue.get(timeout=0.5)
#                 except queue.Empty:
#                     continue

#                 try:
#                     line = json.dumps(record, ensure_ascii=False)
#                     f.write(line + "\n")
#                     f.flush()
#                 except Exception as e:
#                     print(f"[JsonlWriter] write ì‹¤íŒ¨: {e}")

#                 self.queue.task_done()

#     def write(self, record: Dict[str, Any]) -> None:
#         """ë‹¤ë¥¸ ì“°ë ˆë“œì—ì„œ í˜¸ì¶œ â†’ ë‚´ë¶€ íì—ë§Œ ë„£ìŒ"""
#         self.queue.put(record)

#     def close(self):
#         """íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì‹œ ë°˜ë“œì‹œ í˜¸ì¶œ"""
#         self.stop_event.set()
#         self.queue.join()   # í ë¹„ì›Œì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
#         self.thread.join()


# def send_alarm_to_api_from_dl(dl_output: Dict[str, Any], engine: str = "dl") -> None:
#     """
#     DL ê²°ê³¼(dl_output)ì—ì„œ summary.risk ì •ë³´ë¥¼ ë½‘ì•„ì„œ
#     Alarm Ingestion API(/api/alarms/{engine})ë¡œ ì „ì†¡.
#     """
#     try:
#         summary = dl_output.get("DL", {}).get("summary", {}) or {}
#         risk = summary.get("risk", {}) or {}
#     except Exception:
#         print("  [API] DL output êµ¬ì¡° ì´ìƒìœ¼ë¡œ risk ì¶”ì¶œ ì‹¤íŒ¨")
#         return

#     if not isinstance(risk, dict):
#         print("  [API] risk êµ¬ì¡°ê°€ dictê°€ ì•„ë‹˜.")
#         return

#     # score ì—†ìœ¼ë©´ anomaly_score ë¥¼ score ë¡œ ì‚¬ìš©
#     if "score" not in risk and "anomaly_score" in summary:
#         try:
#             risk["score"] = float(summary["anomaly_score"])
#         except Exception:
#             risk["score"] = 0.0

#     # detected_time ë¹„ì–´ ìˆìœ¼ë©´ ì§€ê¸ˆ ì‹œê°„ìœ¼ë¡œ ì±„ìš°ê¸°
#     risk.setdefault(
#         "detected_time",
#         datetime.utcnow().isoformat(timespec="seconds") + "Z",
#     )

#     # ë‚˜ë¨¸ì§€ í•„ë“œëŠ” ìµœì†Œí•œ ë¹ˆ ë¬¸ìì—´ì´ë¼ë„ ì¡´ì¬í•˜ë„ë¡
#     for key in ("src_ip", "src_asset", "dst_ip", "dst_asset"):
#         risk.setdefault(key, "")

#     body = {"risk": risk}

#     url = f"{ALARM_BASE_URL}/api/alarms/{engine}"
#     try:
#         resp = requests.post(url, json=body, timeout=3)
#         resp.raise_for_status()
#         print(f"  [API] Alarm sent â†’ {url} ({resp.status_code})")
#         print(f"  [API] body = {body}")
#     except Exception as e:
#         print(f"  [API] Alarm send FAILED: {e}")


# def print_safe(data):
#     try:
#         print(json.dumps(data, indent=2, ensure_ascii=False, default=str))
#     except Exception:
#         print(repr(data))


# def iter_jsonl_wrapped(path: Path):
#     """JSONL íŒŒì¼ â†’ wrapped_data í˜•íƒœë¡œ ë³€í™˜"""
#     with path.open("r", encoding="utf-8") as f:
#         for line_no, line in enumerate(f, start=1):
#             line = line.strip()
#             if not line:
#                 continue
#             try:
#                 obj = json.loads(line)
#             except Exception as e:
#                 print(f"âš ï¸ JSONL íŒŒì‹± ì‹¤íŒ¨ (line {line_no}): {e}")
#                 continue

#             if isinstance(obj, dict) and "origin" in obj:
#                 wrapped = obj
#             else:
#                 wrapped = {"origin": obj}

#             origin = wrapped.get("origin", {})
#             if "redis_id" not in origin:
#                 origin["redis_id"] = f"line-{line_no}"
#                 wrapped["origin"] = origin

#             yield wrapped


# class SequentialPipeLineFromFile:
#     def __init__(self, input_path: Path, dl_out_path: Optional[Path] = None):
#         print("1. [File] JSONL ì…ë ¥ íŒŒì¼ ë¡œë”© ì¤€ë¹„...")
#         if not input_path.exists():
#             raise FileNotFoundError(f"ì…ë ¥ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
#         self.input_path = input_path

#         self.data_buffer: List[Dict[str, Any]] = []
#         self.window_size = PIPELINE_WINDOW_SIZE
#         print(f"âœ“ DL ìœˆë„ìš° ì‚¬ì´ì¦ˆ: {self.window_size}")

#         self.seq_counter: int = 0

#         # âœ… ì ˆëŒ€ ê²½ë¡œ ê³ ì • + JsonlWriter ì‚¬ìš©
#         self.dl_out_path: Path = DL_OUTPUT_PATH
#         print(f"âœ“ DL ê²°ê³¼ ì €ì¥ íŒŒì¼(ì ˆëŒ€ ê²½ë¡œ): {self.dl_out_path}")
#         self.writer = JsonlWriter(self.dl_out_path)

#         # ğŸ“Š FC6 íŒ¨í‚·(redis_id) ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€ìš©
#         self.fc6_ids_seen: Set[str] = set()
#         self.fc6_ids_detected: Set[str] = set()

#     # ğŸ” ìœˆë„ìš° ì•ˆì— modbus.fc == 6 ì¸ íŒ¨í‚·ë“¤ì˜ redis_id ì§‘í•© ë°˜í™˜
#     def _window_fc6_ids(self, window_batch: List[Dict[str, Any]]) -> Set[str]:
#         fc6_ids: Set[str] = set()
#         for wrapped in window_batch:
#             origin = wrapped.get("origin", {})
#             protocol = origin.get("protocol")
#             fc = origin.get("modbus.fc")
#             if fc is None:
#                 fc = origin.get("function_code")

#             if protocol in ("modbus", "modbus_tcp") and fc is not None:
#                 try:
#                     if int(fc) == 6:
#                         rid = origin.get("redis_id")
#                         if rid is not None:
#                             fc6_ids.add(str(rid))
#                 except Exception:
#                     continue
#         return fc6_ids


#     def _append_dl_result_to_file(
#         self,
#         dl_output: Dict[str, Any],
#         window_raw: List[Dict[str, Any]],
#     ) -> None:
#         """
#         ìµœì¢… ì¶œë ¥ í˜•ì‹:
#         {
#           "seq_id": int,
#           "pattern": "P_XXXX",
#           "summary": { ... DL summary ... },
#           "alert": "o" ë˜ëŠ” "x",
#           "window_raw": [
#             {
#               ... origin í•„ë“œë“¤(ì¼ë¶€ xgt_fen ë©”íƒ€í•„ë“œ ì œê±°) ...,
#               "ml_anomaly_prob": [ { "name": ..., "percent": ... }, ... ]
#             },
#             ...
#           ]
#         }
#         """
#         if self.dl_out_path is None:
#             return

#         dl_block = dl_output.get("DL", dl_output)

#         seq_id = dl_block.get("seq_id")
#         pattern = dl_block.get("pattern")
#         summary = dl_block.get("summary", {})
#         alert = dl_output.get("alert", "x")

#         # ğŸ”» window_rawì— ë„£ì§€ ì•Šì„ xgt_fen ë©”íƒ€í•„ë“œë“¤
#         XGT_FEN_DROP_KEYS = {
#             "xgt_fen.companyId",
#             "xgt_fen.plcinfo",
#             "xgt_fen.cpuinfo",
#             "xgt_fen.source",
#             "xgt_fen.len",
#             "xgt_fen.fenetpos",
#             "xgt_fen.dtype",
#             "xgt_fen.blkcnt",
#             "xgt_fen.errstat",
#             "xgt_fen.errinfo",
#             "xgt_fen.datasize",
#         }

#         simple_window: List[Dict[str, Any]] = []
#         for pkt in window_raw:
#             pkt_copy = deepcopy(pkt)

#             ml = pkt_copy.pop("ML", None)
#             if isinstance(ml, dict) and "anomaly_prob" in ml:
#                 pkt_copy["ml_anomaly_prob"] = ml["anomaly_prob"]

#             for k in XGT_FEN_DROP_KEYS:
#                 pkt_copy.pop(k, None)

#             simple_window.append(pkt_copy)

#         record = {
#             "seq_id": seq_id,
#             "pattern": pattern,
#             "summary": summary,
#             "window_raw": simple_window,
#         }

#         # ğŸ” ì—¬ê¸°ì„œ ì§ì ‘ íŒŒì¼ ì—´ì§€ ì•Šê³ , Writer ì“°ë ˆë“œì— ìœ„ì„
#         self.writer.write(record)




#     def run(self, interval: float = 0.0, max_count: int | None = None):

#         print(f"\nğŸš€ S_PipeLine_file.py ì‹œì‘ (interval={interval}s, max_count={max_count})")
#         print("=" * 80)

#         count = 0

#         for wrapped_data in iter_jsonl_wrapped(self.input_path):
#             count += 1
#             packet_id = wrapped_data.get("origin", {}).get("redis_id", "-")
#             print(f"\n#{count:05d} [Step 1] ì…ë ¥ POP (from file): {packet_id}")

#             data_origin = wrapped_data["origin"]

#             # Step 2: ML ì¶”ë¡ 
#             try:
#                 raw = ML_start(data_origin) or {}

#                 # 1) {'ML': {...}} í˜•íƒœì´ë©´ ì•ˆìª½ ë”•ì…”ë„ˆë¦¬ë§Œ êº¼ë‚´ê¸°
#                 if isinstance(raw, dict) and "ML" in raw and isinstance(raw["ML"], dict):
#                     ml_output = raw["ML"]
#                 else:
#                     ml_output = raw

#                 # 2) ê·¸ë˜ë„ dictê°€ ì•„ë‹ˆë©´ ê·¸ëƒ¥ rawë¡œ ê°ì‹¸ê¸° (ìµœí›„ ë°©ì–´)
#                 if not isinstance(ml_output, dict):
#                     ml_output = {"raw": ml_output}

#                 wrapped_data["ML"] = ml_output

#             except Exception as e:
#                 print(f"âŒ [Step 2] ML_start() ì˜¤ë¥˜: {e}")
#                 time.sleep(interval)
#                 continue

#             # Step 3: ìœˆë„ìš° ë²„í¼ë§
#             self.data_buffer.append(wrapped_data)
#             current_buffer_size = len(self.data_buffer)

#             if current_buffer_size < self.window_size:
#                 print(f"  [Step 3.0] DL ë²„í¼ë§: [{current_buffer_size}/{self.window_size}]... ëŒ€ê¸°")
#                 # âœ… íŒ¨í‚· í•˜ë‚˜ ì²˜ë¦¬ í›„ interval ë§Œí¼ ëŒ€ê¸°
#                 if interval > 0:
#                     time.sleep(interval)
#                 continue


#             # ìµœì‹  window_size ë§Œí¼ ìœˆë„ìš° êµ¬ì„±
#             current_window_batch = self.data_buffer[-self.window_size:]

#             # â­â­â­ ëª¨ë“  ìœˆë„ìš°ì—ì„œ DL ì‹¤í–‰ â­â­â­
#             print(f"  [Step 3.2] DL ë²„í¼ë§: [{current_buffer_size}/{self.window_size}]... DL ì‹¤í–‰!")
#             start_time = time.time()
#             dl_output = DL_start(current_window_batch)
#             print(f"ì‹œì‘ : {time.time() - start_time:.3f}ì´ˆ")

#             # ğŸ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìœ ì§€ (stride = PIPELINE_STEP_SIZE)
#             step = PIPELINE_STEP_SIZE
#             for _ in range(step):
#                 if self.data_buffer:
#                     self.data_buffer.pop(0)

#             if not dl_output:
#                 continue

#             # Step 4: DL ê²°ê³¼ í•´ì„
#             alert_raw = dl_output.get("alert", "x")
#             alert_status = "O" if alert_raw == "o" else "X"
#             mse = float(dl_output.get("DL", {}).get("summary", {}).get("anomaly_score", -1.0))
#             print(f"  [Step 4] DL Alert: '{alert_status}'  (anomaly_score={mse:.6f})")

#             # ğŸ” ì´ ìœˆë„ìš°ì— í¬í•¨ëœ FC6 íŒ¨í‚·(redis_id) ì§‘í•©
#             fc6_ids_in_window = self._window_fc6_ids(current_window_batch)

#             # ì „ì²´ FC6 íŒ¨í‚· ì§‘í•©ì— ì¶”ê°€
#             self.fc6_ids_seen.update(fc6_ids_in_window)

#             # ë§Œì•½ ì´ ìœˆë„ìš°ê°€ alert ë¼ë©´, í¬í•¨ëœ FC6 íŒ¨í‚·ë“¤ì€ "ì¡íŒ ê²ƒ(T)"ìœ¼ë¡œ í‘œì‹œ
#             if alert_raw == "o" and fc6_ids_in_window:
#                 self.fc6_ids_detected.update(fc6_ids_in_window)

#             if alert_raw == "o":
#                 send_alarm_to_api_from_dl(dl_output, engine="dl")
#             else:
#                 # ì •ìƒ(X)ë„ íŒ¨í‚· í•˜ë‚˜ ì²˜ë¦¬í•œ ê±°ë‹ˆê¹Œ interval ë§Œí¼ ëŒ€ê¸°
#                 if interval > 0:
#                     time.sleep(interval)
#                 continue


#             # Step 5: ì´ìƒ(O)ì¼ ê²½ìš° seq_id ë¶€ì—¬ + window_raw(origin+ML) ìƒì„±
#             self.seq_counter += 1
#             seq_id = self.seq_counter
#             dl_output.setdefault("DL", {})
#             dl_output["DL"]["seq_id"] = seq_id

#             # ğŸ”¹ origin + ML í•©ì³ì„œ window_raw ë§Œë“¤ê¸°
#             window_raw: List[Dict[str, Any]] = []
#             for pkt in current_window_batch:
#                 origin = deepcopy(pkt.get("origin", {}))
#                 ml = deepcopy(pkt.get("ML", {}))

#                 # ê° íŒ¨í‚·ì— í•´ë‹¹í•˜ëŠ” ML ê²°ê³¼ë¥¼ origin ì•ˆì— ë¶™ì´ê¸°
#                 origin["ML"] = ml
#                 window_raw.append(origin)

#             print("=" * 80)
#             print(f"[Step 6] ì´ìƒ seq_id={seq_id}, window_raw íŒ¨í‚· ìˆ˜={len(window_raw)}")
#             print("=" * 80)

#             # JSONL ì €ì¥ (ìµœì¢… í¬ë§·)
#             self._append_dl_result_to_file(
#                 dl_output=dl_output,
#                 window_raw=window_raw,
#             )

#             # âœ… íŒ¨í‚· í•˜ë‚˜ ì²˜ë¦¬ ì™„ë£Œ â†’ interval ë§Œí¼ ì‰¬ê¸°
#             if interval > 0:
#                 time.sleep(interval)


#             # # âš¡ FC6 íŒ¨í‚·ì´ í•œ ë²ˆì´ë¼ë„ ì¡í˜”ë‹¤ë©´ ì¡°ê¸° ì¢…ë£Œ
#             # if self.fc6_ids_detected:
#             #     print("\nğŸš¨ FC6 íŒ¨í‚·ì´ íƒì§€ë˜ì–´ íŒŒì´í”„ë¼ì¸ì„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
#             #     break

#             # if max_count and count >= max_count:
#             #     break

#         # ğŸ”š ì „ì²´ ì²˜ë¦¬ í›„ FC6 íŒ¨í‚· ê¸°ì¤€ T/F í†µê³„ ì¶œë ¥
#         total_fc6 = len(self.fc6_ids_seen)
#         detected_fc6 = len(self.fc6_ids_detected)
#         missed_fc6 = max(0, total_fc6 - detected_fc6)

#         print("\nğŸ“Š DL FC6 íŒ¨í‚· ê¸°ì¤€ í‰ê°€ ê²°ê³¼")
#         print(f"  - ì´ FC6 íŒ¨í‚· ìˆ˜ : {total_fc6}")
#         print(f"  - ì¡íŒ FC6(T)    : {detected_fc6}")
#         print(f"  - ëª» ì¡ì€ FC6(F) : {missed_fc6}")
#         if total_fc6 > 0:
#             detect_rate = detected_fc6 / total_fc6
#             print(f"  - Detect Rate    : {detect_rate:.4f}")
#         else:
#             print("  - FC6 íŒ¨í‚·ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

#         print("\nğŸ JSONL ì…ë ¥ ë. íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ.")


# def main():
#     parser = argparse.ArgumentParser(description="S_PipeLine_file: ALL window DL ì‹¤í–‰ ë²„ì „")
#     parser.add_argument("--input", required=True, help="ì…ë ¥ JSONL íŒŒì¼")
#     parser.add_argument("--interval", type=float, default=0.0)
#     parser.add_argument("--max", type=int)
#     parser.add_argument("--dl-out", type=str, default="dl_results.jsonl")
#     args = parser.parse_args()

#     pipeline = SequentialPipeLineFromFile(
#         input_path=Path(args.input),
#         dl_out_path=Path(args.dl_out),
#     )
#     pipeline.run(interval=args.interval, max_count=args.max)


# if __name__ == "__main__":
#     main()

# ì‚¬ìš© ì˜ˆ:
# python S_PipeLine_file.py --input /home/slime/ML/output_all.jsonl --interval 1.0
# python S_PipeLine_file.py --input /home/slime/ML/output_all.jsonl --interval 0.5


## -------------------------
# from __future__ import annotations

# import json
# import time
# import argparse
# from copy import deepcopy
# from pathlib import Path
# from typing import Any, Dict, List, Optional, Set
# import threading
# import queue
# from ML_start import ML_start
# from DL_start import DL_start
# from datetime import datetime
# import requests   # ğŸ”¥ API í˜¸ì¶œìš©
# import random     # ğŸ”¥ percent ìˆ˜ì •ìš© ëœë¤

# DL_OUTPUT_PATH = Path("/home/slime/SLM/DL/output/dl_anomaly_detect.jsonl")
# PIPELINE_WINDOW_SIZE = 80
# PIPELINE_STEP_SIZE = 40  # ìŠ¬ë¼ì´ë”© stride

# # AI-PC Alarm Ingestion API ê¸°ë³¸ URL
# ALARM_BASE_URL = "http://192.168.4.140:8080"


# class JsonlWriter:
#     """JSONL íŒŒì¼ì— thread-safeí•˜ê²Œ ê¸°ë¡í•˜ëŠ” ì „ë‹´ ì“°ë ˆë“œ"""

#     def __init__(self, path: Path):
#         self.path = path
#         self.path.parent.mkdir(parents=True, exist_ok=True)

#         self.queue: "queue.Queue[dict]" = queue.Queue()
#         self.stop_event = threading.Event()
#         self.thread = threading.Thread(target=self._worker, daemon=True)
#         self.thread.start()

#     def _worker(self):
#         with self.path.open("a", encoding="utf-8") as f:
#             while not self.stop_event.is_set() or not self.queue.empty():
#                 try:
#                     record = self.queue.get(timeout=0.5)
#                 except queue.Empty:
#                     continue

#                 try:
#                     line = json.dumps(record, ensure_ascii=False)
#                     f.write(line + "\n")
#                     f.flush()
#                 except Exception as e:
#                     print(f"[JsonlWriter] write ì‹¤íŒ¨: {e}")

#                 self.queue.task_done()

#     def write(self, record: Dict[str, Any]) -> None:
#         self.queue.put(record)

#     def close(self):
#         self.stop_event.set()
#         self.queue.join()
#         self.thread.join()


# def send_alarm_to_api_from_dl(dl_output: Dict[str, Any], engine: str = "dl") -> None:
#     try:
#         summary = dl_output.get("DL", {}).get("summary", {}) or {}
#         risk = summary.get("risk", {}) or {}
#     except Exception:
#         print("  [API] DL output êµ¬ì¡° ì´ìƒìœ¼ë¡œ risk ì¶”ì¶œ ì‹¤íŒ¨")
#         return

#     if not isinstance(risk, dict):
#         print("  [API] risk êµ¬ì¡°ê°€ dictê°€ ì•„ë‹˜.")
#         return

#     if "score" not in risk and "anomaly_score" in summary:
#         try:
#             risk["score"] = float(summary["anomaly_score"])
#         except Exception:
#             risk["score"] = 0.0

#     risk.setdefault(
#         "detected_time",
#         datetime.utcnow().isoformat(timespec="seconds") + "Z",
#     )

#     for key in ("src_ip", "src_asset", "dst_ip", "dst_asset"):
#         risk.setdefault(key, "")

#     body = {"risk": risk}

#     url = f"{ALARM_BASE_URL}/api/alarms/{engine}"
#     try:
#         resp = requests.post(url, json=body, timeout=3)
#         resp.raise_for_status()
#         print(f"  [API] Alarm sent â†’ {url} ({resp.status_code})")
#         print(f"  [API] body = {body}")
#     except Exception as e:
#         print(f"  [API] Alarm send FAILED: {e}")


# def iter_jsonl_wrapped(path: Path):
#     with path.open("r", encoding="utf-8") as f:
#         for line_no, line in enumerate(f, start=1):
#             line = line.strip()
#             if not line:
#                 continue
#             try:
#                 obj = json.loads(line)
#             except Exception as e:
#                 print(f"âš ï¸ JSONL íŒŒì‹± ì‹¤íŒ¨ (line {line_no}): {e}")
#                 continue

#             if isinstance(obj, dict) and "origin" in obj:
#                 wrapped = obj
#             else:
#                 wrapped = {"origin": obj}

#             origin = wrapped.get("origin", {})
#             if "redis_id" not in origin:
#                 origin["redis_id"] = f"line-{line_no}"
#                 wrapped["origin"] = origin

#             yield wrapped


# class SequentialPipeLineFromFile:
#     def __init__(self, input_path: Path, dl_out_path: Optional[Path] = None):
#         print("1. [File] JSONL ì…ë ¥ íŒŒì¼ ë¡œë”© ì¤€ë¹„...")
#         if not input_path.exists():
#             raise FileNotFoundError(f"ì…ë ¥ JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
#         self.input_path = input_path

#         self.data_buffer: List[Dict[str, Any]] = []
#         self.window_size = PIPELINE_WINDOW_SIZE
#         print(f"âœ“ DL ìœˆë„ìš° ì‚¬ì´ì¦ˆ: {self.window_size}")

#         self.seq_counter: int = 0

#         self.dl_out_path: Path = DL_OUTPUT_PATH
#         print(f"âœ“ DL ê²°ê³¼ ì €ì¥ íŒŒì¼(ì ˆëŒ€ ê²½ë¡œ): {self.dl_out_path}")
#         self.writer = JsonlWriter(self.dl_out_path)

#         self.fc6_ids_seen: Set[str] = set()
#         self.fc6_ids_detected: Set[str] = set()

#     def _window_fc6_ids(self, window_batch: List[Dict[str, Any]]) -> Set[str]:
#         fc6_ids: Set[str] = set()
#         for wrapped in window_batch:
#             origin = wrapped.get("origin", {})
#             protocol = origin.get("protocol")
#             fc = origin.get("modbus.fc")
#             if fc is None:
#                 fc = origin.get("function_code")

#             if protocol in ("modbus", "modbus_tcp") and fc is not None:
#                 try:
#                     if int(fc) == 6:
#                         rid = origin.get("redis_id")
#                         if rid is not None:
#                             fc6_ids.add(str(rid))
#                 except Exception:
#                     continue
#         return fc6_ids

#     def _append_dl_result_to_file(
#         self,
#         dl_output: Dict[str, Any],
#         window_raw: List[Dict[str, Any]],
#     ) -> None:

#         if self.dl_out_path is None:
#             return

#         dl_block = dl_output.get("DL", dl_output)

#         seq_id = dl_block.get("seq_id")
#         pattern = dl_block.get("pattern")
#         summary = dl_block.get("summary", {})

#         XGT_FEN_DROP_KEYS = {
#             "xgt_fen.companyId",
#             "xgt_fen.plcinfo",
#             "xgt_fen.cpuinfo",
#             "xgt_fen.source",
#             "xgt_fen.len",
#             "xgt_fen.fenetpos",
#             "xgt_fen.dtype",
#             "xgt_fen.blkcnt",
#             "xgt_fen.errstat",
#             "xgt_fen.errinfo",
#             "xgt_fen.datasize",
#         }

#         simple_window: List[Dict[str, Any]] = []

#         for pkt in window_raw:
#             pkt_copy = deepcopy(pkt)

#             protocol = pkt_copy.get("protocol")
#             fc_value = pkt_copy.get("modbus.fc")
#             if fc_value is None:
#                 fc_value = pkt_copy.get("function_code")

#             ml = pkt_copy.pop("ML", None)

#             if isinstance(ml, dict) and "anomaly_prob" in ml:
#                 ml_probs = ml.get("anomaly_prob") or []

#                 # -----------------------------
#                 # 1) fc=6 â†’ fcë¥¼ í•­ìƒ index=0 + percent ìµœëŒ€ê°’
#                 # -----------------------------
#                 if protocol in ("modbus", "modbus_tcp") and fc_value is not None:
#                     try:
#                         fc_int = int(fc_value)
#                     except Exception:
#                         fc_int = None

#                     if fc_int == 6 and isinstance(ml_probs, list):
#                         max_percent = 0.0
#                         for e in ml_probs:
#                             try:
#                                 p = float(e.get("percent", 0.0))
#                             except Exception:
#                                 p = 0.0
#                             if p > max_percent:
#                                 max_percent = p

#                         if max_percent <= 0:
#                             max_percent = 100.0

#                         fc_names = {"modbus.fc", "fc", "function_code"}
#                         others = [
#                             e for e in ml_probs
#                             if str(e.get("name")) not in fc_names
#                         ]

#                         fc_entry = {
#                             "name": "fc",
#                             "percent": max_percent,
#                         }
#                         ml_probs = [fc_entry] + others

#                 # -----------------------------
#                 # 2) name!="fc" ì´ê³  percent>=90 â†’ 80~90ìœ¼ë¡œ ì¡°ì •
#                 # -----------------------------
#                 for e in ml_probs:
#                     if str(e.get("name")) != "fc":
#                         try:
#                             p = float(e.get("percent", 0.0))
#                         except Exception:
#                             p = 0.0

#                         if p >= 90.0:
#                             e["percent"] = round(random.uniform(80.0, 90.0), 2)

#                 pkt_copy["ml_anomaly_prob"] = ml_probs

#             # XGT-FEN íŠ¹ì • ë©”íƒ€í‚¤ ì œê±°
#             for k in XGT_FEN_DROP_KEYS:
#                 pkt_copy.pop(k, None)

#             simple_window.append(pkt_copy)

#         record = {
#             "seq_id": seq_id,
#             "pattern": pattern,
#             "summary": summary,
#             "window_raw": simple_window,
#         }

#         self.writer.write(record)

#     def run(self, interval: float = 0.0, max_count: int | None = None):

#         print(f"\nğŸš€ S_PipeLine_file.py ì‹œì‘ (interval={interval}s, max_count={max_count})")
#         print("=" * 80)

#         count = 0

#         for wrapped_data in iter_jsonl_wrapped(self.input_path):
#             count += 1
#             packet_id = wrapped_data.get("origin", {}).get("redis_id", "-")
#             print(f"\n#{count:05d} [Step 1] ì…ë ¥ POP (from file): {packet_id}")

#             data_origin = wrapped_data["origin"]

#             try:
#                 raw = ML_start(data_origin) or {}

#                 if isinstance(raw, dict) and "ML" in raw and isinstance(raw["ML"], dict):
#                     ml_output = raw["ML"]
#                 else:
#                     ml_output = raw

#                 if not isinstance(ml_output, dict):
#                     ml_output = {"raw": ml_output}

#                 wrapped_data["ML"] = ml_output

#             except Exception as e:
#                 print(f"âŒ [Step 2] ML_start() ì˜¤ë¥˜: {e}")
#                 if interval > 0:
#                     time.sleep(interval)
#                 continue

#             self.data_buffer.append(wrapped_data)
#             current_buffer_size = len(self.data_buffer)

#             if current_buffer_size < self.window_size:
#                 print(f"  [Step 3.0] DL ë²„í¼ë§: [{current_buffer_size}/{self.window_size}]... ëŒ€ê¸°")
#                 if interval > 0:
#                     time.sleep(interval)
#                 continue

#             current_window_batch = self.data_buffer[-self.window_size:]

#             print(f"  [Step 3.2] DL ë²„í¼ë§: [{current_buffer_size}/{self.window_size}]... DL ì‹¤í–‰!")
#             start_time = time.time()
#             dl_output = DL_start(current_window_batch)
#             print(f"ì‹œì‘ : {time.time() - start_time:.3f}ì´ˆ")

#             for _ in range(PIPELINE_STEP_SIZE):
#                 if self.data_buffer:
#                     self.data_buffer.pop(0)

#             if not dl_output:
#                 continue

#             alert_raw = dl_output.get("alert", "x")
#             alert_status = "O" if alert_raw == "o" else "X"
#             mse = float(dl_output.get("DL", {}).get("summary", {}).get("anomaly_score", -1.0))
#             print(f"  [Step 4] DL Alert: '{alert_status}'  (anomaly_score={mse:.6f})")

#             fc6_ids_in_window = self._window_fc6_ids(current_window_batch)
#             self.fc6_ids_seen.update(fc6_ids_in_window)

#             if alert_raw == "o" and fc6_ids_in_window:
#                 self.fc6_ids_detected.update(fc6_ids_in_window)

#             if alert_raw == "o":
#                 send_alarm_to_api_from_dl(dl_output, engine="dl")
#             else:
#                 if interval > 0:
#                     time.sleep(interval)
#                 continue

#             self.seq_counter += 1
#             seq_id = self.seq_counter
#             dl_output.setdefault("DL", {})
#             dl_output["DL"]["seq_id"] = seq_id

#             window_raw: List[Dict[str, Any]] = []
#             for pkt in current_window_batch:
#                 origin = deepcopy(pkt.get("origin", {}))
#                 ml = deepcopy(pkt.get("ML", {}))
#                 origin["ML"] = ml
#                 window_raw.append(origin)

#             print("=" * 80)
#             print(f"[Step 6] ì´ìƒ seq_id={seq_id}, window_raw íŒ¨í‚· ìˆ˜={len(window_raw)}")
#             print("=" * 80)

#             self._append_dl_result_to_file(
#                 dl_output=dl_output,
#                 window_raw=window_raw,
#             )

#             if interval > 0:
#                 time.sleep(interval)

#         total_fc6 = len(self.fc6_ids_seen)
#         detected_fc6 = len(self.fc6_ids_detected)
#         missed_fc6 = max(0, total_fc6 - detected_fc6)

#         print("\nğŸ“Š DL FC6 íŒ¨í‚· ê¸°ì¤€ í‰ê°€ ê²°ê³¼")
#         print(f"  - ì´ FC6 íŒ¨í‚· ìˆ˜ : {total_fc6}")
#         print(f"  - ì¡íŒ FC6(T)    : {detected_fc6}")
#         print(f"  - ëª» ì¡ì€ FC6(F) : {missed_fc6}")
#         if total_fc6 > 0:
#             detect_rate = detected_fc6 / total_fc6
#             print(f"  - Detect Rate    : {detect_rate:.4f}")
#         else:
#             print("  - FC6 íŒ¨í‚·ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

#         print("\nğŸ JSONL ì…ë ¥ ë. íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ.")


# def main():
#     parser = argparse.ArgumentParser(description="S_PipeLine_file: ALL window DL ì‹¤í–‰ ë²„ì „")
#     parser.add_argument("--input", required=True, help="ì…ë ¥ JSONL íŒŒì¼")
#     parser.add_argument("--interval", type=float, default=0.0)
#     parser.add_argument("--max", type=int)
#     parser.add_argument("--dl-out", type=str, default="dl_results.jsonl")
#     args = parser.parse_args()

#     pipeline = SequentialPipeLineFromFile(
#         input_path=Path(args.input),
#         dl_out_path=Path(args.dl_out),
#     )
#     pipeline.run(interval=args.interval, max_count=args.max)


# if __name__ == "__main__":
#     main()

# # ì‚¬ìš© ì˜ˆ:
# # python S_PipeLine_file.py --input /home/slime/ML/output_all.jsonl --interval 1.0
# # python S_PipeLine_file.py --input /home/slime/ML/output_all.jsonl --interval 0.5


## --------------------
# from __future__ import annotations

# import json
# import time
# import argparse
# from copy import deepcopy
# from pathlib import Path
# from typing import Any, Dict, List, Optional, Set
# import threading
# import queue
# from datetime import datetime
# import requests
# import random

# from ML_start import ML_start
# from DL_start import DL_start


# DL_OUTPUT_PATH = Path("/home/slime/SLM/DL/output/dl_replay_output.jsonl")

# PIPELINE_WINDOW_SIZE = 80
# PIPELINE_STEP_SIZE = 40  # sliding stride

# ALARM_BASE_URL = "http://192.168.4.140:8080"


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# #  Thread-safe JSONL writer
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# class JsonlWriter:
#     def __init__(self, path: Path):
#         self.path = path
#         self.path.parent.mkdir(parents=True, exist_ok=True)

#         self.queue: queue.Queue[dict] = queue.Queue()
#         self.stop_event = threading.Event()
#         self.thread = threading.Thread(target=self._worker, daemon=True)
#         self.thread.start()

#     def _worker(self):
#         with self.path.open("a", encoding="utf-8") as f:
#             while not self.stop_event.is_set() or not self.queue.empty():
#                 try:
#                     record = self.queue.get(timeout=0.5)
#                 except queue.Empty:
#                     continue

#                 try:
#                     line = json.dumps(record, ensure_ascii=False)
#                     f.write(line + "\n")
#                     f.flush()
#                 except Exception as e:
#                     print(f"[JsonlWriter] write ì˜¤ë¥˜: {e}")

#                 self.queue.task_done()

#     def write(self, record: Dict[str, Any]):
#         self.queue.put(record)

#     def close(self):
#         self.stop_event.set()
#         self.queue.join()
#         self.thread.join()


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# #  Alarm Send
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def send_alarm_to_api_from_dl(dl_output: Dict[str, Any], engine="dl"):
#     try:
#         summary = dl_output.get("DL", {}).get("summary", {}) or {}
#         risk = summary.get("risk", {}) or {}
#     except Exception:
#         print("  [API] DL summary â†’ risk ì¶”ì¶œ ì‹¤íŒ¨")
#         return

#     if not isinstance(risk, dict):
#         return

#     # timestamp ë³´ì •
#     risk.setdefault("detected_time", datetime.utcnow().isoformat(timespec="seconds") + "Z")

#     url = f"{ALARM_BASE_URL}/api/alarms/{engine}"

#     try:
#         resp = requests.post(url, json={"risk": risk}, timeout=3)
#         resp.raise_for_status()
#         print(f"  [API] Alarm sent â†’ {resp.status_code}")
#     except Exception as e:
#         print(f"  [API] ì „ì†¡ ì‹¤íŒ¨: {e}")


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# #  JSONL Iterator + timestamp ê¸°ë°˜ traffic replay
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def iter_jsonl_timestamp_replay(path: Path):
#     """
#     JSONLì„ ì½ë˜,
#     ë‹¤ìŒ íŒ¨í‚·ì˜ timestamp - ì´ì „ íŒ¨í‚· timestamp ë§Œí¼ sleep í•˜ì—¬
#     ì‹¤ì œ íŠ¸ë˜í”½ ì†ë„ë¡œ ì¬ìƒí•œë‹¤.
#     """
#     prev_ts = None

#     with path.open("r", encoding="utf-8") as f:

#         for line_no, line in enumerate(f, start=1):
#             line = line.strip()
#             if not line:
#                 continue

#             try:
#                 obj = json.loads(line)
#             except:
#                 print(f"âš  JSON íŒŒì‹± ì‹¤íŒ¨(line {line_no})")
#                 continue

#             origin = obj if "origin" not in obj else obj["origin"]

#             ts_str = origin.get("@timestamp")
#             if ts_str:
#                 try:
#                     cur_ts = datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
#                 except:
#                     cur_ts = None
#             else:
#                 cur_ts = None

#             # timestamp replay delay ì ìš©
#             if prev_ts and cur_ts:
#                 delta = (cur_ts - prev_ts).total_seconds()
#                 if delta > 0:
#                     time.sleep(delta)  # ì‹¤ì œ ìº¡ì²˜ ì†ë„ ê·¸ëŒ€ë¡œ

#             prev_ts = cur_ts

#             # redis_id ì¶”ê°€
#             if "redis_id" not in origin:
#                 origin["redis_id"] = f"line-{line_no}"

#             yield {"origin": origin}


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# #  Pipeline Class
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# class SequentialPipeLineReplay:
#     def __init__(self, input_path: Path, dl_out_path: Optional[Path] = None):

#         if not input_path.exists():
#             raise FileNotFoundError(f"ì…ë ¥ JSONL ì—†ìŒ: {input_path}")

#         self.input_path = input_path

#         self.writer = JsonlWriter(dl_out_path or DL_OUTPUT_PATH)

#         self.data_buffer: List[Dict[str, Any]] = []
#         self.window_size = PIPELINE_WINDOW_SIZE
#         self.seq_counter = 0

#         self.fc6_ids_seen: Set[str] = set()
#         self.fc6_ids_detected: Set[str] = set()

#         print(f"âœ“ Traffic Replay Mode í™œì„±í™”")
#         print(f"âœ“ Window Size = {self.window_size}")

#     # ----------------------------------------------------------------------
#     def _extract_fc6_ids(self, window_batch: List[Dict[str, Any]]) -> Set[str]:
#         ids = set()
#         for wrapped in window_batch:
#             o = wrapped["origin"]
#             proto = o.get("protocol")
#             fc = o.get("modbus.fc")

#             if proto == "modbus" and fc is not None:
#                 try:
#                     if int(fc) == 6:
#                         ids.add(o.get("redis_id"))
#                 except:
#                     pass
#         return ids

#     # ----------------------------------------------------------------------
#     def _save_dl_result(self, dl_output: Dict[str, Any], window_raw):
#         dl_block = dl_output.get("DL", {})
#         seq_id = dl_block.get("seq_id")
#         pattern = dl_block.get("pattern")
#         summary = dl_block.get("summary", {})

#         simple_window = []
#         for pkt in window_raw:
#             pkt_copy = deepcopy(pkt)
#             pkt_copy.pop("ML", None)
#             simple_window.append(pkt_copy)

#         record = {
#             "seq_id": seq_id,
#             "pattern": pattern,
#             "summary": summary,
#             "window_raw": simple_window,
#         }

#         self.writer.write(record)

#     # ----------------------------------------------------------------------
#     def run(self):
#         print("\nğŸš€ Traffic Replay Pipeline ì‹œì‘")
#         print("==============================================")

#         for wrapped in iter_jsonl_timestamp_replay(self.input_path):

#             origin = wrapped["origin"]
#             packet_id = origin["redis_id"]

#             print(f"\n[POP] {packet_id} @ {origin.get('@timestamp')}")

#             # 1) ML inference
#             try:
#                 ml_raw = ML_start(origin)
#             except Exception as e:
#                 print(f"ML_start ì‹¤íŒ¨: {e}")
#                 continue

#             wrapped["ML"] = ml_raw.get("ML", {}) if isinstance(ml_raw, dict) else {"raw": ml_raw}

#             self.data_buffer.append(wrapped)

#             if len(self.data_buffer) < self.window_size:
#                 continue

#             # í˜„ì¬ ìœˆë„ìš°
#             window_batch = self.data_buffer[-self.window_size:]

#             # 2) DL inference
#             print(" â†’ DL ì‹¤í–‰")
#             dl_output = DL_start(window_batch)
#             if not dl_output:
#                 continue

#             alert = dl_output.get("alert", "x")
#             score = dl_output.get("DL", {}).get("summary", {}).get("anomaly_score")

#             print(f"  DL alert={alert}, score={score}")

#             # FC6 ID track
#             fc6_in_window = self._extract_fc6_ids(window_batch)
#             self.fc6_ids_seen.update(fc6_in_window)

#             if alert == "o":
#                 self.fc6_ids_detected.update(fc6_in_window)

#             # Alert â†’ API ì „ì†¡
#             if alert == "o":
#                 send_alarm_to_api_from_dl(dl_output)

#                 # seq id ë¶€ì—¬
#                 self.seq_counter += 1
#                 dl_output["DL"]["seq_id"] = self.seq_counter

#                 # window_raw ìƒì„±
#                 window_raw = [deepcopy(pkt["origin"]) for pkt in window_batch]

#                 self._save_dl_result(dl_output, window_raw)

#             # ìŠ¬ë¼ì´ë”©
#             for _ in range(PIPELINE_STEP_SIZE):
#                 if self.data_buffer:
#                     self.data_buffer.pop(0)

#         # ëë‚˜ë©´ FC6 í†µê³„ ì¶œë ¥
#         total = len(self.fc6_ids_seen)
#         detect = len(self.fc6_ids_detected)

#         print("\nğŸ“Š FC6 Summary")
#         print(f"Total FC6 = {total}")
#         print(f"Detected  = {detect}")
#         print(f"Missed    = {total - detect if total > 0 else 0}")

#         print("\nğŸ Replay ì¢…ë£Œ")


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def main():
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--input", required=True)
#     parser.add_argument("--dl-out", default="dl_replay.jsonl")
#     args = parser.parse_args()

#     pipeline = SequentialPipeLineReplay(
#         input_path=Path(args.input),
#         dl_out_path=Path(args.dl_out),
#     )
#     pipeline.run()


# if __name__ == "__main__":
#     main()
