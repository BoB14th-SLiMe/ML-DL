"""
detect_lstm_ae_with_similarity_XAI.py
---------------------------------
LSTM Autoencoder ê¸°ë°˜ ì´ìƒíƒì§€ + íŒ¨í„´ ìœ ì‚¬ë„ + XAI í™•ì¥ ë²„ì „
"""
import json
import numpy as np
import tensorflow as tf
from loguru import logger
from pathlib import Path
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
from numpy.linalg import norm

# ============================================================
# âš™ï¸ Global Detection Parameters (ì¤‘ì•™ ì„¤ì • ê´€ë¦¬)
# ============================================================

# CONFIG = { # ê°±ì‹  ì „
#     # -----------------------------
#     # ğŸ“ ë°ì´í„° ë¡œë“œ ê´€ë ¨
#     # -----------------------------
#     "limit": 10000,             # ìµœëŒ€ íŒ¨í‚· ë¡œë“œ ìˆ˜
#     "window_step": 1,           # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° step (ê¸°ë³¸ 1)
#     "adaptive_threshold": True, # IQR ê¸°ë°˜ ìë™ ì„ê³„ê°’ ê³„ì‚° ì‚¬ìš© ì—¬ë¶€

#     # -----------------------------
#     # ğŸ“ˆ Reconstruction Error Threshold
#     # -----------------------------
#     "threshold_factor": 3.0,    # mean + n * std ë°©ì‹ì—ì„œ n
#     "iqr_factor": 1.5,          # IQR ê¸°ë°˜ threshold multiplier

#     # -----------------------------
#     # ğŸ§  Semantic Score Weights
#     # -----------------------------
#     "mse_weight": 0.30,
#     "latent_weight": 0.35,
#     "entropy_weight": 0.25,
#     "temporal_weight": 0.10,

#     # -----------------------------
#     # ğŸš¨ Semantic íŒë‹¨ ê¸°ì¤€
#     # -----------------------------
#     "semantic_threshold": 0.75,  # ì˜ë¯¸ë¡ ì  ì´ìƒ íƒì§€ ê¸°ì¤€
#     "similarity_cutoff": 85.0,   # ìœ ì‚¬ë„ % ê¸°ì¤€
#     "mse_high": 1000.0,          # êµ¬ì¡°ì  ì´ìƒìœ¼ë¡œ íŒë‹¨í•  MSE ê¸°ì¤€
# }

CONFIG = { # ê°±ì‹  í›„
    "limit": 100000,
    "window_step": 1,
    "adaptive_threshold": False,
    "threshold_factor": 2.0,
    "iqr_factor": 1.5,

    "mse_weight": 0.46,
    "latent_weight": 0.29,
    "entropy_weight": 0.25,
    "temporal_weight": 0.0,

    "semantic_threshold": 0.75,
    "similarity_cutoff": 85.0,
    "mse_high": 1000.0,
}


def auto_config_tuning(mse_scores):
    """ë°ì´í„° í†µê³„ ê¸°ë°˜ìœ¼ë¡œ CONFIG ê°’ì„ ìë™ ì¡°ì •"""
    mean_mse, std_mse = mse_scores.mean(), mse_scores.std()
    q1, q3 = np.percentile(mse_scores, [25, 75])
    iqr = q3 - q1

    # 1ï¸âƒ£ threshold ê´€ë ¨ ìë™í™”
    if std_mse > mean_mse * 0.5:      # ë¶„ì‚°ì´ í° ê²½ìš° adaptive threshold
        CONFIG["adaptive_threshold"] = True
        CONFIG["iqr_factor"] = np.clip(iqr / mean_mse, 1.2, 3.0)
    else:
        CONFIG["adaptive_threshold"] = False
        CONFIG["threshold_factor"] = np.clip(std_mse / mean_mse * 5, 2.0, 4.0)

    # 2ï¸âƒ£ Semantic Score ê°€ì¤‘ì¹˜ ìë™ ì¡°ì •
    entropy_level = np.mean(np.log1p(mse_scores)) / 5
    CONFIG["mse_weight"] = round(0.25 + entropy_level * 0.1, 2)
    CONFIG["latent_weight"] = round(0.35 + (1 - entropy_level) * 0.05, 2)
    CONFIG["entropy_weight"] = 0.25
    CONFIG["temporal_weight"] = round(
        1.0 - (CONFIG["mse_weight"] + CONFIG["latent_weight"] + CONFIG["entropy_weight"]), 2
    )

    logger.info(f"ğŸ§© Auto-Tuned CONFIG: {CONFIG}")
    return CONFIG

# ============================================================
# 1ï¸âƒ£ í”„ë¡œí† ì½œ ë§¤í•‘
# ============================================================
PROTO_MAP = {
    "unknown": 0, "arp": 1, "bacnet": 2, "dhcp": 3, "dnp3": 4, "dns": 5,
    "ethernet_ip": 6, "iec104": 7, "mms": 8, "modbus_tcp": 9,
    "opc_ua": 10, "s7comm": 11, "tcp_session": 12, "xgt-fen": 13
}
DIR_MAP = {"request": 0, "response": 1, "unknown": 2}


# ============================================================
# 2ï¸âƒ£ Feature Extractor
# ============================================================
def safe_float(x, default=0.0):
    try:
        if isinstance(x, (list, dict)):
            return default
        return float(x)
    except Exception:
        return default


def extract_features(pkt):
    proto = pkt.get("protocol", "unknown")
    proto_id = PROTO_MAP.get(proto, 0)
    dir_flag = DIR_MAP.get(pkt.get("dir"), 2)
    fc, addr, val, flen = 0.0, 0.0, 0.0, 0.0

    d = pkt.get("d", {})
    if isinstance(d, dict) and "len" in d:
        flen = safe_float(d.get("len"))

    if proto == "modbus_tcp":
        pdu = d.get("pdu", {})
        fc = safe_float(pdu.get("fc"))
        addr = safe_float(pdu.get("addr"))
        if isinstance(pdu.get("regs"), dict):
            vals = [safe_float(v) for v in pdu["regs"].values()]
            val = np.mean(vals) if vals else 0.0
    elif proto == "xgt-fen":
        inst = d.get("inst", {})
        fc = safe_float(inst.get("cmd"))
        val = safe_float(inst.get("dataSize"))
        varNm = inst.get("varNm", "")
        digits = "".join(ch for ch in varNm if ch.isdigit())
        addr = safe_float(digits)
    elif proto == "s7comm":
        pdu = d.get("pdu", {})
        prm = pdu.get("prm", {})
        fc = safe_float(prm.get("fn"))
        itms = prm.get("itms", [])
        if itms:
            addr = safe_float(itms[0].get("addr"))
            val = safe_float(itms[0].get("amt"))
    elif proto == "mms":
        val = safe_float(d.get("len"))

    delta_t = safe_float(pkt.get("_delta_t", 0.0))
    return [proto_id, dir_flag, fc, addr, val, flen, delta_t]


# ============================================================
# 3ï¸âƒ£ JSONL ë¡œë“œ
# ============================================================
def get_model_window_size(model):
    input_shape = model.input_shape
    return input_shape[1]


def load_sequences_from_jsonl(path, window_size=14, overlap=1, limit=10000):
    packets, raw_packets = [], []
    with open(path, "r", encoding="utf-8") as f:
        for line_idx, line in enumerate(tqdm(f, desc=f"Parsing JSONL (limit={limit})")):
            if line_idx >= limit:
                break
            if not line.strip():
                continue
            try:
                pkt = json.loads(line)
                packets.append(extract_features(pkt))
                # ğŸ”¹ ì›ë³¸ ì „ì²´ JSON ë³´ì¡´
                raw_packets.append(pkt)
            except Exception:
                continue

    total_packets = len(packets)
    logger.info(f"ğŸ“¦ Loaded {total_packets} packets (limited to {limit}) from {path}")

    sequences, seq_raw = [], []
    step = CONFIG["window_step"]
    for i in range(0, total_packets - window_size + 1, step):
        window = packets[i: i + window_size]
        raw_window = raw_packets[i: i + window_size]
        sequences.append(np.array(window, dtype="float32"))  # âœ… ë°˜ë“œì‹œ numpy ë°°ì—´ë¡œ ë³€í™˜í•´ì•¼ í•¨
        seq_raw.append(raw_window)


    logger.info(f"ğŸ“‚ Generated {len(sequences)} windows (size={window_size}, step={step})")
    return sequences, seq_raw



# ============================================================
# 4ï¸âƒ£ Padding
# ============================================================
def pad_with_mask(X_list):
    if not X_list:
        raise ValueError("âŒ No valid sequences found for padding.")
    X_list = [np.array(x, dtype="float32") if not isinstance(x, np.ndarray) else x for x in X_list]
    max_len = max(x.shape[0] for x in X_list)
    feat_dim = X_list[0].shape[1]
    X_padded = np.zeros((len(X_list), max_len, feat_dim), dtype="float32")
    for i, x in enumerate(X_list):
        X_padded[i, :x.shape[0], :] = x
    logger.info(f"âœ… Padded shape: {X_padded.shape}")
    return X_padded


# ============================================================
# 5ï¸âƒ£ DL - Reconstruction & Latent
# ============================================================
def get_latent_vectors(model, X):
    encoder = tf.keras.Model(inputs=model.input, outputs=model.get_layer("lstm_1").output)
    return encoder.predict(X, verbose=0)


def reconstruction_error(model, X):
    recon = model.predict(X, verbose=0)
    mse = np.mean(np.square(X - recon), axis=(1, 2))
    return mse, recon


# ============================================================
# 6ï¸âƒ£ DL ê¸°ë°˜ XAI ì§€í‘œ ê³„ì‚°
# ============================================================
def compute_xai_metrics(model, X, recon, latent_vecs, sim_matrix, best_indices, pattern_centroids):
    feature_err = np.mean(np.square(X - recon), axis=1)
    feature_names = ["proto_id", "dir_flag", "fc", "addr", "val", "flen", "delta_t"]

    temporal_err = np.mean(np.square(X - recon), axis=2)
    pattern_vectors = np.stack(list(pattern_centroids.values()))

    latent_dist = np.array([
        norm(latent_vecs[i] - pattern_vectors[best_indices[i]])
        for i in range(len(latent_vecs))
    ])

    sim_norm = sim_matrix / (np.sum(sim_matrix, axis=1, keepdims=True) + 1e-9)
    entropy = -np.sum(sim_norm * np.log(sim_norm + 1e-9), axis=1)

    return feature_err, temporal_err, latent_dist, entropy, feature_names


# ============================================================
# 7ï¸âƒ£ Main Detection Function
# ============================================================
# ============================================================
# 7ï¸âƒ£ Main Detection Function
# ============================================================
import time  # â±ï¸ ì¶”ê°€

def detect_anomalies_with_similarity_XAI(model_path, jsonl_path, pattern_centroids, threshold=None):
    global CONFIG
    logger.info(f"ğŸš€ Loading model: {model_path}")
    model = tf.keras.models.load_model(model_path)

    # ------------------------------------------------------------
    # ğŸ•’ ì „ì²´ ë³€í™˜ + ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì‹œì‘
    # ------------------------------------------------------------
    pipeline_start = time.time()

    # ------------------------
    # 1ï¸âƒ£ ë°ì´í„° ë³€í™˜ (ë¡œë“œ + Feature ì¶”ì¶œ + Padding)
    # ------------------------
    window_size = get_model_window_size(model)
    X_list, meta_windows = load_sequences_from_jsonl(
        jsonl_path, window_size=window_size, overlap=window_size // 2, limit=CONFIG["limit"]
    )
    X_padded = pad_with_mask(X_list)

    # ------------------------
    # 2ï¸âƒ£ DL ì¶”ë¡  (Reconstruction + Latent Vector)
    # ------------------------
    infer_start = time.time()
    mse_scores, recon = reconstruction_error(model, X_padded)
    latent_vecs = get_latent_vectors(model, X_padded)
    infer_end = time.time()

    # ------------------------
    # 3ï¸âƒ£ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ
    # ------------------------
    pipeline_end = time.time()

    # ------------------------
    # â±ï¸ ì‹œê°„ ê³„ì‚°
    # ------------------------
    total_inference_time = infer_end - infer_start
    total_pipeline_time = pipeline_end - pipeline_start
    avg_inference_time = total_inference_time / len(X_padded)
    avg_pipeline_time = total_pipeline_time / len(X_padded)

    logger.info(f"ğŸ§  Total DL inference time: {total_inference_time:.3f} sec")
    logger.info(f"âš¡ Avg inference time per window: {avg_inference_time:.6f} sec")
    logger.info(f"ğŸ§© Total pipeline time (load+transform+inference): {total_pipeline_time:.3f} sec")
    logger.info(f"ğŸš€ Avg pipeline time per window: {avg_pipeline_time:.6f} sec")

    # ------------------------------------------------------------
    # 4ï¸âƒ£ ì„ê³„ê°’ ê³„ì‚° ë° ì˜ˆì¸¡
    # ------------------------------------------------------------
    mean_mse, std_mse = mse_scores.mean(), mse_scores.std()
    if CONFIG["adaptive_threshold"]:
        q1, q3 = np.percentile(mse_scores, [25, 75])
        iqr = q3 - q1
        threshold = q3 + CONFIG["iqr_factor"] * iqr
    else:
        threshold = mean_mse + CONFIG["threshold_factor"] * std_mse

    logger.info(f"ğŸ“Š mean={mean_mse:.4f}, std={std_mse:.4f}, threshold={threshold:.4f}")
    preds = (mse_scores > threshold).astype(int)

    # ------------------------------------------------------------
    # 5ï¸âƒ£ Latent Similarity + XAI Metric ê³„ì‚°
    # ------------------------------------------------------------
    pattern_names = list(pattern_centroids.keys())
    pattern_vectors = np.stack(list(pattern_centroids.values()))
    sim_matrix = cosine_similarity(latent_vecs, pattern_vectors)
    best_indices = np.argmax(sim_matrix, axis=1)
    best_patterns = [pattern_names[i] for i in best_indices]
    best_scores = [sim_matrix[j, i] * 100 for j, i in enumerate(best_indices)]

    feat_err, time_err, latent_dist, entropy, feat_names = compute_xai_metrics(
        model, X_padded, recon, latent_vecs, sim_matrix, best_indices, pattern_centroids
    )

    # ------------------------------------------------------------
    # 6ï¸âƒ£ ê²°ê³¼ ì €ì¥
    # ------------------------------------------------------------
    result_path = Path(jsonl_path).with_name("reconstruction_detect_with_XAI.json")
    results = []

    for idx, m in enumerate(mse_scores):
        results.append({
            "seq_id": int(idx),
            "mse": float(m),
            "is_anomaly": bool(preds[idx]),
            "closest_pattern": best_patterns[idx],
            "similarity": round(best_scores[idx], 2),
            "latent_distance": float(latent_dist[idx]),
            "similarity_entropy": float(entropy[idx]),
            "feature_error": {feat_names[k]: float(feat_err[idx, k]) for k in range(len(feat_names))},
            "temporal_error_mean": float(np.mean(time_err[idx])),
            "temporal_error_max": float(np.max(time_err[idx])),
            "window_raw": meta_windows[idx],
        })

    # ------------------------------------------------------------
    # 7ï¸âƒ£ ìš”ì•½ ì •ë³´ (ì¶”ë¡  + ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œê°„)
    # ------------------------------------------------------------
    summary_info = {
        "inference_summary": {
            "num_windows": len(X_padded),
            "total_inference_time_sec": round(total_inference_time, 4),
            "avg_inference_time_per_window_sec": round(avg_inference_time, 6),
            "total_pipeline_time_sec": round(total_pipeline_time, 4),
            "avg_pipeline_time_per_window_sec": round(avg_pipeline_time, 6)
        }
    }

    import copy
    with open(result_path, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(copy.deepcopy(r), ensure_ascii=False, default=lambda o: float(o)) + "\n")
        f.write(json.dumps(summary_info, ensure_ascii=False) + "\n")

    logger.success(f"âœ… XAI Detection done â†’ {result_path.resolve()}")
    logger.info(f"ğŸ“ˆ Avg similarity={np.mean(best_scores):.2f}%, Avg entropy={np.mean(entropy):.4f}")
    logger.info(f"ğŸ§¾ Inference Summary â†’ {summary_info}")
    return results



# ============================================================
# 8ï¸âƒ£ Semantic Score & SLM Input Generator
# ============================================================
def compute_semantic_score(mse, latent_distance, entropy, temporal_mean, temporal_max):
    mse_norm = np.log1p(mse) / 10
    ld_norm = latent_distance / (latent_distance + 5)
    ent_norm = entropy / (entropy + 2)
    temp_norm = np.log1p(temporal_max) / 10

    score = (
        CONFIG["mse_weight"] * mse_norm +
        CONFIG["latent_weight"] * ld_norm +
        CONFIG["entropy_weight"] * ent_norm +
        CONFIG["temporal_weight"] * temp_norm
    )
    return float(np.clip(score, 0.0, 1.0))


def generate_SLM_input(results, save_path):
    out_path = Path(save_path).with_name("for_XAI_SLM.jsonl")
    slm_ready = []

    for r in results:
        score = compute_semantic_score(
            r["mse"], r["latent_distance"], r["similarity_entropy"],
            r["temporal_error_mean"], r["temporal_error_max"]
        )

        if score > CONFIG["semantic_threshold"] and r["similarity"] < CONFIG["similarity_cutoff"]:
            anomaly_type = "semantic_deformation"
        elif r["is_anomaly"] and r["mse"] > CONFIG["mse_high"]:
            anomaly_type = "structural_deviation"
        else:
            anomaly_type = "normal"

        top_feat = max(r["feature_error"], key=r["feature_error"].get)
        top_val = r["feature_error"][top_feat]

        context = (
            f"ì´ ì‹œí€€ìŠ¤ëŠ” {r['closest_pattern']} íŒ¨í„´ì— ì†í•˜ì§€ë§Œ "
            f"{top_feat} í•„ë“œì—ì„œ ë†’ì€ ë³µì› ì˜¤ì°¨({top_val:.3f})ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
            f"ìœ ì‚¬ë„ {r['similarity']:.2f}%, ì—”íŠ¸ë¡œí”¼ {r['similarity_entropy']:.3f}, "
            f"latent distance {r['latent_distance']:.3f}ë¡œ ì˜ë¯¸ì  ë¶ˆì•ˆì •ì„±ì´ ê°ì§€ë©ë‹ˆë‹¤. "
            f"Semantic score={score:.3f}."
        )

        slm_ready.append({
            "seq_id": r["seq_id"],
            "pattern": r["closest_pattern"],
            "summary": {
                "semantic_score": score,
                "anomaly_type": anomaly_type,
                "similarity": r["similarity"],
                "similarity_entropy": r["similarity_entropy"],
                "latent_distance": r["latent_distance"],
                "feature_error": r["feature_error"],
                "temporal_error_max": r["temporal_error_max"],
                "context": context
            },
            "window_raw": r.get("window_raw", []),
            "prompt": (
                "ì´ ì‹œí€€ìŠ¤ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ì–´ë–¤ ì´ìƒì„ ë‚˜íƒ€ë‚´ëŠ”ê°€? "
                "ì •ìƒ íŒ¨í„´ê³¼ ë¹„êµí•˜ì—¬ ì–´ë–¤ í•„ë“œê°€ ë³€í˜•ë˜ì—ˆëŠ”ì§€ ì„¤ëª…í•˜ê³ , "
                "í•´ë‹¹ í–‰ìœ„ê°€ ë°¸ë¸Œë‚˜ ì•¡ì¶”ì—ì´í„°ë¥¼ ì†ìƒì‹œí‚¬ ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ë¼."
            )
        })

    with open(out_path, "w", encoding="utf-8") as f:
        for row in slm_ready:
            f.write(json.dumps(row, ensure_ascii=False, default=lambda o: float(o)) + "\n")

    logger.success(f"ğŸ§  SLM Input file generated â†’ {out_path.resolve()}")
    logger.info(f"ğŸ’¡ í‰ê·  Semantic Score: {np.mean([r['summary']['semantic_score'] for r in slm_ready]):.3f}")


# ============================================================
# ğŸ”š Entry Point
# ============================================================
if __name__ == "__main__":
    MODEL_PATH = "outputs/models/LSTM_AE_Flexible_v2.keras"
    JSONL_PATH = "dataset/PLS-JSONL/merged.jsonl"
    pattern_centroids = np.load("outputs/pattern_centroids.npy", allow_pickle=True).item()

    results = detect_anomalies_with_similarity_XAI(
        model_path=MODEL_PATH,
        jsonl_path=JSONL_PATH,
        pattern_centroids=pattern_centroids
    )

    generate_SLM_input(results, JSONL_PATH)



"""
{
  "seq_id": 9576,                # ì‹œí€€ìŠ¤ ê³ ìœ  ID (DL ì…ë ¥ ìœˆë„ìš° ë˜ëŠ” ìŠ¬ë¼ì´ë”© ì‹œí€€ìŠ¤ì˜ ì‹ë³„ì)
  "pattern": "P_0002",           # SLM ë˜ëŠ” DLì´ ë¶„ë¥˜í•œ íŒ¨í„´ëª… (ì˜ˆ: ê³µì • ì œì–´ ì‹œí€€ìŠ¤ ìœ í˜•)
  "summary": {
    "semantic_score": 0.7091595467880214,  # SLMì´ ê³„ì‚°í•œ ì˜ë¯¸ì  ì¼ê´€ì„± ì ìˆ˜ (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •ìƒ)
    "anomaly_type": "normal",              # DL/SLM íŒë‹¨ ê²°ê³¼: "normal" ë˜ëŠ” "anomaly"
    "similarity": 74.56999969482422,       # í•™ìŠµëœ ì •ìƒ íŒ¨í„´(P_0002)ê³¼ì˜ ìœ ì‚¬ë„ (%)
    "similarity_entropy": 1.7913528680801392,  # ìœ ì‚¬ë„ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ (ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤ì„±ì´ í¼)
    "latent_distance": 3.7522637844085693,     # ì ì¬ê³µê°„(latent space) ê±°ë¦¬ (ê°’ì´ í´ìˆ˜ë¡ ë¹„ì •ìƒ ê°€ëŠ¥ì„±)
    
    # ğŸ”¹ ì¬êµ¬ì„±(ë³µì›) ë‹¨ê³„ì—ì„œ í•„ë“œë³„ ì˜¤ì°¨(Feature Reconstruction Error)
    "feature_error": {
      "proto_id": 122.9727554321289,  # í”„ë¡œí† ì½œ ID í•„ë“œ ë³µì› ì˜¤ì°¨
      "dir_flag": 1.0189133882522583, # ë°©í–¥ í”Œë˜ê·¸(ìš”ì²­/ì‘ë‹µ) ë³µì› ì˜¤ì°¨
      "fc": 3558.677978515625,        # Modbus/S7 í•¨ìˆ˜ ì½”ë“œ(Function Code) ë³µì› ì˜¤ì°¨
      "addr": 174670.53125,           # ì£¼ì†Œ(Address) í•„ë“œ ë³µì› ì˜¤ì°¨ (ê°€ì¥ í° ì´ìƒ ì§•í›„ ë°œìƒ)
      "val": 33.698707580566406,      # ê°’(Value) í•„ë“œ ë³µì› ì˜¤ì°¨
      "flen": 5.429335594177246,      # í”„ë ˆì„ ê¸¸ì´(Field Length) ë³µì› ì˜¤ì°¨
      "delta_t": 0.07732956856489182  # ì—°ì† íŒ¨í‚· ê°„ ì‹œê°„ ê°„ê²© ë³µì› ì˜¤ì°¨
    },

    "temporal_error_max": 205339.265625,  # ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ë°œìƒí•œ ìµœëŒ€ ì‹œê³„ì—´ ì˜¤ì°¨ê°’
    "context": "ì´ ì‹œí€€ìŠ¤ëŠ” P_0002 íŒ¨í„´ì— ì†í•˜ì§€ë§Œ addr í•„ë“œì—ì„œ ë†’ì€ ë³µì› ì˜¤ì°¨(174670.531)ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ..."
              # SLMì´ ìƒì„±í•œ ìì—°ì–´ ì„¤ëª…: ì–´ë–¤ í•„ë“œì—ì„œ ì˜¤ì°¨ê°€ ì»¸ëŠ”ì§€, ì˜ë¯¸ë¡ ì  ì•ˆì •ì„± ìˆ˜ì¤€ ì„¤ëª…
  },
  "window_raw": ì›ë³¸ íŒ¨í‚·ì— ê´€í•œ json ë°ì´í„°
  "prompt": "ì´ ì‹œí€€ìŠ¤ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ì–´ë–¤ ì´ìƒì„ ë‚˜íƒ€ë‚´ëŠ”ê°€? ..."  
            # LLM(XAI) ì§ˆì˜ í”„ë¡¬í”„íŠ¸: SLMì´ DLì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ì„ì  ì„¤ëª…ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„
}

"""