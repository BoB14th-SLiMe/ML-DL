#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
benchmark_and_eval.py

ìœˆë„ìš° ë‹¨ìœ„ JSONL + LSTM-AE ëª¨ë¸ â†’ ìœˆë„ìš°ë³„ MSE ê³„ì‚° + (ì„ íƒ) GTì™€ ë¹„êµí•œ
íƒì§€ ì„±ëŠ¥ ì§€í‘œ / MSE ë¶„í¬ê¹Œì§€ í•œ ë²ˆì— ê³„ì‚°í•´ ì£¼ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸.
"""

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd

import matplotlib
from matplotlib.patches import Patch
matplotlib.use("Agg")  # GUI ì—†ì´ ì €ì¥ìš©
import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# Argument parsing
# ---------------------------------------------------------------------
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description="ìœˆë„ìš° JSONL + LSTM-AE ëª¨ë¸ â†’ MSE ê³„ì‚° + íƒì§€ ì„±ëŠ¥ / MSE í†µê³„ê¹Œì§€ í•œ ë²ˆì— ìˆ˜í–‰"
    )

    # ìœˆë„ìš° JSONL + ì „ì²˜ë¦¬ (ì „ì²˜ë¦¬ ë””ë ‰í† ë¦¬ëŠ” í˜„ì¬ ë²„ì „ì—ì„œëŠ” ì´ë¦„ë§Œ ë°›ìŒ)
    p.add_argument(
        "--input",
        "-i",
        required=True,
        help="ìœˆë„ìš° ë‹¨ìœ„ JSONL ê²½ë¡œ (ê° line = 1 window, sequence_group í¬í•¨)",
    )
    p.add_argument(
        "--pre-dir",
        "-p",
        required=False,
        default=None,
        help="(í˜¸í™˜ìš©) ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ë””ë ‰í† ë¦¬. í˜„ì¬ ë²„ì „ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.",
    )
    p.add_argument(
        "--window-size",
        "-w",
        type=int,
        default=80,
        help="ìœˆë„ìš° ê¸¸ì´(T, time steps). sequence_group ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ pad/truncate",
    )
    p.add_argument(
        "--output-dir",
        "-o",
        required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (metrics_*.json, analyze_mse_dist_*.json ì €ì¥ ìœ„ì¹˜)",
    )
    p.add_argument(
        "--no-pad-last",
        action="store_true",
        help="(ì´ì „ ë²„ì „ í˜¸í™˜ìš©) í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. í•­ìƒ pad í•´ì„œ ê³ ì • ê¸¸ì´ ìœˆë„ìš° ìƒì„±.",
    )

    # DL ëª¨ë¸ / inference ê´€ë ¨ ì˜µì…˜
    p.add_argument(
        "--model-dir",
        "-m",
        required=True,
        help="train_lstm_ae_windows_keras.py ê²°ê³¼ ë””ë ‰í† ë¦¬ (model.h5, config.json, feature_keys.txt ë“±)",
    )
    p.add_argument(
        "--batch-size",
        "-b",
        type=int,
        default=128,
        help="DL ëª¨ë¸ inference batch size (ê¸°ë³¸=128)",
    )
    p.add_argument(
        "--threshold",
        "-t",
        type=float,
        default=None,
        help=(
            "ìœˆë„ìš° MSE anomaly threshold "
            "(ë¯¸ì§€ì • ì‹œ model_dir/threshold.json ê°’ì„ ì‚¬ìš©, ë‘˜ ë‹¤ ì—†ìœ¼ë©´ is_anomaly=-1)"
        ),
    )
    p.add_argument(
        "--tag",
        default=None,
        help="ì¶œë ¥ íŒŒì¼ ì´ë¦„ì— ë¶™ì¼ íƒœê·¸ (ê¸°ë³¸: ì…ë ¥ JSONL íŒŒì¼ëª… stem)",
    )

    # í‰ê°€ìš© GT / ì¶œë ¥ ê²½ë¡œ
    p.add_argument(
        "--attack-csv",
        "-a",
        default=None,
        help="(ì„ íƒ) ì‹¤ì œ ê³µê²© ì—¬ë¶€ê°€ ë“¤ì–´ìˆëŠ” íŒŒì¼ (CSV ë˜ëŠ” JSONL; ì˜ˆ: attack_result_XXX.csv, attack_ver2_window.jsonl)",
    )
    p.add_argument(
        "--metrics-json",
        default=None,
        help=(
            "(ì„ íƒ) íƒì§€ ì„±ëŠ¥ ì§€í‘œë¥¼ ì €ì¥í•  JSON ê²½ë¡œ "
            "(ë¯¸ì§€ì • ì‹œ output-dir/metrics_{tag}.json ìœ¼ë¡œ ì €ì¥)"
        ),
    )
    p.add_argument(
        "--mse-stats-json",
        default=None,
        help=(
            "(ì„ íƒ) MSE í†µê³„ë¥¼ ë³„ë„ë¡œ ì €ì¥í•  JSON ê²½ë¡œ "
            "(ë¯¸ì§€ì • ì‹œ output-dir/analyze_mse_dist_{tag}.json ìœ¼ë¡œ ì €ì¥)"
        ),
    )
    p.add_argument(
        "--ignore-pred-minus1",
        action="store_true",
        help="is_anomaly_pred == -1 ì¸ ìœˆë„ìš°ëŠ” í‰ê°€/í†µê³„ì—ì„œ ì œì™¸ (threshold ì•ˆ ì“´ ê²½ìš° ë“±)",
    )

    # â­ ì¶”ê°€: feature weight íŒŒì¼ ì§ì ‘ ì§€ì • (ì˜µì…˜)
    p.add_argument(
        "--feature-weights-file",
        type=str,
        default=None,
        help=(
            "train ë•Œ ì‚¬ìš©í•œ feature_weights.txt ê²½ë¡œ. "
            "ì§€ì • ì•ˆ í•˜ë©´ config.feature_weights_file ê¸°ë°˜ìœ¼ë¡œ ìë™ íƒìƒ‰. "
            "ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ê· ì¼ ê°€ì¤‘ì¹˜(1.0) ì‚¬ìš©."
        ),
    )

    return p.parse_args()


# ---------------------------------------------------------------------
# Utils
# ---------------------------------------------------------------------
def safe_div(num: float, den: float) -> float:
    if den == 0:
        return 0.0
    return float(num) / float(den)


def try_compute_auc(
    y_true: np.ndarray,
    scores: np.ndarray,
) -> Dict[str, Optional[float]]:
    """
    mse ê¸°ë°˜ ROC-AUC / PR-AUC ê³„ì‚° (ê°€ëŠ¥í•˜ë©´).
    - y_true: 0/1
    - scores: í´ìˆ˜ë¡ ê³µê²©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ score (ì˜ˆ: mse)
    """
    result: Dict[str, Optional[float]] = {"roc_auc": None, "pr_auc": None}
    try:
        from sklearn.metrics import roc_auc_score, average_precision_score
    except ImportError:
        print("[WARN] scikit-leë¥¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ ROC-AUC / PR-AUC ê³„ì‚°ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return result

    # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë©´ AUC ê³„ì‚° ë¶ˆê°€
    if len(np.unique(y_true)) < 2:
        print("[WARN] y_true ì— í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë¼ AUC ê³„ì‚° ë¶ˆê°€ (ì–‘ì„±/ìŒì„± ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•¨).")
        return result

    try:
        roc_auc = float(roc_auc_score(y_true, scores))
        pr_auc = float(average_precision_score(y_true, scores))
        result["roc_auc"] = roc_auc
        result["pr_auc"] = pr_auc
        print(f"[INFO] ROC-AUC={roc_auc:.4f}, PR-AUC={pr_auc:.4f}")
    except Exception as e:
        print(f"[WARN] AUC ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return result


def compute_mse_stats(
    y_true: np.ndarray,
    scores: np.ndarray,
) -> Dict[str, Any]:
    """
    ë¼ë²¨(0=ì •ìƒ, 1=ê³µê²©)ë³„ë¡œ score(mse)ì˜ í†µê³„ë¥¼ ê³„ì‚°í•œë‹¤.
    - ê¸°ë³¸ í†µê³„: count, mean, std, min, max
    - í¼ì„¼íƒ€ì¼: p5, p10, ..., p95 + p99
    """

    def _summary(arr: np.ndarray) -> Dict[str, Any]:
        arr = np.asarray(arr, dtype=float)
        # ë¹ˆ ë°°ì—´ì´ë©´ Noneìœ¼ë¡œ ì±„ìš°ê¸°
        if arr.size == 0:
            base = {
                "count": 0,
                "mean": None,
                "std": None,
                "min": None,
                "max": None,
            }
            for p in range(5, 100, 5):  # p5, p10, ..., p95
                base[f"p{p}"] = None
            base["p99"] = None
            return base

        base = {
            "count": int(arr.size),
            "mean": float(np.mean(arr)),
            "std": float(np.std(arr)),
            "min": float(np.min(arr)),
            "max": float(np.max(arr)),
        }
        # 5% ë‹¨ìœ„ percentile (5, 10, ..., 95)
        for p in range(5, 100, 5):
            base[f"p{p}"] = float(np.percentile(arr, p))
        # ì¶”ê°€ë¡œ p99 ìœ ì§€
        base["p99"] = float(np.percentile(arr, 99))
        return base

    attack_mask = y_true == 1
    normal_mask = y_true == 0

    attack_scores = scores[attack_mask]
    normal_scores = scores[normal_mask]

    attack_stats = _summary(attack_scores)
    normal_stats = _summary(normal_scores)

    mse_stats = {
        "meta": {
            "label_col": "is_anomaly_true",
            "score_col": "mse",
            "n_total": int(len(scores)),
            "n_attack": int(attack_mask.sum()),
            "n_normal": int(normal_mask.sum()),
        },
        "attack": attack_stats,
        "normal": normal_stats,
    }

    return mse_stats


def load_gt_table(path: Path) -> pd.DataFrame:
    """
    GT íŒŒì¼ì„ ë¡œë“œí•´ì„œ DataFrame ìœ¼ë¡œ ë°˜í™˜.
    - .csv  â†’ pandas.read_csv
    - .jsonl / .json â†’ JSON Lines ë¡œë“œ í›„ DataFrame
    """
    suffix = path.suffix.lower()
    if suffix == ".csv":
        print(f"[INFO] GT íŒŒì¼ í˜•ì‹: CSV ({path.name})")
        return pd.read_csv(path)
    elif suffix in [".jsonl", ".json"]:
        print(f"[INFO] GT íŒŒì¼ í˜•ì‹: JSONL/JSON ({path.name})")
        rows = []
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    rows.append(obj)
                except Exception as e:
                    print(f"[WARN] JSONL íŒŒì‹± ì‹¤íŒ¨: {e} (line ì¼ë¶€) â†’ ìŠ¤í‚µ")
                    continue
        if not rows:
            raise ValueError(f"GT JSONL ì—ì„œ ìœ íš¨í•œ ë ˆì½”ë“œë¥¼ í•˜ë‚˜ë„ ëª» ì½ì—ˆìŠµë‹ˆë‹¤: {path}")
        df = pd.DataFrame(rows)
        return df
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” GT íŒŒì¼ í™•ì¥ìì…ë‹ˆë‹¤: {path} (csv/jsonl/json ë§Œ ì§€ì›)")

def build_window_label_map(df_gt: pd.DataFrame) -> Dict[int, int]:
    tmp = df_gt.copy()

    tmp["window_index"] = pd.to_numeric(tmp["window_index"], errors="coerce")
    tmp = tmp.dropna(subset=["window_index"])
    tmp["window_index"] = tmp["window_index"].astype(int)

    tmp["is_anomaly"] = pd.to_numeric(tmp["is_anomaly"], errors="coerce").fillna(0).astype(int)

    # window_index ì¤‘ë³µì´ ìˆìœ¼ë©´ ê³µê²©(1)ì„ ìš°ì„ (ìµœëŒ€ê°’)
    s = tmp.groupby("window_index")["is_anomaly"].max()
    return s.to_dict()

# ---------------------------------------------------------------------
# DL model loading (LSTM-AE with repeat_latent)
# ---------------------------------------------------------------------
def load_model_from_dir(model_dir: Path):
    config_path = model_dir / "config.json"
    if not config_path.exists():
        raise FileNotFoundError(f"âŒ config.json ì—†ìŒ: {config_path}")

    with config_path.open("r", encoding="utf-8") as f:
        config = json.load(f)

    print(f"[INFO] config ë¡œë“œ: {config_path}")
    T = config.get("T")
    D = config.get("D")
    print(f"[INFO] í•™ìŠµ ì‹œ T={T}, D={D}")

    import tensorflow as tf  # noqa: F401
    from tensorflow.keras.models import load_model

    model_path = model_dir / "model.h5"
    if not model_path.exists():
        raise FileNotFoundError(f"âŒ model.h5 ì—†ìŒ: {model_path}")

    print(f"[INFO] model ë¡œë“œ: {model_path}")

    # decoder ì—ì„œ latent ë²¡í„°ë¥¼ T ê¸¸ì´ë¡œ ë°˜ë³µí•˜ëŠ” ì»¤ìŠ¤í…€ ë ˆì´ì–´ í•¨ìˆ˜
    T_for_repeat = T

    def repeat_latent(x):
        x = tf.expand_dims(x, axis=1)         # (B, 1, latent_dim)
        x = tf.tile(x, [1, T_for_repeat, 1])  # (B, T, latent_dim)
        return x

    custom_objects = {
        "repeat_latent": repeat_latent,
    }

    model = load_model(
        model_path,
        compile=False,
        custom_objects=custom_objects,
    )

    # threshold.json ì½ê¸°
    thresh_path = model_dir / "threshold.json"
    threshold = None
    if thresh_path.exists():
        try:
            with thresh_path.open("r", encoding="utf-8") as f:
                th_cfg = json.load(f)

            if "threshold" in th_cfg:
                threshold = float(th_cfg["threshold"])
                print(f"[INFO] threshold.json(threshold) ì‚¬ìš©: {threshold}")
            elif "threshold_p99" in th_cfg:
                threshold = float(th_cfg["threshold_p99"])
                print(f"[INFO] threshold_p99 ì‚¬ìš©: {threshold}")
            elif "threshold_mu3" in th_cfg:
                threshold = float(th_cfg["threshold_mu3"])
                print(f"[INFO] threshold_mu3 ì‚¬ìš©: {threshold}")
            else:
                print(
                    "[INFO] threshold.json ì•ˆì— ì‚¬ìš©í•  í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. "
                    "(threshold / threshold_p99 / threshold_mu3 ì—†ìŒ)"
                )
        except Exception as e:
            print(f"[WARN] threshold.json ë¡œë”© ì‹¤íŒ¨: {e}")

    # feature_keys.txt (í•™ìŠµì— ì‚¬ìš©í–ˆë˜ feature ìˆœì„œ)
    feat_path = model_dir / "feature_keys.txt"
    if not feat_path.exists():
        raise FileNotFoundError(f"âŒ feature_keys.txt ì—†ìŒ: {feat_path}")

    with feat_path.open("r", encoding="utf-8") as f:
        raw_keys = [line.strip() for line in f if line.strip()]

    # í˜¹ì‹œ "0 protocol_norm" ê°™ì€ í˜•ì‹ì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ, ë§ˆì§€ë§‰ í† í°ë§Œ feature ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
    feature_keys = [rk.split()[-1] for rk in raw_keys]

    print(f"[INFO] feature_keys.txt ë¡œë“œ, ê¸¸ì´ = {len(feature_keys)}")
    print("[DEBUG] feature_keys[:20] =", feature_keys[:20])

    pad_value = float(config.get("pad_value", 0.0))
    print(f"[INFO] config pad_value = {pad_value}")

    return model, config, feature_keys, threshold, pad_value


# ---------------------------------------------------------------------
# Feature weights ë¡œë”© (train ìŠ¤í¬ë¦½íŠ¸ì™€ ì¼ê´€ë˜ê²Œ)
# ---------------------------------------------------------------------
def load_feature_weights(
    config: Dict[str, Any],
    feature_keys: List[str],
    model_dir: Path,
    cli_path: Optional[str] = None,
) -> np.ndarray:
    """
    feature_weights.txt ë¥¼ ì°¾ì•„ì„œ feature ìˆœì„œì— ë§ëŠ” weight ë²¡í„°(D,) ìƒì„±.
    ìš°ì„ ìˆœìœ„:
      1) --feature-weights-file (CLIì—ì„œ ì§ì ‘ ì§€ì •)
      2) config["feature_weights_file"] ê²½ë¡œ (ì—¬ëŸ¬ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •)
      3) ì•„ë¬´ê²ƒë„ ëª» ì°¾ìœ¼ë©´ ì „ì²´ 1.0
    """
    feature_weights = np.ones(len(feature_keys), dtype=np.float32)

    # 1) í›„ë³´ ê²½ë¡œë“¤ ëª¨ìœ¼ê¸°
    candidates: List[Path] = []

    # (1) CLIë¡œ ì§ì ‘ ì§€ì •ëœ ê²½ë¡œ
    if cli_path:
        candidates.append(Path(cli_path))

    # (2) config ì— ì €ì¥ëœ ê²½ë¡œ
    fw_cfg = config.get("feature_weights_file")
    if fw_cfg:
        p_cfg = Path(fw_cfg)
        candidates.append(p_cfg)

        # ìƒëŒ€ê²½ë¡œì¼ ê²½ìš° ëª‡ ê°€ì§€ heuristic ì‹œë„
        if not p_cfg.is_absolute():
            # 2-1) model_dir ê¸°ì¤€
            candidates.append((model_dir / p_cfg).resolve())

            # 2-2) input_jsonl ê¸°ì¤€ìœ¼ë¡œ train root ì¶”ì • â†’ train/data/feature_weights.txt
            in_path = config.get("input_jsonl")
            if in_path:
                in_path = Path(in_path)
                train_root = in_path.parent.parent  # .../train/result â†’ .. â†’ .../train
                candidates.append((train_root / "data" / p_cfg.name).resolve())

    # 2) ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì²« ë²ˆì§¸ ê²½ë¡œ ì„ íƒ
    fw_path: Optional[Path] = None
    for c in candidates:
        try:
            if c.exists():
                fw_path = c
                break
        except Exception:
            continue

    if fw_path is None:
        print("[INFO] feature_weights íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. â†’ ëª¨ë“  feature ê°€ì¤‘ì¹˜ 1.0 ì‚¬ìš©")
        return feature_weights

    print(f"[INFO] feature_weights íŒŒì¼ ì‚¬ìš©: {fw_path}")

    # 3) íŒŒì¼ íŒŒì‹± (train ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ í˜•ì‹: "feature_name weight")
    weight_map: Dict[str, float] = {}
    try:
        with fw_path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                parts = line.split()
                if len(parts) < 2:
                    print(f"[WARN] ì˜ëª»ëœ weight ë¼ì¸(ë¬´ì‹œ): {line}")
                    continue
                name = parts[0]
                try:
                    w = float(parts[1])
                except ValueError:
                    print(f"[WARN] weight íŒŒì‹± ì‹¤íŒ¨(ë¬´ì‹œ): {line}")
                    continue
                weight_map[name] = w
    except Exception as e:
        print(f"[WARN] feature_weights íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return feature_weights

    # 4) feature_keys ìˆœì„œì— ë§ì¶° ë²¡í„° ì±„ìš°ê¸°
    for i, k in enumerate(feature_keys):
        if k in weight_map:
            feature_weights[i] = weight_map[k]

    print("[INFO] feature-wise weights (ì• 10ê°œ):")
    for k, w in list(zip(feature_keys, feature_weights))[:10]:
        print(f"  - {k:25s}: {w}")

    return feature_weights


# ---------------------------------------------------------------------
# Window reconstruction error (train ì½”ë“œì™€ ë™ì¼)
# ---------------------------------------------------------------------
def compute_window_errors(
    X_true: np.ndarray,
    X_pred: np.ndarray,
    pad_value: float,
    feature_weights: np.ndarray | None = None,
) -> np.ndarray:
    # íŒ¨ë”©ì´ ì•„ë‹Œ timestep ë§ˆìŠ¤í¬ (N, T)
    not_pad = np.any(np.not_equal(X_true, pad_value), axis=-1)
    mask = not_pad.astype(np.float32)

    # íƒ€ì„ìŠ¤í…ë³„ SE (N, T, D)
    se = (X_pred - X_true) ** 2  # (N, T, D)

    if feature_weights is not None:
        se = se * feature_weights[np.newaxis, np.newaxis, :]  # (N, T, D)

    # feature í‰ê·  â†’ (N, T)
    se = np.mean(se, axis=-1)  # (N, T)

    se_masked = se * mask

    denom = np.sum(mask, axis=-1) + 1e-8
    errors = np.sum(se_masked, axis=-1) / denom
    return errors

def _contiguous_segments(xs: np.ndarray, mask: np.ndarray):
    """
    xsëŠ” ì •ë ¬ëœ xì¶• ê°’(ì •ìˆ˜), maskëŠ” ë™ì¼ ê¸¸ì´ bool.
    Trueê°€ ì—°ì†ë˜ëŠ” êµ¬ê°„ì„ [(x_start, x_end)]ë¡œ ë°˜í™˜ (x_end í¬í•¨).
    """
    segs = []
    start = None
    for i in range(len(xs)):
        if mask[i] and start is None:
            start = xs[i]
        if (not mask[i]) and start is not None:
            segs.append((start, xs[i - 1]))
            start = None
    if start is not None:
        segs.append((start, xs[-1]))
    return segs


def save_recon_error_plot(
    meta_list: List[Dict[str, Any]],
    mse_per_window: np.ndarray,
    out_png: Path,
    threshold: Optional[float] = None,
    y_true_for_plot: Optional[np.ndarray] = None,
    smooth_window: int = 31,          # âœ… ìŠ¤ë¬´ë”© ìœˆë„ìš° (ëˆˆ í”¼ë¡œ ì¤„ì„)
    plot_points: str = "exceed",      # "none" | "exceed" | "all"
    point_stride: int = 10,           # âœ… ì ì„ ì°ë”ë¼ë„ ë“¬ì„±ë“¬ì„±
):
    # xì¶•: window_indexê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 0..N-1
    xs = []
    for i, m in enumerate(meta_list):
        try:
            xs.append(int(m.get("window_index", i)))
        except Exception:
            xs.append(i)

    xs_arr = np.asarray(xs, dtype=int)
    ys = np.asarray(mse_per_window, dtype=float)

    # window_indexê°€ ì„ì˜€ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì •ë ¬
    order = np.argsort(xs_arr)
    xs_arr = xs_arr[order]
    ys = ys[order]

    fig, ax = plt.subplots(figsize=(12, 4), dpi=150)

    # 1) GTê°€ ìˆìœ¼ë©´ ê³µê²© êµ¬ê°„ì„ "ë°°ê²½ ìŒì˜"ìœ¼ë¡œ í‘œì‹œ (ì  ìƒ‰ì¹ ë³´ë‹¤ í›¨ì”¬ ëœ í”¼ê³¤)
    if y_true_for_plot is not None:
        y_true_sorted = np.asarray(y_true_for_plot, dtype=int)[order]
        attack_mask = (y_true_sorted == 1)

        # ê³µê²© êµ¬ê°„ ìŒì˜
        for (x0, x1) in _contiguous_segments(xs_arr, attack_mask):
            ax.axvspan(x0, x1, alpha=0.10)

        # ë²”ë¡€ìš© íŒ¨ì¹˜
        attack_patch = Patch(alpha=0.10, label="Attack region (GT=1)")
    else:
        attack_patch = None

    # 2) MSE ì›ë³¸ ì„ (ì–‡ê²Œ, íšŒìƒ‰)
    ax.plot(xs_arr, ys, linewidth=0.8, color="0.5", label="MSE")

    # 3) ìŠ¤ë¬´ë”© ì„ (êµµê²Œ) - ì¶”ì„¸ê°€ í›¨ì”¬ ì˜ ë³´ì„
    if smooth_window and smooth_window > 1:
        ys_smooth = pd.Series(ys).rolling(smooth_window, center=True, min_periods=1).mean().to_numpy()
        ax.plot(xs_arr, ys_smooth, linewidth=2.0, color="0.15", label=f"Smoothed (w={smooth_window})")

    # 4) threshold
    if threshold is not None:
        ax.axhline(float(threshold), linestyle="--", linewidth=1.2, color="0.2",
                   label=f"threshold={threshold:.6g}")

    # 5) ì ì€ ìµœì†Œí™”: (ê¸°ë³¸) ì„ê³„ê°’ ì´ˆê³¼ì ë§Œ + strideë¡œ ë“¬ì„±ë“¬ì„±
    if plot_points != "none":
        idx = np.arange(len(xs_arr))

        if plot_points == "exceed" and threshold is not None:
            idx = idx[ys > float(threshold)]

        if point_stride and point_stride > 1:
            idx = idx[::point_stride]

        if idx.size > 0:
            if y_true_for_plot is None:
                ax.scatter(xs_arr[idx], ys[idx], s=10, alpha=0.6, label="points")
            else:
                y_true_sorted = np.asarray(y_true_for_plot, dtype=int)[order]
                idx_n = idx[y_true_sorted[idx] == 0]
                idx_a = idx[y_true_sorted[idx] == 1]
                if idx_n.size > 0:
                    ax.scatter(xs_arr[idx_n], ys[idx_n], s=10, alpha=0.7, label="Normal points")
                if idx_a.size > 0:
                    ax.scatter(xs_arr[idx_a], ys[idx_a], s=10, alpha=0.7, label="Attack points")

    ax.set_xlabel("window_index")
    ax.set_ylabel("MSE")
    ax.set_title("Reconstruction Error (Attack region shaded)")
    ax.grid(True, alpha=0.15)

    handles, labels = ax.get_legend_handles_labels()
    if attack_patch is not None:
        handles = [attack_patch] + handles
        labels = [attack_patch.get_label()] + labels
    ax.legend(handles, labels, loc="upper left", framealpha=0.9)

    fig.tight_layout()
    out_png.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(str(out_png))
    plt.close(fig)

    print(f"[INFO] recon error plot ì €ì¥ â†’ {out_png}")


def save_roc_curve_and_points(
    y_true: np.ndarray,
    scores: np.ndarray,
    out_png: Path,
    out_csv: Optional[Path] = None,
) -> Optional[float]:
    """
    y_true: 0/1
    scores: í´ìˆ˜ë¡ ê³µê²©ì¼ ê°€ëŠ¥ì„± ë†’ì€ score (ì—¬ê¸°ì„œëŠ” mse)
    """
    try:
        from sklearn.metrics import roc_curve, auc
    except ImportError:
        print("[WARN] scikit-learn ë¯¸ì„¤ì¹˜ â†’ ROC curve ì €ì¥ ìƒëµ")
        return None

    y_true = np.asarray(y_true).astype(int)
    scores = np.asarray(scores).astype(float)

    if len(np.unique(y_true)) < 2:
        print("[WARN] y_true í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ â†’ ROC curve ê³„ì‚°/ì €ì¥ ë¶ˆê°€")
        return None

    fpr, tpr, thr = roc_curve(y_true, scores)
    roc_auc = float(auc(fpr, tpr))

    # PNG
    plt.figure()
    plt.plot(fpr, tpr, label=f"ROC (AUC={roc_auc:.4f})")
    plt.plot([0, 1], [0, 1], linestyle="--", label="Random")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    out_png.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(str(out_png), dpi=150)
    plt.close()
    print(f"[INFO] ROC curve ì €ì¥ â†’ {out_png}")

    # ì  ë°ì´í„° CSV (ì˜µì…˜)
    if out_csv is not None:
        out_csv.parent.mkdir(parents=True, exist_ok=True)
        df = pd.DataFrame({"fpr": fpr, "tpr": tpr, "threshold": thr})
        df.to_csv(out_csv, index=False, encoding="utf-8")
        print(f"[INFO] ROC points CSV ì €ì¥ â†’ {out_csv}")

    return roc_auc

# ---------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------
def main():
    args = parse_args()

    input_path = Path(args.input)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    tag = args.tag if args.tag is not None else input_path.stem
    window_size = args.window_size

    print(f"[INFO] ì…ë ¥ JSONL : {input_path}")
    print(f"[INFO] ì¶œë ¥ ë””ë ‰í† ë¦¬ : {output_dir}")
    print(f"[INFO] ì‚¬ìš© íƒœê·¸(tag) = {tag}")
    print(f"[INFO] window_size = {window_size}")
    if args.pre_dir:
        print(f"[INFO] (ì°¸ê³ ìš©) pre-dir = {args.pre_dir}")

    # -----------------------------
    # 1) DL ëª¨ë¸ / feature_keys ë¡œë“œ
    # -----------------------------
    model_dir = Path(args.model_dir)
    print(f"[INFO] DL ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_dir}")

    model, config, feature_keys, threshold_from_file, pad_value = load_model_from_dir(model_dir)

    # ğŸ”¥ feature_weights ë¡œë“œ (config + CLI ê¸°ë°˜)
    feature_weights = load_feature_weights(
        config=config,
        feature_keys=feature_keys,
        model_dir=model_dir,
        cli_path=args.feature_weights_file,
    )

    # threshold ê²°ì •: CLI > threshold.json > None
    threshold = args.threshold
    if threshold is None and threshold_from_file is not None:
        threshold = threshold_from_file
        print(f"[INFO] threshold.jsonì˜ ê°’ì„ ì‚¬ìš©: threshold={threshold}")
    elif threshold is not None:
        print(f"[INFO] CLIë¡œ ì „ë‹¬ëœ threshold ì‚¬ìš©: threshold={threshold}")
    else:
        print("[INFO] threshold ë¯¸ì§€ì • â†’ is_anomaly_pred = -1 ë¡œë§Œ ê¸°ë¡ (ROC-AUC, MSE í†µê³„ëŠ” ê°€ëŠ¥)")

    # -----------------------------
    # 2) ì…ë ¥ JSONL â†’ [N, T, D] ìœˆë„ìš° í–‰ë ¬ ìƒì„±
    # -----------------------------
    all_windows: List[np.ndarray] = []
    meta_list: List[Dict[str, Any]] = []

    D_model: int = len(feature_keys)
    print(f"[INFO] feature dimension (D_model) = {D_model}")

    with input_path.open("r", encoding="utf-8") as fin:
        for line_idx, line in enumerate(fin):
            line = line.strip()
            if not line:
                continue

            try:
                obj = json.loads(line)
            except Exception as e:
                print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨ (line {line_idx}): {e}")
                continue

            seq_group = obj.get("sequence_group", [])
            if not seq_group:
                continue

            # ìœˆë„ìš° í–‰ë ¬ ì´ˆê¸°í™”
            X_win = np.full((window_size, D_model), pad_value, dtype=np.float32)

            for t, pkt in enumerate(seq_group):
                if t >= window_size:
                    break
                for j, feat_name in enumerate(feature_keys):
                    v = pkt.get(feat_name, pad_value)
                    try:
                        X_win[t, j] = float(v)
                    except Exception:
                        X_win[t, j] = pad_value

            all_windows.append(X_win)
            meta_list.append(
                {
                    "window_index": int(obj.get("window_index", len(meta_list))),
                    "pattern": obj.get("pattern", None),
                    "valid_len": int(min(len(seq_group), window_size)),
                }
            )

    num_windows = len(all_windows)
    print(f"[INFO] ìƒì„±ëœ ìœˆë„ìš° ìˆ˜ = {num_windows}")

    if num_windows == 0:
        print("[WARN] ìƒì„±ëœ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥/íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    X_windows = np.stack(all_windows, axis=0)  # [N, T, D]
    print("[DEBUG] X_windows shape:", X_windows.shape)
    print(
        "[DEBUG] X_windows ì „ì²´ í†µê³„:",
        "min=",
        float(X_windows.min()),
        "max=",
        float(X_windows.max()),
        "mean=",
        float(X_windows.mean()),
    )

    # -----------------------------
    # 3) DL ëª¨ë¸ inference
    # -----------------------------
    N, T_cur, D_cur = X_windows.shape
    print(f"[INFO] ëª¨ë¸ ì…ë ¥ìš© X_windows shape: (N={N}, T={T_cur}, D={D_cur})")

    T_cfg = config.get("T")
    D_cfg = config.get("D")
    if T_cfg is not None and T_cfg != T_cur:
        print(f"[WARN] config.T({T_cfg}) != í˜„ì¬ window_size({T_cur})")
    if D_cfg is not None and D_cfg != D_cur:
        print(f"[WARN] config.D({D_cfg}) != í˜„ì¬ D({D_cur})")

    # ë””ë°”ì´ìŠ¤ ì •ë³´ (ì˜µì…˜)
    try:
        import tensorflow as tf

        gpus = tf.config.list_physical_devices("GPU")
        if gpus:
            print(f"[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {gpus}")
        else:
            print("[INFO] GPU ì—†ìŒ, CPU ì‚¬ìš©")
    except Exception:
        pass

    print("[INFO] DL ëª¨ë¸ë¡œ ìœˆë„ìš°ë³„ reconstruction ì˜ˆì¸¡ ì¤‘...")
    recon = model.predict(X_windows, batch_size=args.batch_size, verbose=1)

    print(
        "[DEBUG] recon ì „ì²´ í†µê³„:",
        "min=",
        float(recon.min()),
        "max=",
        float(recon.max()),
        "mean=",
        float(recon.mean()),
    )

    # ğŸ”¥ ê°€ì¤‘ì¹˜ê¹Œì§€ í¬í•¨í•œ window-level MSE ê³„ì‚°
    mse_per_window = compute_window_errors(
        X_windows,
        recon,
        pad_value,
        feature_weights=feature_weights,
    )
    print("[DEBUG] mse_per_window[:20] =", mse_per_window[:20])
    print(
        "[DEBUG] mse_per_window í†µê³„: "
        f"min={float(mse_per_window.min())}, "
        f"max={float(mse_per_window.max())}, "
        f"mean={float(mse_per_window.mean())}"
    )

    print(
        "[INFO] benchmark ìœˆë„ìš° MSE í†µê³„: "
        f"mean={mse_per_window.mean():.4f}, "
        f"std={mse_per_window.std():.4f}, "
        f"min={mse_per_window.min():.4f}, "
        f"max={mse_per_window.max():.4f}"
    )

    # -----------------------------
    # 3.5) ì¬êµ¬ì„± ì˜¤ì°¨ ì‹œê³„ì—´ ê·¸ë˜í”„ (ì •ìƒ=íŒŒë‘, ê³µê²©=ë¹¨ê°•)
    # -----------------------------
    y_true_for_plot = None

    if args.attack_csv is not None:
        attack_path = Path(args.attack_csv)
        df_gt = load_gt_table(attack_path)

        # pattern ê¸°ë°˜ is_anomaly ìƒì„± (ì—†ì„ ë•Œ ìë™ ì²˜ë¦¬)
        if "is_anomaly" not in df_gt.columns and "pattern" in df_gt.columns:
            df_gt["is_anomaly"] = df_gt["pattern"].apply(
                lambda x: 1 if "ATTACK" in str(x).strip().upper() else 0
            )

        # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
        if "window_index" in df_gt.columns and "is_anomaly" in df_gt.columns:
            label_map = build_window_label_map(df_gt)
            y_true_for_plot = np.array(
                [label_map.get(int(m.get("window_index", -1)), -1) for m in meta_list],
                dtype=int
            )
        else:
            print("[WARN] GTì— window_index/is_anomalyê°€ ì—†ì–´ ìƒ‰ì¹ ìš© ë¼ë²¨ ìƒì„± ë¶ˆê°€")

    recon_png = output_dir / f"recon_error_{tag}.png"
    save_recon_error_plot(
        meta_list,
        mse_per_window,
        recon_png,
        threshold=threshold,
        y_true_for_plot=y_true_for_plot,   # âœ… ì¶”ê°€
    )

    # ì›í•˜ë©´ per-window MSEë„ íŒŒì¼ë¡œ ì €ì¥(ë¶„ì„/ë””ë²„ê¹…ìš©)
    mse_csv = output_dir / f"mse_per_window_{tag}.csv"
    rows = []
    for m, mse in zip(meta_list, mse_per_window):
        wi = int(m.get("window_index", -1))
        rows.append({
            "window_index": wi,
            "mse": float(mse),
            "pattern": m.get("pattern", None),
            "valid_len": int(m.get("valid_len", 0)),
            "is_anomaly_pred": (int(float(mse) > float(threshold)) if threshold is not None else -1),
        })
    pd.DataFrame(rows).to_csv(mse_csv, index=False, encoding="utf-8")
    print(f"[INFO] mse_per_window CSV ì €ì¥ â†’ {mse_csv}")


    # -----------------------------
    # 4) (ì„ íƒ) GTì™€ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ ê³„ì‚°
    # -----------------------------
    if args.attack_csv is None:
        print("[INFO] --attack-csv ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
              "â†’ GT ê¸°ë°˜ íƒì§€ ì„±ëŠ¥ / MSE í†µê³„ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    attack_path = Path(args.attack_csv)
    print(f"[INFO] GT íŒŒì¼ (attack)          : {attack_path}")

    # metrics / mse-stats ì¶œë ¥ ê²½ë¡œ
    metrics_out_path = (
        Path(args.metrics_json)
        if args.metrics_json is not None
        else output_dir / f"metrics_{tag}.json"
    )
    mse_out_path = (
        Path(args.mse_stats_json)
        if args.mse_stats_json is not None
        else output_dir / f"analyze_mse_dist_{tag}.json"
    )
    print(f"[INFO] ì¶œë ¥ JSON (metrics)       : {metrics_out_path}")
    print(f"[INFO] ì¶œë ¥ JSON (MSE stats)    : {mse_out_path}")

    # GT ë¡œë“œ
    df_gt = load_gt_table(attack_path)

    # pattern ê¸°ë°˜ is_anomaly ìƒì„± (ì—†ì„ ë•Œ ìë™ ì²˜ë¦¬)
    if "is_anomaly" not in df_gt.columns and "pattern" in df_gt.columns:
        df_gt["is_anomaly"] = df_gt["pattern"].apply(
            lambda x: 1 if "ATTACK" in str(x).strip().upper() else 0
        )
        print(
            f"[INFO] 'pattern' ì»¬ëŸ¼ì—ì„œ is_anomaly ìë™ ìƒì„± ì™„ë£Œ "
            f"({df_gt['is_anomaly'].sum()}ê°œ ê³µê²© ìœˆë„ìš°)"
        )

    # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
    if "window_index" not in df_gt.columns or "is_anomaly" not in df_gt.columns:
        raise ValueError(
            f"GT íŒŒì¼ì— 'window_index' ë˜ëŠ” 'is_anomaly' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ({attack_path})"
        )

    # Pred DataFrame ìƒì„± (ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°”ë¡œ ë§Œë“  MSE / is_anomaly_pred ì‚¬ìš©)
    pred_rows: List[Dict[str, Any]] = []
    for m, mse in zip(meta_list, mse_per_window):
        if threshold is not None:
            is_anom = int(mse > threshold)
        else:
            is_anom = -1  # threshold ì—†ìœ¼ë©´ ë¼ë²¨ ì—†ìŒ
        pred_rows.append(
            {
                "window_index": m["window_index"],
                "mse": float(mse),
                "is_anomaly": is_anom,
            }
        )
    df_pred = pd.DataFrame(pred_rows)

    # window_index ê¸°ì¤€ inner join
    has_mse = "mse" in df_pred.columns
    pred_cols = ["window_index", "is_anomaly"]
    if has_mse:
        pred_cols.append("mse")

    merged = pd.merge(
        df_gt[["window_index", "is_anomaly"]],
        df_pred[pred_cols],
        on="window_index",
        how="inner",
        suffixes=("_true", "_pred"),
    )

    print(f"[INFO] join í›„ í–‰ ìˆ˜: {len(merged)}")

    # íƒ€ì… ì •ë¦¬
    merged["is_anomaly_true"] = merged["is_anomaly_true"].astype(int)
    merged["is_anomaly_pred"] = merged["is_anomaly_pred"].astype(int)

    # is_anomaly_pred == -1 (ë¯¸ë¼ë²¨) ì œê±° ì˜µì…˜
    if args.ignore_pred_minus1:
        before = len(merged)
        merged = merged[merged["is_anomaly_pred"] != -1].copy()
        after = len(merged)
        print(f"[INFO] is_anomaly_pred == -1 ì œê±°: {before} -> {after}")

    if len(merged) == 0:
        print("[WARN] í‰ê°€ì— ì‚¬ìš©í•  ìœˆë„ìš°ê°€ 0ê°œì…ë‹ˆë‹¤. (join ê²°ê³¼ ë˜ëŠ” ignore-pred-minus1 ì˜í–¥)")
        return

    y_true = merged["is_anomaly_true"].to_numpy()
    y_pred = merged["is_anomaly_pred"].to_numpy()

    # í˜¼ë™í–‰ë ¬ ìš”ì†Œ ê³„ì‚° (y_pred ì— -1 ì´ ë‚¨ì•„ ìˆìœ¼ë©´ 0ìœ¼ë¡œ ì·¨ê¸‰)
    y_pred_bin = np.where(y_pred <= 0, 0, 1)

    tp = int(np.sum((y_true == 1) & (y_pred_bin == 1)))
    tn = int(np.sum((y_true == 0) & (y_pred_bin == 0)))
    fp = int(np.sum((y_true == 0) & (y_pred_bin == 1)))
    fn = int(np.sum((y_true == 1) & (y_pred_bin == 0)))

    total = tp + tn + fp + fn

    accuracy = safe_div(tp + tn, total)
    precision = safe_div(tp, tp + fp)
    recall = safe_div(tp, tp + fn)  # TPR
    f1 = safe_div(2 * precision * recall, precision + recall)

    tpr = recall  # same
    fpr = safe_div(fp, fp + tn)
    tnr = safe_div(tn, tn + fp)
    fnr = safe_div(fn, fn + tp)

    # ì¶”ê°€ ì§€í‘œ: prevalence, predicted positive rate, balanced accuracy
    prevalence = safe_div(tp + fn, total)  # ì‹¤ì œ ê³µê²© ë¹„ìœ¨
    pred_positive_rate = safe_div(tp + fp, total)  # ëª¨ë¸ì´ ê³µê²©ì´ë¼ ë•Œë¦° ë¹„ìœ¨
    balanced_accuracy = 0.5 * (tpr + tnr)

    # AUC + MSE í†µê³„ ê³„ì‚° (mseê°€ ìˆì„ ë•Œë§Œ ì‹œë„)
    auc_dict: Dict[str, Optional[float]] = {"roc_auc": None, "pr_auc": None}
    mse_stats: Optional[Dict[str, Any]] = None

    if has_mse:
        scores = merged["mse"].to_numpy(dtype=float)
        auc_dict = try_compute_auc(y_true, scores)
        mse_stats = compute_mse_stats(y_true, scores)
        roc_png = output_dir / f"roc_curve_{tag}.png"
        roc_csv = output_dir / f"roc_points_{tag}.csv"
        _ = save_roc_curve_and_points(y_true, scores, roc_png, out_csv=roc_csv)

    else:
        print("[INFO] 'mse' ì»¬ëŸ¼ì´ ì—†ì–´ ROC-AUC / PR-AUC / MSE í†µê³„ ê³„ì‚°ì„ ìƒëµí•©ë‹ˆë‹¤.")

    metrics: Dict[str, Any] = {
        "num_samples": int(total),
        "confusion_matrix": {
            "TP": tp,
            "TN": tn,
            "FP": fp,
            "FN": fn,
        },
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "tpr": tpr,
        "fpr": fpr,
        "tnr": tnr,
        "fnr": fnr,
        "prevalence": prevalence,
        "pred_positive_rate": pred_positive_rate,
        "balanced_accuracy": balanced_accuracy,
        "roc_auc": auc_dict["roc_auc"],
        "pr_auc": auc_dict["pr_auc"],
    }

    print("===== Detection Metrics =====")
    print(f"Samples (windows)       : {total}")
    print(f"TP={tp}, FP={fp}, FN={fn}, TN={tn}")
    print(f"Accuracy                : {accuracy:.4f}")
    print(f"Precision               : {precision:.4f}")
    print(f"Recall (TPR)            : {recall:.4f}")
    print(f"F1-score                : {f1:.4f}")
    print(f"FPR                     : {fpr:.6f}")
    print(f"TNR (Specificity)       : {tnr:.4f}")
    print(f"FNR                     : {fnr:.4f}")
    print(f"Prevalence (Attack rate): {prevalence:.6f}")
    print(f"Pred Positive Rate      : {pred_positive_rate:.6f}")
    print(f"Balanced Accuracy       : {balanced_accuracy:.4f}")
    if metrics["roc_auc"] is not None:
        print(f"ROC-AUC                 : {metrics['roc_auc']:.4f}")
    if metrics["pr_auc"] is not None:
        print(f"PR-AUC                  : {metrics['pr_auc']:.4f}")

    # metrics JSON ì €ì¥
    metrics_out_path.parent.mkdir(parents=True, exist_ok=True)
    with metrics_out_path.open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    print(f"[INFO] ì„±ëŠ¥ ì§€í‘œ JSON ì €ì¥ ì™„ë£Œ â†’ {metrics_out_path}")

    # MSE í†µê³„ JSON ë³„ë„ ì €ì¥
    if mse_stats is not None:
        mse_out_path.parent.mkdir(parents=True, exist_ok=True)
        with mse_out_path.open("w", encoding="utf-8") as f:
            json.dump(mse_stats, f, indent=2, ensure_ascii=False)
        print(f"[INFO] MSE í†µê³„ JSON ì €ì¥ ì™„ë£Œ â†’ {mse_out_path}")


if __name__ == "__main__":
    main()

"""
python benchmark_and_eval.py \
  --input ../data/attack_windows.jsonl \
  --pre-dir ../preprocessing/result \
  --window-size 80 \
  --output-dir ../result/benchmark \
  --model-dir ../DL/result_train/data \
  --batch-size 128 \
  --attack-csv ../result/attack_result.csv \
  --ignore-pred-minus1
"""

"""
python benchmark_and_eval.py \
  --input ../data/attack_windows.jsonl \
  --window-size 16 \
  --output-dir ../result/benchmark \
  --model-dir ../../result_train/data \
  --batch-size 128 \
  --attack-csv ../result/attack_result.csv \
  --ignore-pred-minus1 \
  --feature-weights-file "../train/data/feature_weights.txt"
"""
