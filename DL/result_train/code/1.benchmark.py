#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
benchmark.py

íŒ¨í‚· ë‹¨ìœ„ JSONL (ê° lineì´ í•˜ë‚˜ì˜ íŒ¨í‚·) â†’
ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° + packet_feature_extractor ë¥¼ ì´ìš©í•´ì„œ

1) íŒ¨í‚· window_sizeê°œì”© ìˆœì„œëŒ€ë¡œ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ window(sequence_group)ë¡œ ë³´ê³ 
2) í•´ë‹¹ windowë¥¼ feature matrix (shape = [window_size, feat_dim]) ë¡œ ë³€í™˜
3) ëª¨ë“  windowë¥¼ ëª¨ì•„ì„œ X_windows.npy ë¡œ ì €ì¥
4) window ë©”íƒ€ì •ë³´ëŠ” windows_meta.jsonl ë¡œ ì €ì¥
5) (ì„ íƒ) í•™ìŠµëœ LSTM-AE(Keras) ëª¨ë¸ì„ ë¶ˆëŸ¬ì„œ ìœˆë„ìš°ë³„ reconstruction MSE ê³„ì‚° + anomaly ì—¬ë¶€ ê¸°ë¡

âš ï¸ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°:
   start = 0, step_size, 2*step_size, ...
   ì˜ˆ: window_size=80, step_size=40 ì´ë©´
       [0~79], [40~119], [80~159], ...
"""

import argparse
import json
import csv
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
from datetime import datetime

# ê¸°ì¡´ì— ë§Œë“¤ì–´ë‘” ëª¨ë“ˆ ì¬ì‚¬ìš©
from packet_feature_extractor import (
    load_preprocess_params,
    sequence_group_to_feature_matrix,
    PACKET_FEATURE_COLUMNS,
)


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()

    p.add_argument(
        "--input", "-i", required=True,
        help="íŒ¨í‚· ë‹¨ìœ„ JSONL ê²½ë¡œ (ê° line = 1 packet)",
    )
    p.add_argument(
        "--pre-dir", "-p", required=True,
        help="ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° JSONë“¤ì´ ëª¨ì—¬ìˆëŠ” ë””ë ‰í† ë¦¬",
    )
    p.add_argument(
        "--window-size", "-w", type=int, default=80,
        help="ìœˆë„ìš° ê¸¸ì´(ë¬¶ì„ íŒ¨í‚· ê°œìˆ˜, ê¸°ë³¸=80)",
    )
    p.add_argument(
        "--step-size", "-s", type=int, default=None,
        help="ìŠ¬ë¼ì´ë”© stride (ê¸°ë³¸: window-sizeì™€ ë™ì¼ â†’ non-overlap)",
    )
    p.add_argument(
        "--output-dir", "-o", required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (X_windows.npy, windows_meta.jsonl, window_scores.csv)",
    )
    p.add_argument(
        "--no-pad-last", action="store_true",
        help="ë§ˆì§€ë§‰ ìœˆë„ìš°ê°€ window-size ë³´ë‹¤ ì§§ìœ¼ë©´ 0 íŒ¨ë”© ì—†ì´ ë²„ë¦¼",
    )

    # â¬‡ï¸ DL ëª¨ë¸ inference ê´€ë ¨ ì˜µì…˜ (model_dir ë°©ì‹)
    p.add_argument(
        "--model-dir", "-m", default=None,
        help="train_lstm_ae_windows_keras.py ê²°ê³¼ ë””ë ‰í† ë¦¬ (model.h5, config.json ë“±)",
    )
    p.add_argument(
        "--batch-size", "-b", type=int, default=128,
        help="DL ëª¨ë¸ inference batch size (ê¸°ë³¸=128)",
    )
    p.add_argument(
        "--threshold", "-t", type=float, default=None,
        help=(
            "ìœˆë„ìš° MSE anomaly threshold "
            "(ë¯¸ì§€ì • ì‹œ threshold.jsonì´ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ì‚¬ìš©, ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì ìˆ˜ë§Œ ê¸°ë¡)"
        ),
    )
    p.add_argument(
        "--tag", default=None,
        help="ì¶œë ¥ íŒŒì¼ ì´ë¦„ì— ë¶™ì¼ íƒœê·¸ (ê¸°ë³¸: ì…ë ¥ JSONL íŒŒì¼ëª… stem)",
    )

    
    return p.parse_args()


# -----------------------------
# timestamp â†’ delta_t ê³„ì‚°
# -----------------------------
def parse_timestamp(ts: Any) -> float:
    """
    "@timestamp": "2025-11-10T17:43:40.425Z" í˜•íƒœë¥¼ float epoch ë¡œ ë³€í™˜.
    ì‹¤íŒ¨í•˜ë©´ 0.0 ë¦¬í„´.
    """
    if not ts:
        return 0.0
    if isinstance(ts, (int, float)):
        return float(ts)
    ts_str = str(ts)
    try:
        if ts_str.endswith("Z"):
            ts_str = ts_str[:-1]
        dt = datetime.fromisoformat(ts_str)
        return dt.timestamp()
    except Exception:
        return 0.0


# -----------------------------
# train ì½”ë“œì™€ ë™ì¼í•œ window MSE ê³„ì‚°
# -----------------------------
def compute_window_errors(
    X_true: np.ndarray,
    X_pred: np.ndarray,
    pad_value: float,
) -> np.ndarray:
    """
    X_true, X_pred: shape (N, T, D)
    pad_value    : íŒ¨ë”© ê°’ (í•´ë‹¹ timestepì€ ë§ˆìŠ¤í¬)

    ë°˜í™˜:
      errors: shape (N,), ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨
    """
    # íŒ¨ë”©ì´ ì•„ë‹Œ timestep ë§ˆìŠ¤í¬ (N, T)
    not_pad = np.any(np.not_equal(X_true, pad_value), axis=-1)
    mask = not_pad.astype(np.float32)

    # íƒ€ì„ìŠ¤í…ë³„ MSE (N, T)
    se = np.mean((X_pred - X_true) ** 2, axis=-1)
    se_masked = se * mask

    denom = np.sum(mask, axis=-1) + 1e-8
    errors = np.sum(se_masked, axis=-1) / denom
    return errors


# -----------------------------
# ëª¨ë¸ ë¡œë”© (benchmark_lstm_ae_inference ìŠ¤íƒ€ì¼)
# -----------------------------
def load_first_packet(jsonl_path: Path) -> Dict[str, Any]:
    with jsonl_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue
            return obj
    raise RuntimeError("JSONLì—ì„œ ìœ íš¨í•œ íŒ¨í‚·ì„ í•˜ë‚˜ë„ ëª» ì½ì—ˆìŒ")


def load_model_from_dir(model_dir: Path):
    """
    model_dir ë‚´ë¶€:
      - config.json : {"T": ..., "D": ..., "pad_value": ...} í¬í•¨
      - model.h5    : Keras LSTM-AE ëª¨ë¸
      - threshold.json (ì„ íƒ)

    ë°˜í™˜:
      model, config(dict), threshold(float ë˜ëŠ” None)
    """
    config_path = model_dir / "config.json"
    if not config_path.exists():
        raise FileNotFoundError(f"âŒ config.json ì—†ìŒ: {config_path}")

    with config_path.open("r", encoding="utf-8") as f:
        config = json.load(f)

    print(f"[INFO] config ë¡œë“œ: {config_path}")
    T = config.get("T")
    D = config.get("D")
    print(f"[INFO] í•™ìŠµ ì‹œ T={T}, D={D}")

    import tensorflow as tf  # noqa: F401
    from tensorflow.keras.models import load_model

    model_path = model_dir / "model.h5"
    if not model_path.exists():
        raise FileNotFoundError(f"âŒ model.h5 ì—†ìŒ: {model_path}")

    print(f"[INFO] model ë¡œë“œ: {model_path}")

    # ğŸ”¥ config.json ì—ì„œ T, D ì½ê¸° (decoderì—ì„œ ë°˜ë³µí•  ê¸¸ì´)
    T = config.get("T")
    D = config.get("D")

    def repeat_latent(x):
        """
        train_lstm_ae_windows_keras.py ì—ì„œ ì¼ë˜ repeat_latentì™€ ë™ì¼í•œ ë™ì‘:
        (B, latent_dim) -> (B, T, latent_dim)
        """
        x = tf.expand_dims(x, axis=1)      # (B, 1, latent_dim)
        x = tf.tile(x, [1, T, 1])          # (B, T, latent_dim)
        return x

    custom_objects = {
        "repeat_latent": repeat_latent,
    }

    model = load_model(
        model_path,
        compile=False,
        custom_objects=custom_objects,
    )

    # ---------------------------
    # threshold.json ì½ê¸° (êµ¬ì¡° ë§ê²Œ)
    # ---------------------------
    thresh_path = model_dir / "threshold.json"
    threshold = None
    if thresh_path.exists():
        try:
            with thresh_path.open("r", encoding="utf-8") as f:
                th_cfg = json.load(f)

            # train ì½”ë“œì—ì„œ ì €ì¥í•œ êµ¬ì¡°:
            # {
            #   "threshold_p99": ...,
            #   "threshold_mu3": ...,
            #   "stats": {...}
            # }
            if "threshold" in th_cfg:
                # í˜¹ì‹œ ë‚˜ì¤‘ì— ë‹¨ì¼ thresholdë¡œ ì €ì¥í•´ë‘” ë²„ì „
                threshold = float(th_cfg["threshold"])
                print(f"[INFO] threshold.json(threshold) ì‚¬ìš©: {threshold}")
            elif "threshold_p99" in th_cfg:
                threshold = float(th_cfg["threshold_p99"])
                print(f"[INFO] threshold_p99 ì‚¬ìš©: {threshold}")
            elif "threshold_mu3" in th_cfg:
                threshold = float(th_cfg["threshold_mu3"])
                print(f"[INFO] threshold_mu3 ì‚¬ìš©: {threshold}")
            else:
                print(
                    "[INFO] threshold.json ì•ˆì— ì‚¬ìš©í•  í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. "
                    "(threshold / threshold_p99 / threshold_mu3 ì—†ìŒ)"
                )
        except Exception as e:
            print(f"[WARN] threshold.json ë¡œë”© ì‹¤íŒ¨: {e}")

    # feature_keys.txt ìˆìœ¼ë©´ ê¸¸ì´ í™•ì¸ ìš©ë„ë¡œë§Œ ì‚¬ìš©
    feat_path = model_dir / "feature_keys.txt"
    if feat_path.exists():
        try:
            with feat_path.open("r", encoding="utf-8") as f:
                feature_keys = [line.strip() for line in f if line.strip()]
            print(f"[INFO] feature_keys.txt ê¸¸ì´ = {len(feature_keys)}")
        except Exception as e:
            print(f"[WARN] feature_keys.txt ë¡œë”© ì‹¤íŒ¨: {e}")

    return model, config, threshold


def main():
    args = parse_args()

    input_path = Path(args.input)
    pre_dir = Path(args.pre_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    tag = args.tag if args.tag is not None else input_path.stem

    window_size = args.window_size
    step_size = args.step_size if args.step_size is not None else window_size
    pad_last = not args.no_pad_last

    print(f"[INFO] ì…ë ¥ JSONL : {input_path}")
    print(f"[INFO] ì „ì²˜ë¦¬ ë””ë ‰í† ë¦¬ : {pre_dir}")
    print(f"[INFO] ì¶œë ¥ ë””ë ‰í† ë¦¬ : {output_dir}")
    print(f"[INFO] ì‚¬ìš© íƒœê·¸(tag) = {tag}")
    print(f"[INFO] window_size = {window_size}, step_size = {step_size}, pad_last = {pad_last}")

    # 1) ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ë¡œë”© (packet_feature_extractor ë‚´ë¶€ ì •ì˜ì— ë§ê²Œ)
    print("[INFO] ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ë¡œë”© ì¤‘...")
    params = load_preprocess_params(pre_dir)
    feat_dim = len(PACKET_FEATURE_COLUMNS)
    print(f"[INFO] feature dimension = {feat_dim} (PACKET_FEATURE_COLUMNS ê¸¸ì´)")

    # 2) JSONL ì „ì²´ë¥¼ ì½ì–´ì„œ delta_t í¬í•¨í•œ packet ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    all_packets: List[Dict[str, Any]] = []
    prev_ts = None

    with input_path.open("r", encoding="utf-8") as fin:
        for line in fin:
            line = line.strip()
            if not line:
                continue
            try:
                pkt = json.loads(line)
            except Exception as e:
                print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue

            # delta_t ê³„ì‚°
            ts_val = parse_timestamp(pkt.get("@timestamp"))
            if prev_ts is None:
                delta_t = 0.0
            else:
                dt = ts_val - prev_ts
                delta_t = dt if dt >= 0 else 0.0
            prev_ts = ts_val

            pkt["delta_t"] = delta_t  # feature extractor ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë„£ì–´ì¤Œ
            all_packets.append(pkt)

    total_packets = len(all_packets)
    print(f"[INFO] ì´ íŒ¨í‚· ìˆ˜ = {total_packets}")

    if total_packets == 0:
        print("[WARN] ì…ë ¥ JSONLì—ì„œ ìœ íš¨í•œ íŒ¨í‚·ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
    all_windows: List[np.ndarray] = []
    meta_list: List[Dict[str, Any]] = []

    start_idx = 0
    window_index = 0

    while start_idx < total_packets:
        end_idx = start_idx + window_size
        group = all_packets[start_idx:end_idx]

        if len(group) == 0:
            break

        # ë§ˆì§€ë§‰ ìœˆë„ìš°ê°€ window_sizeë³´ë‹¤ ì§§ê³  pad_lastê°€ Falseë©´ ì¤‘ë‹¨
        if len(group) < window_size and not pad_last:
            print(f"[INFO] ë§ˆì§€ë§‰ window({start_idx}~{total_packets-1})ëŠ” ê¸¸ì´ {len(group)} < {window_size}, íŒ¨ë”© ì—†ì´ ë²„ë¦¼")
            break

        X_list = sequence_group_to_feature_matrix(group, params)
        if not X_list:
            print(f"[WARN] window({start_idx}~{min(end_idx, total_packets)-1})ì—ì„œ feature ì¶”ì¶œ ì‹¤íŒ¨, ìŠ¤í‚±")
            start_idx += step_size
            continue

        X = np.array(X_list, dtype="float32")
        valid_len = X.shape[0]

        if valid_len < window_size:
            if pad_last:
                pad_len = window_size - valid_len
                pad_block = np.zeros((pad_len, feat_dim), dtype=X.dtype)
                X = np.concatenate([X, pad_block], axis=0)
        elif valid_len > window_size:
            X = X[:window_size, :]
            valid_len = window_size

        all_windows.append(X)
        meta_list.append({
            "window_index": window_index,
            "start_packet_idx": start_idx,
            "end_packet_idx": min(end_idx, total_packets) - 1,
            "valid_len": int(valid_len),
        })

        window_index += 1
        start_idx += step_size

    num_windows = len(all_windows)
    print(f"[INFO] ìƒì„±ëœ ìœˆë„ìš° ìˆ˜ = {num_windows}")

    if num_windows == 0:
        print("[WARN] ìƒì„±ëœ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥/íŒŒë¼ë¯¸í„°/step_sizeë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # 4) numpy ì €ì¥
    X_windows = np.stack(all_windows, axis=0)  # [num_windows, window_size, feat_dim]
    
    # protocol_norm
    X_windows = normalize_protocol_column(
        X_windows,
        min_code=0.0,
        max_code=8.0,   # PROTOCOL_MAP ìµœëŒ“ê°’(=dns) ê¸°ì¤€
        col_name="protocol",
    )
    out_npy = output_dir / f"X_windows_{tag}.npy"
    np.save(out_npy, X_windows)
    print(f"[INFO] ì €ì¥ ì™„ë£Œ: {out_npy} (shape={X_windows.shape})")

    # 5) ë©”íƒ€ì •ë³´ JSONL ì €ì¥
    out_meta = output_dir / f"windows_meta_{tag}.jsonl"
    with out_meta.open("w", encoding="utf-8") as fmeta:
        for m in meta_list:
            fmeta.write(json.dumps(m, ensure_ascii=False) + "\n")
    print(f"[INFO] ë©”íƒ€ ì •ë³´ ì €ì¥ ì™„ë£Œ: {out_meta}")


    # ============================
    # 6) DL ëª¨ë¸ ë¶ˆëŸ¬ì„œ íƒì§€ (ì„ íƒ)
    # ============================
    if args.model_dir:
        model_dir = Path(args.model_dir)
        print(f"[INFO] DL ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_dir}")

        model, config, threshold_from_file = load_model_from_dir(model_dir)

        # ---------------------------
        # 1) feature_keys.txt ì½ê¸°
        # ---------------------------
        feat_path = model_dir / "feature_keys.txt"
        if not feat_path.exists():
            raise FileNotFoundError(f"âŒ feature_keys.txt ì—†ìŒ: {feat_path}")

        with feat_path.open("r", encoding="utf-8") as f:
            feature_keys = [line.strip() for line in f if line.strip()]

        print(f"[INFO] feature_keys.txt ë¡œë“œ, ê¸¸ì´ = {len(feature_keys)}")

        # PACKET_FEATURE_COLUMNS ê¸°ì¤€ìœ¼ë¡œ ì´ë¦„ â†’ ì¸ë±ìŠ¤ ë§¤í•‘
        col_index = {name: idx for idx, name in enumerate(PACKET_FEATURE_COLUMNS)}

        # ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” feature_keys ìˆœì„œëŒ€ë¡œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
        model_col_indices = []
        missing_keys = []
        for k in feature_keys:
            if k in col_index:
                model_col_indices.append(col_index[k])
            else:
                missing_keys.append(k)

        if missing_keys:
            print("[WARN] benchmarkì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” feature key:", missing_keys)
            print("       â†’ í•´ë‹¹ featureëŠ” pad_valueë¡œ ì±„ì›ë‹ˆë‹¤.")

        # ---------------------------
        # 2) X_windows â†’ X_model ë¡œ ë³€í™˜
        #    (í•™ìŠµ ë•Œ ì‚¬ìš©í•œ feature ì§‘í•©/ìˆœì„œë¡œ ë§ì¶”ê¸°)
        # ---------------------------
        N, T_cur, D_cur = X_windows.shape
        print(f"[INFO] ì›ë³¸ X_windows shape: (N={N}, T={T_cur}, D={D_cur})")

        T_cfg = config.get("T")
        D_cfg = config.get("D")
        pad_value = float(config.get("pad_value", 0.0))

        if T_cfg is not None and T_cfg != T_cur:
            print(f"[WARN] config.T({T_cfg}) != í˜„ì¬ window_size({T_cur})")
        if D_cfg is not None and D_cfg != len(feature_keys):
            print(f"[WARN] config.D({D_cfg}) != feature_keys ê¸¸ì´({len(feature_keys)})")

        # ëª¨ë¸ ì…ë ¥ ì°¨ì› = í•™ìŠµì—ì„œ ì‚¬ìš©í•œ feature ê°œìˆ˜
        D_model = len(feature_keys)
        X_model = np.zeros((N, T_cur, D_model), dtype=X_windows.dtype)

        for j, k in enumerate(feature_keys):
            if k in col_index:
                src_idx = col_index[k]
                X_model[:, :, j] = X_windows[:, :, src_idx]
            else:
                # í•™ìŠµ featureì¸ë° í˜„ì¬ benchmarkì—ëŠ” ì—†ëŠ” ê²½ìš° â†’ pad_valueë¡œ ì±„ì›€
                X_model[:, :, j] = pad_value

        print(f"[INFO] ëª¨ë¸ ì…ë ¥ìš© X_model shape: {X_model.shape}")

        # threshold ê²°ì •: CLI > threshold.json > None
        threshold = args.threshold
        if threshold is None and threshold_from_file is not None:
            threshold = threshold_from_file
            print(f"[INFO] threshold.jsonì˜ ê°’ì„ ì‚¬ìš©: threshold={threshold}")
        elif threshold is not None:
            print(f"[INFO] CLIë¡œ ì „ë‹¬ëœ threshold ì‚¬ìš©: threshold={threshold}")
        else:
            print("[INFO] threshold ë¯¸ì§€ì • â†’ anomaly ë¼ë²¨ë§ ì—†ì´ MSEë§Œ ê¸°ë¡")

        # ë””ë°”ì´ìŠ¤ ì •ë³´ (ì˜µì…˜)
        try:
            import tensorflow as tf
            gpus = tf.config.list_physical_devices("GPU")
            if gpus:
                print(f"[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {gpus}")
            else:
                print("[INFO] GPU ì—†ìŒ, CPU ì‚¬ìš©")
        except Exception:
            pass

        print("[INFO] DL ëª¨ë¸ë¡œ ìœˆë„ìš°ë³„ reconstruction ì˜ˆì¸¡ ì¤‘...")
        recon = model.predict(X_model, batch_size=args.batch_size, verbose=1)

        if recon.shape != X_model.shape:
            print(f"[WARN] ì¬êµ¬ì„± ê²°ê³¼ shape {recon.shape} != ì…ë ¥ shape {X_model.shape}")
            print("       ë¸Œë¡œë“œìºìŠ¤íŠ¸ ê°€ëŠ¥í•œì§€ í™•ì¸ í›„ MSE ê³„ì‚°ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

        # ğŸ”¥ train ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìœˆë„ìš°ë³„ MSE ê³„ì‚° (pad_value ë§ˆìŠ¤í‚¹)
        pad_value = float(config.get("pad_value", 0.0))

        diff = X_model - recon  # (N, T, D_model)

        # ê° timestepì´ padì¸ì§€ ì•„ë‹Œì§€: feature ì¤‘ í•˜ë‚˜ë¼ë„ pad_valueê°€ ì•„ë‹ˆë©´ ìœ íš¨
        not_pad = np.any(np.not_equal(X_model, pad_value), axis=-1)  # (N, T)
        mask = not_pad.astype(np.float32)  # (N, T)

        # timestepë³„ MSE
        se = np.mean(diff ** 2, axis=-1)         # (N, T)
        se_masked = se * mask                    # (N, T)

        denom = np.sum(mask, axis=-1) + 1e-8     # (N,)
        mse_per_window = np.sum(se_masked, axis=-1) / denom  # (N,)

        print(
            "[INFO] benchmark ìœˆë„ìš° MSE í†µê³„: "
            f"mean={mse_per_window.mean():.4f}, "
            f"std={mse_per_window.std():.4f}, "
            f"min={mse_per_window.min():.4f}, "
            f"max={mse_per_window.max():.4f}"
        )

        out_scores = output_dir / f"window_scores_{tag}.csv"
        with out_scores.open("w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                "window_index",
                "start_packet_idx",
                "end_packet_idx",
                "valid_len",
                "mse",
                "is_anomaly",  # threshold ì—†ìœ¼ë©´ -1
            ])
            for m, mse in zip(meta_list, mse_per_window):
                if threshold is not None:
                    is_anom = int(mse > threshold)
                else:
                    is_anom = -1  # ë¼ë²¨ ì—†ìŒ
                writer.writerow([
                    m["window_index"],
                    m["start_packet_idx"],
                    m["end_packet_idx"],
                    m["valid_len"],
                    float(mse),
                    is_anom,
                ])

        print(f"[INFO] ìœˆë„ìš°ë³„ MSE / anomaly ì—¬ë¶€ ì €ì¥ ì™„ë£Œ: {out_scores}")

        if threshold is not None:
            num_anom = int(np.sum(mse_per_window > threshold))
            print(f"[INFO] threshold={threshold} ê¸°ì¤€ anomaly ìœˆë„ìš° ìˆ˜: {num_anom}/{num_windows}")
        else:
            print("[INFO] thresholdê°€ ì—†ìœ¼ë¯€ë¡œ is_anomaly = -1 ë¡œë§Œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. CSVì—ì„œ MSE ë¶„í¬ë¥¼ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”.")

# -----------------------------
# protocol ì •ê·œí™” ìœ í‹¸
# -----------------------------
def normalize_protocol_column(
    X: np.ndarray,
    min_code: float = 0.0,
    max_code: float = 8.0,   # s7comm=1, tcp=2, ..., dns=8 ê¸°ì¤€
    col_name: str = "protocol",
    new_col_name: str = "protocol_norm",
) -> np.ndarray:
    if col_name not in PACKET_FEATURE_COLUMNS:
        print(f"[INFO] PACKET_FEATURE_COLUMNSì— '{col_name}' ì—†ìŒ â†’ protocol ì •ê·œí™” ìŠ¤í‚µ")
        return X

    col_idx = PACKET_FEATURE_COLUMNS.index(col_name)

    proto_vals = X[:, :, col_idx]  # (N, T)

    # max_codeë¥¼ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì¡ê³  ì‹¶ìœ¼ë©´ Noneìœ¼ë¡œ ë‘ê³  ì—¬ê¸°ì„œ ê³„ì‚° ê°€ëŠ¥
    if max_code is None:
        max_code = float(proto_vals.max())
        print(f"[INFO] ë°ì´í„° ê¸°ì¤€ protocol max_code={max_code} ë¡œ ì‚¬ìš©")

    denom = (max_code - min_code) + 1e-9

    # ì „ì²´ë¥¼ ë¨¼ì € -2.0(ì„¼í‹°ë„)ë¡œ ì±„ì›Œë‘ê³ 
    proto_norm = np.full_like(proto_vals, fill_value=-2.0, dtype=np.float32)

    # min_code ~ max_code ì•ˆì— ë“¤ì–´ì˜¤ëŠ” ê°’ë§Œ 0~1ë¡œ ìŠ¤ì¼€ì¼ë§
    in_range = (proto_vals >= min_code) & (proto_vals <= max_code)
    if np.any(in_range):
        scaled = (proto_vals[in_range] - min_code) / denom
        # í˜¹ì‹œë¼ë„ ìˆ˜ì¹˜ì˜¤ì°¨ë¡œ ì‚´ì§ ë²—ì–´ë‚˜ë©´ 0~1ë¡œ í´ë¦½
        scaled = np.clip(scaled, 0.0, 1.0)
        proto_norm[in_range] = scaled

    # ìƒˆ ì»¬ëŸ¼ì„ ë§ˆì§€ë§‰ ì¶•ì— ì¶”ê°€
    proto_norm_expanded = proto_norm[:, :, None]  # (N, T, 1)
    X_new = np.concatenate([X, proto_norm_expanded], axis=-1)  # (N, T, D+1)

    # ì»¬ëŸ¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ì—ë„ ì¶”ê°€í•´ì„œ ì¸ë±ìŠ¤ ë§¤í•‘ì´ ë§ë„ë¡
    if new_col_name not in PACKET_FEATURE_COLUMNS:
        PACKET_FEATURE_COLUMNS.append(new_col_name)
        print(
            f"[INFO] '{new_col_name}' ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ: "
            f"old_dim={X.shape[-1]}, new_dim={X_new.shape[-1]}"
        )

    print(
        f"[INFO] protocol_norm ìƒì„± ì™„ë£Œ: "
        f"min_code={min_code}, max_code={max_code}, "
        f"src_col_idx={col_idx}, new_col_idx={len(PACKET_FEATURE_COLUMNS)-1}"
    )
    return X_new


if __name__ == "__main__":
    main()

"""
ì‹¤í–‰ ì˜ˆì‹œ:

# 1) non-overlap ìœˆë„ìš° + featureë§Œ ë§Œë“¤ê³  ì‹¶ì„ ë•Œ
python 1.benchmark.py --input "../data/attack.jsonl" --pre-dir "../../preprocessing/result" --window-size 76 --output-dir "../result/benchmark"

# 2) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (size=80, step=40) + LSTM-AE íƒì§€ê¹Œì§€ ìˆ˜í–‰í•  ë•Œ
python 1.benchmark.py --input "../data/attack.jsonl" --pre-dir "../../preprocessing/result" --window-size 41 --step-size 20 --output-dir "../result/benchmark" --model-dir "../data" --batch-size 128

# 3) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (size=80, step=40) + LSTM-AE íƒì§€ê¹Œì§€ ìˆ˜í–‰í•  ë•Œ threshold ì§€ì •
python 1.benchmark.py --input "../data/attack.jsonl" --pre-dir "../../preprocessing/result" --window-size 8 --step-size 4 --output-dir "../result/benchmark" --model-dir "../data" --batch-size 128 --threshold 100

"""
