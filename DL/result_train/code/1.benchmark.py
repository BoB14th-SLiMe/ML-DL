#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
benchmark.py

íŒ¨í‚· ë‹¨ìœ„ JSONL (ê° lineì´ í•˜ë‚˜ì˜ íŒ¨í‚·) â†’
ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° + packet_feature_extractor ë¥¼ ì´ìš©í•´ì„œ

1) íŒ¨í‚· window_sizeê°œì”© ìˆœì„œëŒ€ë¡œ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ window(sequence_group)ë¡œ ë³´ê³ 
2) í•´ë‹¹ windowë¥¼ feature matrix (shape = [window_size, feat_dim]) ë¡œ ë³€í™˜
3) ëª¨ë“  windowë¥¼ ëª¨ì•„ì„œ X_windows.npy ë¡œ ì €ì¥
4) window ë©”íƒ€ì •ë³´ëŠ” windows_meta.jsonl ë¡œ ì €ì¥
5) (ì„ íƒ) í•™ìŠµëœ LSTM-AE(Keras) ëª¨ë¸ì„ ë¶ˆëŸ¬ì„œ ìœˆë„ìš°ë³„ reconstruction MSE ê³„ì‚° + anomaly ì—¬ë¶€ ê¸°ë¡

âš ï¸ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°:
   start = 0, step_size, 2*step_size, ...
   ì˜ˆ: window_size=80, step_size=40 ì´ë©´
       [0~79], [40~119], [80~159], ...
"""

import argparse
import json
import csv
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
from datetime import datetime

# ê¸°ì¡´ì— ë§Œë“¤ì–´ë‘” ëª¨ë“ˆ ì¬ì‚¬ìš©
from packet_feature_extractor import (
    load_preprocess_params,
    sequence_group_to_feature_matrix,
    PACKET_FEATURE_COLUMNS,
)


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()

    p.add_argument(
        "--input", "-i", required=True,
        help="íŒ¨í‚· ë‹¨ìœ„ JSONL ê²½ë¡œ (ê° line = 1 packet)",
    )
    p.add_argument(
        "--pre-dir", "-p", required=True,
        help="ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° JSONë“¤ì´ ëª¨ì—¬ìˆëŠ” ë””ë ‰í† ë¦¬",
    )
    p.add_argument(
        "--window-size", "-w", type=int, default=80,
        help="ìœˆë„ìš° ê¸¸ì´(ë¬¶ì„ íŒ¨í‚· ê°œìˆ˜, ê¸°ë³¸=80)",
    )
    p.add_argument(
        "--step-size", "-s", type=int, default=None,
        help="ìŠ¬ë¼ì´ë”© stride (ê¸°ë³¸: window-sizeì™€ ë™ì¼ â†’ non-overlap)",
    )
    p.add_argument(
        "--output-dir", "-o", required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (X_windows.npy, windows_meta.jsonl, window_scores.csv)",
    )
    p.add_argument(
        "--no-pad-last", action="store_true",
        help="ë§ˆì§€ë§‰ ìœˆë„ìš°ê°€ window-size ë³´ë‹¤ ì§§ìœ¼ë©´ 0 íŒ¨ë”© ì—†ì´ ë²„ë¦¼",
    )

    # â¬‡ï¸ DL ëª¨ë¸ inference ê´€ë ¨ ì˜µì…˜ (model_dir ë°©ì‹)
    p.add_argument(
        "--model-dir", "-m", default=None,
        help="train_lstm_ae_windows_keras.py ê²°ê³¼ ë””ë ‰í† ë¦¬ (model.h5, config.json ë“±)",
    )
    p.add_argument(
        "--batch-size", "-b", type=int, default=128,
        help="DL ëª¨ë¸ inference batch size (ê¸°ë³¸=128)",
    )
    p.add_argument(
        "--threshold", "-t", type=float, default=None,
        help=(
            "ìœˆë„ìš° MSE anomaly threshold "
            "(ë¯¸ì§€ì • ì‹œ threshold.jsonì´ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ì‚¬ìš©, ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì ìˆ˜ë§Œ ê¸°ë¡)"
        ),
    )

    return p.parse_args()


# -----------------------------
# timestamp â†’ delta_t ê³„ì‚°
# -----------------------------
def parse_timestamp(ts: Any) -> float:
    """
    "@timestamp": "2025-11-10T17:43:40.425Z" í˜•íƒœë¥¼ float epoch ë¡œ ë³€í™˜.
    ì‹¤íŒ¨í•˜ë©´ 0.0 ë¦¬í„´.
    """
    if not ts:
        return 0.0
    if isinstance(ts, (int, float)):
        return float(ts)
    ts_str = str(ts)
    try:
        if ts_str.endswith("Z"):
            ts_str = ts_str[:-1]
        dt = datetime.fromisoformat(ts_str)
        return dt.timestamp()
    except Exception:
        return 0.0


# -----------------------------
# train ì½”ë“œì™€ ë™ì¼í•œ window MSE ê³„ì‚°
# -----------------------------
def compute_window_errors(
    X_true: np.ndarray,
    X_pred: np.ndarray,
    pad_value: float,
) -> np.ndarray:
    """
    X_true, X_pred: shape (N, T, D)
    pad_value    : íŒ¨ë”© ê°’ (í•´ë‹¹ timestepì€ ë§ˆìŠ¤í¬)

    ë°˜í™˜:
      errors: shape (N,), ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨
    """
    # íŒ¨ë”©ì´ ì•„ë‹Œ timestep ë§ˆìŠ¤í¬ (N, T)
    not_pad = np.any(np.not_equal(X_true, pad_value), axis=-1)
    mask = not_pad.astype(np.float32)

    # íƒ€ì„ìŠ¤í…ë³„ MSE (N, T)
    se = np.mean((X_pred - X_true) ** 2, axis=-1)
    se_masked = se * mask

    denom = np.sum(mask, axis=-1) + 1e-8
    errors = np.sum(se_masked, axis=-1) / denom
    return errors


# -----------------------------
# ëª¨ë¸ ë¡œë”© (benchmark_lstm_ae_inference ìŠ¤íƒ€ì¼)
# -----------------------------
def load_model_from_dir(model_dir: Path):
    """
    model_dir ë‚´ë¶€:
      - config.json : {"T": ..., "D": ..., "pad_value": ...} í¬í•¨
      - model.h5    : Keras LSTM-AE ëª¨ë¸
      - threshold.json (ì„ íƒ)

    ë°˜í™˜:
      model, config(dict), threshold(float ë˜ëŠ” None)
    """
    config_path = model_dir / "config.json"
    if not config_path.exists():
        raise FileNotFoundError(f"âŒ config.json ì—†ìŒ: {config_path}")

    with config_path.open("r", encoding="utf-8") as f:
        config = json.load(f)

    print(f"[INFO] config ë¡œë“œ: {config_path}")
    T = config.get("T")
    D = config.get("D")
    print(f"[INFO] í•™ìŠµ ì‹œ T={T}, D={D}")

    import tensorflow as tf  # noqa: F401
    from tensorflow.keras.models import load_model

    model_path = model_dir / "model.h5"
    if not model_path.exists():
        raise FileNotFoundError(f"âŒ model.h5 ì—†ìŒ: {model_path}")

    print(f"[INFO] model ë¡œë“œ: {model_path}")

    # ğŸ”¥ config.json ì—ì„œ T, D ì½ê¸° (decoderì—ì„œ ë°˜ë³µí•  ê¸¸ì´)
    T = config.get("T")
    D = config.get("D")

    def repeat_latent(x):
        """
        train_lstm_ae_windows_keras.py ì—ì„œ ì¼ë˜ repeat_latentì™€ ë™ì¼í•œ ë™ì‘:
        (B, latent_dim) -> (B, T, latent_dim)
        """
        x = tf.expand_dims(x, axis=1)      # (B, 1, latent_dim)
        x = tf.tile(x, [1, T, 1])          # (B, T, latent_dim)
        return x

    custom_objects = {
        "repeat_latent": repeat_latent,
    }

    model = load_model(
        model_path,
        compile=False,
        custom_objects=custom_objects,
    )

    # ---------------------------
    # threshold.json ì½ê¸° (êµ¬ì¡° ë§ê²Œ)
    # ---------------------------
    thresh_path = model_dir / "threshold.json"
    threshold = None
    if thresh_path.exists():
        try:
            with thresh_path.open("r", encoding="utf-8") as f:
                th_cfg = json.load(f)

            # train ì½”ë“œì—ì„œ ì €ì¥í•œ êµ¬ì¡°:
            # {
            #   "threshold_p99": ...,
            #   "threshold_mu3": ...,
            #   "stats": {...}
            # }
            if "threshold" in th_cfg:
                # í˜¹ì‹œ ë‚˜ì¤‘ì— ë‹¨ì¼ thresholdë¡œ ì €ì¥í•´ë‘” ë²„ì „
                threshold = float(th_cfg["threshold"])
                print(f"[INFO] threshold.json(threshold) ì‚¬ìš©: {threshold}")
            elif "threshold_p99" in th_cfg:
                threshold = float(th_cfg["threshold_p99"])
                print(f"[INFO] threshold_p99 ì‚¬ìš©: {threshold}")
            elif "threshold_mu3" in th_cfg:
                threshold = float(th_cfg["threshold_mu3"])
                print(f"[INFO] threshold_mu3 ì‚¬ìš©: {threshold}")
            else:
                print(
                    "[INFO] threshold.json ì•ˆì— ì‚¬ìš©í•  í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. "
                    "(threshold / threshold_p99 / threshold_mu3 ì—†ìŒ)"
                )
        except Exception as e:
            print(f"[WARN] threshold.json ë¡œë”© ì‹¤íŒ¨: {e}")

    # feature_keys.txt ìˆìœ¼ë©´ ê¸¸ì´ í™•ì¸ ìš©ë„ë¡œë§Œ ì‚¬ìš©
    feat_path = model_dir / "feature_keys.txt"
    if feat_path.exists():
        try:
            with feat_path.open("r", encoding="utf-8") as f:
                feature_keys = [line.strip() for line in f if line.strip()]
            print(f"[INFO] feature_keys.txt ê¸¸ì´ = {len(feature_keys)}")
        except Exception as e:
            print(f"[WARN] feature_keys.txt ë¡œë”© ì‹¤íŒ¨: {e}")

    return model, config, threshold


def main():
    args = parse_args()

    input_path = Path(args.input)
    pre_dir = Path(args.pre_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    window_size = args.window_size
    step_size = args.step_size if args.step_size is not None else window_size
    pad_last = not args.no_pad_last

    print(f"[INFO] ì…ë ¥ JSONL : {input_path}")
    print(f"[INFO] ì „ì²˜ë¦¬ ë””ë ‰í† ë¦¬ : {pre_dir}")
    print(f"[INFO] ì¶œë ¥ ë””ë ‰í† ë¦¬ : {output_dir}")
    print(f"[INFO] window_size = {window_size}, step_size = {step_size}, pad_last = {pad_last}")

    # 1) ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ë¡œë”© (packet_feature_extractor ë‚´ë¶€ ì •ì˜ì— ë§ê²Œ)
    print("[INFO] ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ë¡œë”© ì¤‘...")
    params = load_preprocess_params(pre_dir)
    feat_dim = len(PACKET_FEATURE_COLUMNS)
    print(f"[INFO] feature dimension = {feat_dim} (PACKET_FEATURE_COLUMNS ê¸¸ì´)")

    # 2) JSONL ì „ì²´ë¥¼ ì½ì–´ì„œ delta_t í¬í•¨í•œ packet ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    all_packets: List[Dict[str, Any]] = []
    prev_ts = None

    with input_path.open("r", encoding="utf-8") as fin:
        for line in fin:
            line = line.strip()
            if not line:
                continue
            try:
                pkt = json.loads(line)
            except Exception as e:
                print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue

            # delta_t ê³„ì‚°
            ts_val = parse_timestamp(pkt.get("@timestamp"))
            if prev_ts is None:
                delta_t = 0.0
            else:
                dt = ts_val - prev_ts
                delta_t = dt if dt >= 0 else 0.0
            prev_ts = ts_val

            pkt["delta_t"] = delta_t  # feature extractor ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë„£ì–´ì¤Œ
            all_packets.append(pkt)

    total_packets = len(all_packets)
    print(f"[INFO] ì´ íŒ¨í‚· ìˆ˜ = {total_packets}")

    if total_packets == 0:
        print("[WARN] ì…ë ¥ JSONLì—ì„œ ìœ íš¨í•œ íŒ¨í‚·ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
    all_windows: List[np.ndarray] = []
    meta_list: List[Dict[str, Any]] = []

    start_idx = 0
    window_index = 0

    while start_idx < total_packets:
        end_idx = start_idx + window_size
        group = all_packets[start_idx:end_idx]

        if len(group) == 0:
            break

        # ë§ˆì§€ë§‰ ìœˆë„ìš°ê°€ window_sizeë³´ë‹¤ ì§§ê³  pad_lastê°€ Falseë©´ ì¤‘ë‹¨
        if len(group) < window_size and not pad_last:
            print(f"[INFO] ë§ˆì§€ë§‰ window({start_idx}~{total_packets-1})ëŠ” ê¸¸ì´ {len(group)} < {window_size}, íŒ¨ë”© ì—†ì´ ë²„ë¦¼")
            break

        X_list = sequence_group_to_feature_matrix(group, params)
        if not X_list:
            print(f"[WARN] window({start_idx}~{min(end_idx, total_packets)-1})ì—ì„œ feature ì¶”ì¶œ ì‹¤íŒ¨, ìŠ¤í‚±")
            start_idx += step_size
            continue

        X = np.array(X_list, dtype="float32")
        valid_len = X.shape[0]

        if valid_len < window_size:
            if pad_last:
                pad_len = window_size - valid_len
                pad_block = np.zeros((pad_len, feat_dim), dtype=X.dtype)
                X = np.concatenate([X, pad_block], axis=0)
        elif valid_len > window_size:
            X = X[:window_size, :]
            valid_len = window_size

        all_windows.append(X)
        meta_list.append({
            "window_index": window_index,
            "start_packet_idx": start_idx,
            "end_packet_idx": min(end_idx, total_packets) - 1,
            "valid_len": int(valid_len),
        })

        window_index += 1
        start_idx += step_size

    num_windows = len(all_windows)
    print(f"[INFO] ìƒì„±ëœ ìœˆë„ìš° ìˆ˜ = {num_windows}")

    if num_windows == 0:
        print("[WARN] ìƒì„±ëœ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥/íŒŒë¼ë¯¸í„°/step_sizeë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # 4) numpy ì €ì¥
    X_windows = np.stack(all_windows, axis=0)  # [num_windows, window_size, feat_dim]
    out_npy = output_dir / "X_windows.npy"
    np.save(out_npy, X_windows)
    print(f"[INFO] ì €ì¥ ì™„ë£Œ: {out_npy} (shape={X_windows.shape})")

    # 5) ë©”íƒ€ì •ë³´ JSONL ì €ì¥
    out_meta = output_dir / "windows_meta.jsonl"
    with out_meta.open("w", encoding="utf-8") as fmeta:
        for m in meta_list:
            fmeta.write(json.dumps(m, ensure_ascii=False) + "\n")
    print(f"[INFO] ë©”íƒ€ ì •ë³´ ì €ì¥ ì™„ë£Œ: {out_meta}")

    # ============================
    # 6) DL ëª¨ë¸ ë¶ˆëŸ¬ì„œ íƒì§€ (ì„ íƒ)
    # ============================
    if args.model_dir:
        model_dir = Path(args.model_dir)
        print(f"[INFO] DL ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_dir}")

        model, config, threshold_from_file = load_model_from_dir(model_dir)

        # configì™€ í˜„ì¬ ë°ì´í„° shape consistency ì²´í¬
        T_cfg = config.get("T")
        D_cfg = config.get("D")
        pad_value = float(config.get("pad_value", 0.0))
        _, T_cur, D_cur = X_windows.shape

        if T_cfg is not None and T_cfg != T_cur:
            print(f"[WARN] config.T({T_cfg}) != í˜„ì¬ window_size({T_cur})")
        if D_cfg is not None and D_cfg != D_cur:
            print(f"[WARN] config.D({D_cfg}) != í˜„ì¬ feature_dim({D_cur})")

        # threshold ê²°ì •: CLI > threshold.json > None
        threshold = args.threshold
        if threshold is None and threshold_from_file is not None:
            threshold = threshold_from_file
            print(f"[INFO] threshold.jsonì˜ ê°’ì„ ì‚¬ìš©: threshold={threshold}")
        elif threshold is not None:
            print(f"[INFO] CLIë¡œ ì „ë‹¬ëœ threshold ì‚¬ìš©: threshold={threshold}")
        else:
            print("[INFO] threshold ë¯¸ì§€ì • â†’ anomaly ë¼ë²¨ë§ ì—†ì´ MSEë§Œ ê¸°ë¡")

        # ë””ë°”ì´ìŠ¤ ì •ë³´ (ì˜µì…˜)
        try:
            import tensorflow as tf
            gpus = tf.config.list_physical_devices("GPU")
            if gpus:
                print(f"[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {gpus}")
            else:
                print("[INFO] GPU ì—†ìŒ, CPU ì‚¬ìš©")
        except Exception:
            pass

        print("[INFO] DL ëª¨ë¸ë¡œ ìœˆë„ìš°ë³„ reconstruction ì˜ˆì¸¡ ì¤‘...")
        recon = model.predict(X_windows, batch_size=args.batch_size, verbose=1)

        if recon.shape != X_windows.shape:
            print(f"[WARN] ì¬êµ¬ì„± ê²°ê³¼ shape {recon.shape} != ì…ë ¥ shape {X_windows.shape}")
            print("       ë¸Œë¡œë“œìºìŠ¤íŠ¸ ê°€ëŠ¥í•œì§€ í™•ì¸ í›„ MSE ê³„ì‚°ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

        # ğŸ”¥ train ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìœˆë„ìš°ë³„ MSE ê³„ì‚° (pad_value ë§ˆìŠ¤í‚¹)
        # -------------------------
        # ìœˆë„ìš°ë³„ MSE ê³„ì‚° (í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹, pad mask ì ìš©)
        # -------------------------
        pad_value = float(config.get("pad_value", 0.0))

        diff = X_windows - recon  # (N, T, D)

        # ê° timestepì´ padì¸ì§€ ì•„ë‹Œì§€: feature ì¤‘ í•˜ë‚˜ë¼ë„ pad_valueê°€ ì•„ë‹ˆë©´ ìœ íš¨
        not_pad = np.any(np.not_equal(X_windows, pad_value), axis=-1)  # (N, T)
        mask = not_pad.astype(np.float32)  # (N, T)

        # timestepë³„ MSE
        se = np.mean(diff ** 2, axis=-1)         # (N, T)
        se_masked = se * mask                    # (N, T)

        denom = np.sum(mask, axis=-1) + 1e-8     # (N,)
        mse_per_window = np.sum(se_masked, axis=-1) / denom  # (N,)

        print(
            "[INFO] benchmark ìœˆë„ìš° MSE í†µê³„: "
            f"mean={mse_per_window.mean():.4f}, "
            f"std={mse_per_window.std():.4f}, "
            f"min={mse_per_window.min():.4f}, "
            f"max={mse_per_window.max():.4f}"
        )

        out_scores = output_dir / "window_scores.csv"
        with out_scores.open("w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                "window_index",
                "start_packet_idx",
                "end_packet_idx",
                "valid_len",
                "mse",
                "is_anomaly",  # threshold ì—†ìœ¼ë©´ -1
            ])
            for m, mse in zip(meta_list, mse_per_window):
                if threshold is not None:
                    is_anom = int(mse > threshold)
                else:
                    is_anom = -1  # ë¼ë²¨ ì—†ìŒ
                writer.writerow([
                    m["window_index"],
                    m["start_packet_idx"],
                    m["end_packet_idx"],
                    m["valid_len"],
                    float(mse),
                    is_anom,
                ])

        print(f"[INFO] ìœˆë„ìš°ë³„ MSE / anomaly ì—¬ë¶€ ì €ì¥ ì™„ë£Œ: {out_scores}")

        if threshold is not None:
            num_anom = int(np.sum(mse_per_window > threshold))
            print(f"[INFO] threshold={threshold} ê¸°ì¤€ anomaly ìœˆë„ìš° ìˆ˜: {num_anom}/{num_windows}")
        else:
            print("[INFO] thresholdê°€ ì—†ìœ¼ë¯€ë¡œ is_anomaly = -1 ë¡œë§Œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. CSVì—ì„œ MSE ë¶„í¬ë¥¼ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()


"""
ì‹¤í–‰ ì˜ˆì‹œ:

# 1) non-overlap ìœˆë„ìš° + featureë§Œ ë§Œë“¤ê³  ì‹¶ì„ ë•Œ
python 1.benchmark.py --input "../data/attack.jsonl" --pre-dir "../../preprocessing/result" --window-size 80 --output-dir "../result/benchmark"

# 2) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (size=80, step=40) + LSTM-AE íƒì§€ê¹Œì§€ ìˆ˜í–‰í•  ë•Œ
python 1.benchmark.py --input "../data/attack.jsonl" --pre-dir "../../preprocessing/result" --window-size 80 --step-size 30 --output-dir "../result/benchmark" --model-dir "../data" --batch-size 128

python 1.benchmark.py --input "../data/attack.jsonl" --pre-dir "../../preprocessing/result" --window-size 80 --step-size 30 --output-dir "../result/benchmark" --model-dir "../data" --batch-size 128 --threshold 100

# threshold.json ì•ˆì— threshold_p99 / threshold_mu3ê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©,
# CLIì—ì„œ --threshold 300 ê°™ì´ ì£¼ë©´ ê·¸ ê°’ì´ ìš°ì„  ì‚¬ìš©ë¨.
"""
