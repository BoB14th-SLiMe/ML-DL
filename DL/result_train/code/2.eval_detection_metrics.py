#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
eval_detection_metrics.py

ë‘ ê°œì˜ ìœˆë„ìš° ë ˆë²¨ CSVë¥¼ ë¹„êµí•´ì„œ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

ì…ë ¥ CSV í˜•ì‹ (ì˜ˆì‹œ):

1) attack_result_XXX.csv  (ì‹¤ì œ ë¼ë²¨, GT)
   window_index,start_packet_idx,end_packet_idx,valid_len,is_anomaly
   - is_anomaly: 1 = ê³µê²©, 0 = ì •ìƒ

2) window_scores_XXX.csv  (ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼)
   window_index,start_packet_idx,end_packet_idx,valid_len,mse,is_anomaly
   - is_anomaly: 1 = ëª¨ë¸ì´ ì´ìƒìœ¼ë¡œ íŒë‹¨, 0 = ì •ìƒ
   - (thresholdê°€ ì—†ì–´ì„œ -1ì¼ ìˆ˜ë„ ìˆìŒ â†’ ì´ ê²½ìš°ëŠ” í‰ê°€ì—ì„œ ì œì™¸í•˜ê±°ë‚˜ ë³„ë„ ì²˜ë¦¬ ê°€ëŠ¥)

ë™ì‘:
  - window_index ê¸°ì¤€ inner join
  - (í•„ìš”ì‹œ) ì˜ˆì¸¡ is_anomaly == -1 ì¸ í–‰ì€ í‰ê°€ì—ì„œ ì œì™¸
  - TP/TN/FP/FN, accuracy, precision, recall, F1, TPR, FPR, Balanced Accuracy ë“± ê³„ì‚°
  - (mse ì»¬ëŸ¼ì´ ìˆìœ¼ë©´) ROC-AUC, PR-AUC ê³„ì‚° ì‹œë„
  - ì½˜ì†”ì— ì¶œë ¥ + JSON íŒŒì¼ë¡œ ì €ì¥

ì¶œë ¥ íŒŒì¼ ì´ë¦„:
  - --output-json ì„ ëª…ì‹œí•˜ë©´ ê·¸ ê²½ë¡œë¥¼ ì‚¬ìš©
  - ì•„ë‹ˆë©´ pred CSV ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ metrics_{tag}.json ìœ¼ë¡œ ì €ì¥
    * tag ê¸°ë³¸ê°’: pred CSV íŒŒì¼ëª… stem (í™•ì¥ì ì œê±°)
"""

import argparse
import json
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np
import pandas as pd


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()

    p.add_argument(
        "--attack-csv", "-a", required=True,
        help="ì‹¤ì œ ê³µê²© ì—¬ë¶€ê°€ ë“¤ì–´ìˆëŠ” CSV (ì˜ˆ: attack_result_XXX.csv, is_anomaly=GT)"
    )
    p.add_argument(
        "--pred-csv", "-p", required=True,
        help="ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ CSV (ì˜ˆ: window_scores_XXX.csv, is_anomaly=ì˜ˆì¸¡)"
    )
    p.add_argument(
        "--output-json", "-o", default=None,
        help=(
            "ì„±ëŠ¥ ì§€í‘œë¥¼ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ "
            "(ë¯¸ì§€ì • ì‹œ pred CSV ë””ë ‰í† ë¦¬ì— metrics_{tag}.json ìœ¼ë¡œ ì €ì¥)"
        ),
    )
    p.add_argument(
        "--ignore-pred-minus1", action="store_true",
        help="ì˜ˆì¸¡ CSVì—ì„œ is_anomaly == -1 ì¸ í–‰ì€ í‰ê°€ì—ì„œ ì œì™¸ (threshold ì•ˆ ì“´ ê²½ìš° ë“±)"
    )
    p.add_argument(
        "--tag", default=None,
        help=(
            "ì¶œë ¥ JSON ì´ë¦„ì— ì‚¬ìš©í•  íƒœê·¸ "
            "(ê¸°ë³¸: pred CSV íŒŒì¼ëª… stem, ì˜ˆ: window_scores_attack_ver5_1)"
        ),
    )

    return p.parse_args()


def safe_div(num: float, den: float) -> float:
    if den == 0:
        return 0.0
    return float(num) / float(den)


def try_compute_auc(
    y_true: np.ndarray,
    scores: np.ndarray,
) -> Dict[str, Optional[float]]:
    """
    mse ê¸°ë°˜ ROC-AUC / PR-AUC ê³„ì‚° (ê°€ëŠ¥í•˜ë©´).
    - y_true: 0/1
    - scores: í´ìˆ˜ë¡ ê³µê²©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ score (ì˜ˆ: mse)
    """
    result = {"roc_auc": None, "pr_auc": None}
    try:
        from sklearn.metrics import roc_auc_score, average_precision_score
    except ImportError:
        print("[WARN] scikit-learn ì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ ROC-AUC / PR-AUC ê³„ì‚°ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return result

    # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë©´ AUC ê³„ì‚° ë¶ˆê°€
    if len(np.unique(y_true)) < 2:
        print("[WARN] y_true ì— í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë¼ AUC ê³„ì‚° ë¶ˆê°€ (ì–‘ì„±/ìŒì„± ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•¨).")
        return result

    try:
        roc_auc = float(roc_auc_score(y_true, scores))
        pr_auc = float(average_precision_score(y_true, scores))
        result["roc_auc"] = roc_auc
        result["pr_auc"] = pr_auc
        print(f"[INFO] ROC-AUC={roc_auc:.4f}, PR-AUC={pr_auc:.4f}")
    except Exception as e:
        print(f"[WARN] AUC ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return result


def main():
    args = parse_args()

    attack_path = Path(args.attack_csv)
    pred_path = Path(args.pred_csv)

    # ğŸ”¥ tag ê²°ì • (ê¸°ë³¸: pred CSV stem)
    tag = args.tag if args.tag is not None else pred_path.stem

    # ğŸ”¥ output JSON ê²½ë¡œ ê²°ì •
    if args.output_json is not None:
        out_path = Path(args.output_json)
    else:
        out_path = pred_path.parent / f"metrics_{tag}.json"

    print(f"[INFO] GT CSV (attack)      : {attack_path}")
    print(f"[INFO] Pred CSV (model)     : {pred_path}")
    print(f"[INFO] ì‚¬ìš© íƒœê·¸(tag)       : {tag}")
    print(f"[INFO] ì¶œë ¥ JSON (metrics)  : {out_path}")

    df_gt = pd.read_csv(attack_path)
    df_pred = pd.read_csv(pred_path)

    # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
    for col in ["window_index", "is_anomaly"]:
        if col not in df_gt.columns:
            raise ValueError(f"GT CSVì— '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        if col not in df_pred.columns:
            raise ValueError(f"Pred CSVì— '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # pred ìª½ì—ì„œ mse ì»¬ëŸ¼ë„ ê°™ì´ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìœ¼ë©´ AUCìš©ìœ¼ë¡œ ì‚¬ìš©
    pred_cols = ["window_index", "is_anomaly"]
    has_mse = "mse" in df_pred.columns
    if has_mse:
        pred_cols.append("mse")

    # window_index ê¸°ì¤€ inner join
    merged = pd.merge(
        df_gt[["window_index", "is_anomaly"]],
        df_pred[pred_cols],
        on="window_index",
        how="inner",
        suffixes=("_true", "_pred"),
    )

    print(f"[INFO] join í›„ í–‰ ìˆ˜: {len(merged)}")

    # íƒ€ì… ì •ë¦¬
    merged["is_anomaly_true"] = merged["is_anomaly_true"].astype(int)
    merged["is_anomaly_pred"] = merged["is_anomaly_pred"].astype(int)

    # is_anomaly_pred == -1 (ë¯¸ë¼ë²¨) ì œê±° ì˜µì…˜
    if args.ignore_pred_minus1:
        before = len(merged)
        merged = merged[merged["is_anomaly_pred"] != -1].copy()
        after = len(merged)
        print(f"[INFO] is_anomaly_pred == -1 ì œê±°: {before} -> {after}")

    y_true = merged["is_anomaly_true"].to_numpy()
    y_pred = merged["is_anomaly_pred"].to_numpy()

    # í˜¼ë™í–‰ë ¬ ìš”ì†Œ ê³„ì‚°
    tp = int(np.sum((y_true == 1) & (y_pred == 1)))
    tn = int(np.sum((y_true == 0) & (y_pred == 0)))
    fp = int(np.sum((y_true == 0) & (y_pred == 1)))
    fn = int(np.sum((y_true == 1) & (y_pred == 0)))

    total = tp + tn + fp + fn

    accuracy = safe_div(tp + tn, total)
    precision = safe_div(tp, tp + fp)
    recall = safe_div(tp, tp + fn)  # TPR
    f1 = safe_div(2 * precision * recall, precision + recall)

    tpr = recall  # same
    fpr = safe_div(fp, fp + tn)
    tnr = safe_div(tn, tn + fp)
    fnr = safe_div(fn, fn + tp)

    # ì¶”ê°€ ì§€í‘œ: prevalence, predicted positive rate, balanced accuracy
    prevalence = safe_div(tp + fn, total)          # ì‹¤ì œ ê³µê²© ë¹„ìœ¨
    pred_positive_rate = safe_div(tp + fp, total)  # ëª¨ë¸ì´ ê³µê²©ì´ë¼ ë•Œë¦° ë¹„ìœ¨
    balanced_accuracy = 0.5 * (tpr + tnr)

    # AUC ê³„ì‚° (mseê°€ ìˆì„ ë•Œë§Œ ì‹œë„)
    auc_dict = {"roc_auc": None, "pr_auc": None}
    if has_mse:
        scores = merged["mse"].to_numpy(dtype=float)
        auc_dict = try_compute_auc(y_true, scores)
    else:
        print("[INFO] pred CSVì— 'mse' ì»¬ëŸ¼ì´ ì—†ì–´ ROC-AUC / PR-AUC ê³„ì‚°ì„ ìƒëµí•©ë‹ˆë‹¤.")

    metrics: Dict[str, Any] = {
        "num_samples": int(total),
        "confusion_matrix": {
            "TP": tp,
            "TN": tn,
            "FP": fp,
            "FN": fn,
        },
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "tpr": tpr,
        "fpr": fpr,
        "tnr": tnr,
        "fnr": fnr,
        "prevalence": prevalence,
        "pred_positive_rate": pred_positive_rate,
        "balanced_accuracy": balanced_accuracy,
        "roc_auc": auc_dict["roc_auc"],
        "pr_auc": auc_dict["pr_auc"],
    }

    print("===== Detection Metrics =====")
    print(f"Samples (windows)       : {total}")
    print(f"TP={tp}, FP={fp}, FN={fn}, TN={tn}")
    print(f"Accuracy                : {accuracy:.4f}")
    print(f"Precision               : {precision:.4f}")
    print(f"Recall (TPR)            : {recall:.4f}")
    print(f"F1-score                : {f1:.4f}")
    print(f"FPR                     : {fpr:.6f}")
    print(f"TNR (Specificity)       : {tnr:.4f}")
    print(f"FNR                     : {fnr:.4f}")
    print(f"Prevalence (Attack rate): {prevalence:.6f}")
    print(f"Pred Positive Rate      : {pred_positive_rate:.6f}")
    print(f"Balanced Accuracy       : {balanced_accuracy:.4f}")
    if metrics["roc_auc"] is not None:
        print(f"ROC-AUC                 : {metrics['roc_auc']:.4f}")
    if metrics["pr_auc"] is not None:
        print(f"PR-AUC                  : {metrics['pr_auc']:.4f}")

    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)

    print(f"[INFO] ì„±ëŠ¥ ì§€í‘œ JSON ì €ì¥ ì™„ë£Œ â†’ {out_path}")


if __name__ == "__main__":
    main()


"""
# attack_result.csv vs window_scores.csv ë¹„êµ
python 2.eval_detection_metrics.py \
  --attack-csv ../result/attack_result.csv \
  --pred-csv ../result/benchmark/window_scores.csv \
  --output-json ../result/eval_detection_metrics.json \
  --ignore-pred-minus1
"""
