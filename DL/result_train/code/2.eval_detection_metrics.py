#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
eval_detection_metrics.py

ë‘ ê°œì˜ ìœˆë„ìš° ë ˆë²¨ íŒŒì¼ì„ ë¹„êµí•´ì„œ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

ì…ë ¥ í˜•ì‹:

1) GT (ì‹¤ì œ ë¼ë²¨)
   - CSV (attack_result_XXX.csv)
     window_index,start_packet_idx,end_packet_idx,valid_len,is_anomaly
   - ë˜ëŠ” JSONL (attack_ver2_window.jsonl ë“±)
     {"window_index": ..., "start_packet_idx": ..., "end_packet_idx": ..., "valid_len": ..., "is_anomaly": ...}
     ë˜ëŠ” pattern ì»¬ëŸ¼(NORMAL/ATTACK)ë§Œ ìˆì„ ê²½ìš° â†’ ë‚´ë¶€ì—ì„œ is_anomaly ìƒì„±

2) Pred (ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼, í˜„ì¬ëŠ” CSV)
   window_scores_XXX.csv:
     window_index,start_packet_idx,end_packet_idx,valid_len,mse,is_anomaly
     - is_anomaly: 1 = ëª¨ë¸ì´ ì´ìƒìœ¼ë¡œ íŒë‹¨, 0 = ì •ìƒ
     - (thresholdê°€ ì—†ì–´ì„œ -1ì¼ ìˆ˜ë„ ìˆìŒ â†’ ì˜µì…˜ìœ¼ë¡œ í‰ê°€ì—ì„œ ì œì™¸ ê°€ëŠ¥)

ë™ì‘:
  - window_index ê¸°ì¤€ inner join
  - (ì˜µì…˜) ì˜ˆì¸¡ is_anomaly == -1 ì¸ í–‰ì€ í‰ê°€ì—ì„œ ì œì™¸
  - TP/TN/FP/FN, accuracy, precision, recall, F1, TPR, FPR, Balanced Accuracy ë“± ê³„ì‚°
  - (mse ì»¬ëŸ¼ì´ ìˆìœ¼ë©´)
      - ROC-AUC, PR-AUC ê³„ì‚° ì‹œë„
      - ê³µê²©/ì •ìƒ ë³„ mse í†µê³„ (mean, std, min, max, p50, p90, p95, p99) ê³„ì‚°
      - ë³„ë„ JSON íŒŒì¼(mse_stats_*.json)ë¡œ ì €ì¥
"""

import argparse
import json
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np
import pandas as pd


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()

    p.add_argument(
        "--attack-csv", "-a", required=True,
        help="ì‹¤ì œ ê³µê²© ì—¬ë¶€ê°€ ë“¤ì–´ìˆëŠ” íŒŒì¼ (CSV ë˜ëŠ” JSONL; ì˜ˆ: attack_result_XXX.csv, attack_ver2_window.jsonl)"
    )
    p.add_argument(
        "--pred-csv", "-p", required=True,
        help="ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ CSV (ì˜ˆ: window_scores_XXX.csv, is_anomaly=ì˜ˆì¸¡)"
    )
    p.add_argument(
        "--output-json", "-o", default=None,
        help=(
            "ì„±ëŠ¥ ì§€í‘œë¥¼ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ "
            "(ë¯¸ì§€ì • ì‹œ pred CSV ë””ë ‰í† ë¦¬ì— metrics_{tag}.json ìœ¼ë¡œ ì €ì¥)"
        ),
    )
    p.add_argument(
        "--mse-stats-json", default=None,
        help=(
            "MSE í†µê³„ë¥¼ ë³„ë„ë¡œ ì €ì¥í•  JSON ê²½ë¡œ "
            "(ë¯¸ì§€ì • ì‹œ pred CSV ë””ë ‰í† ë¦¬ì— mse_stats_{tag}.json ìœ¼ë¡œ ì €ì¥)"
        ),
    )
    p.add_argument(
        "--ignore-pred-minus1", action="store_true",
        help="ì˜ˆì¸¡ CSVì—ì„œ is_anomaly == -1 ì¸ í–‰ì€ í‰ê°€ì—ì„œ ì œì™¸ (threshold ì•ˆ ì“´ ê²½ìš° ë“±)"
    )
    p.add_argument(
        "--tag", default=None,
        help=(
            "ì¶œë ¥ JSON ì´ë¦„ì— ì‚¬ìš©í•  íƒœê·¸ "
            "(ê¸°ë³¸: pred CSV íŒŒì¼ëª… stem, ì˜ˆ: window_scores_attack_ver5_1)"
        ),
    )

    return p.parse_args()


def safe_div(num: float, den: float) -> float:
    if den == 0:
        return 0.0
    return float(num) / float(den)


def try_compute_auc(
    y_true: np.ndarray,
    scores: np.ndarray,
) -> Dict[str, Optional[float]]:
    """
    mse ê¸°ë°˜ ROC-AUC / PR-AUC ê³„ì‚° (ê°€ëŠ¥í•˜ë©´).
    - y_true: 0/1
    - scores: í´ìˆ˜ë¡ ê³µê²©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ score (ì˜ˆ: mse)
    """
    result = {"roc_auc": None, "pr_auc": None}
    try:
        from sklearn.metrics import roc_auc_score, average_precision_score
    except ImportError:
        print("[WARN] scikit-leë¥¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ ROC-AUC / PR-AUC ê³„ì‚°ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return result

    # í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë©´ AUC ê³„ì‚° ë¶ˆê°€
    if len(np.unique(y_true)) < 2:
        print("[WARN] y_true ì— í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë¿ì´ë¼ AUC ê³„ì‚° ë¶ˆê°€ (ì–‘ì„±/ìŒì„± ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•¨).")
        return result

    try:
        roc_auc = float(roc_auc_score(y_true, scores))
        pr_auc = float(average_precision_score(y_true, scores))
        result["roc_auc"] = roc_auc
        result["pr_auc"] = pr_auc
        print(f"[INFO] ROC-AUC={roc_auc:.4f}, PR-AUC={pr_auc:.4f}")
    except Exception as e:
        print(f"[WARN] AUC ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return result


def compute_mse_stats(
    y_true: np.ndarray,
    scores: np.ndarray,
) -> Dict[str, Any]:
    """
    ë¼ë²¨(0=ì •ìƒ, 1=ê³µê²©)ë³„ë¡œ score(mse)ì˜ í†µê³„ë¥¼ ê³„ì‚°í•œë‹¤.
    ë°˜í™˜ í˜•ì‹ ì˜ˆ:

    {
      "meta": {
        "label_col": "is_anomaly_true",
        "score_col": "mse",
        "n_total": 4684,
        "n_attack": 107,
        "n_normal": 4577
      },
      "attack": {
        "count": 107,
        "mean": ...,
        "std": ...,
        "min": ...,
        "max": ...,
        "p50": ...,
        "p90": ...,
        "p95": ...,
        "p99": ...
      },
      "normal": {
        "count": 4577,
        "mean": ...,
        "std": ...,
        "min": ...,
        "max": ...,
        "p50": ...,
        "p90": ...,
        "p95": ...,
        "p99": ...
      }
    }
    """

    def _summary(arr: np.ndarray) -> Dict[str, Any]:
        arr = np.asarray(arr, dtype=float)
        if arr.size == 0:
            return {
                "count": 0,
                "mean": None,
                "std": None,
                "min": None,
                "max": None,
                "p50": None,
                "p90": None,
                "p95": None,
                "p99": None,
            }
        return {
            "count": int(arr.size),
            "mean": float(np.mean(arr)),
            "std": float(np.std(arr)),
            "min": float(np.min(arr)),
            "max": float(np.max(arr)),
            "p50": float(np.percentile(arr, 50)),
            "p90": float(np.percentile(arr, 90)),
            "p95": float(np.percentile(arr, 95)),
            "p99": float(np.percentile(arr, 99)),
        }

    attack_mask = (y_true == 1)
    normal_mask = (y_true == 0)

    attack_scores = scores[attack_mask]
    normal_scores = scores[normal_mask]

    attack_stats = _summary(attack_scores)
    normal_stats = _summary(normal_scores)

    mse_stats = {
        "meta": {
            "label_col": "is_anomaly_true",
            "score_col": "mse",
            "n_total": int(len(scores)),
            "n_attack": int(attack_mask.sum()),
            "n_normal": int(normal_mask.sum()),
        },
        "attack": attack_stats,
        "normal": normal_stats,
    }

    return mse_stats


def load_gt_table(path: Path) -> pd.DataFrame:
    """
    GT íŒŒì¼ì„ ë¡œë“œí•´ì„œ DataFrame ìœ¼ë¡œ ë°˜í™˜.
    - .csv  â†’ pandas.read_csv
    - .jsonl / .json â†’ JSON Lines ë¡œë“œ í›„ DataFrame
    """
    suffix = path.suffix.lower()
    if suffix == ".csv":
        print(f"[INFO] GT íŒŒì¼ í˜•ì‹: CSV ({path.name})")
        return pd.read_csv(path)
    elif suffix in [".jsonl", ".json"]:
        print(f"[INFO] GT íŒŒì¼ í˜•ì‹: JSONL/JSON ({path.name})")
        rows = []
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    rows.append(obj)
                except Exception as e:
                    print(f"[WARN] JSONL íŒŒì‹± ì‹¤íŒ¨: {e} (line ì¼ë¶€) â†’ ìŠ¤í‚µ")
                    continue
        if not rows:
            raise ValueError(f"GT JSONL ì—ì„œ ìœ íš¨í•œ ë ˆì½”ë“œë¥¼ í•˜ë‚˜ë„ ëª» ì½ì—ˆìŠµë‹ˆë‹¤: {path}")
        df = pd.DataFrame(rows)
        return df
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” GT íŒŒì¼ í™•ì¥ìì…ë‹ˆë‹¤: {path} (csv/jsonl/json ë§Œ ì§€ì›)")


def main():
    args = parse_args()

    attack_path = Path(args.attack_csv)
    pred_path = Path(args.pred_csv)

    # ğŸ”¥ tag ê²°ì • (ê¸°ë³¸: pred CSV stem)
    tag = args.tag if args.tag is not None else pred_path.stem

    # # ğŸ”¥ output JSON ê²½ë¡œ ê²°ì • (ì„±ëŠ¥ ì§€í‘œìš©)
    # if args.output_json is not None:
    #     out_path = Path(args.output_json)
    # else:
    #     out_path = pred_path.parent / f"metrics_{tag}.json"

    # ğŸ”¥ MSE í†µê³„ JSON ê²½ë¡œ ê²°ì •
    if args.mse_stats_json is not None:
        mse_out_path: Optional[Path] = Path(args.mse_stats_json)
    else:
        mse_out_path = pred_path.parent / f"analyze_mse_dist_{tag}.json"

    print(f"[INFO] GT íŒŒì¼ (attack)          : {attack_path}")
    print(f"[INFO] Pred CSV (model)          : {pred_path}")
    print(f"[INFO] ì‚¬ìš© íƒœê·¸(tag)            : {tag}")
    # print(f"[INFO] ì¶œë ¥ JSON (metrics)       : {out_path}")
    print(f"[INFO] ì¶œë ¥ JSON (MSE stats)     : {mse_out_path}")

    # # âœ… GT: CSV ë˜ëŠ” JSONL ì²˜ë¦¬
    df_gt = load_gt_table(attack_path)

    # # âœ… pattern ê¸°ë°˜ is_anomaly ìƒì„± (ì—†ì„ ë•Œ ìë™ ì²˜ë¦¬)
    # if "is_anomaly" not in df_gt.columns and "pattern" in df_gt.columns:
    #     df_gt["is_anomaly"] = df_gt["pattern"].apply(
    #         lambda x: 1 if str(x).strip().upper() == "ATTACK" else 0
    #     )
    #     print(f"[INFO] 'pattern' ì»¬ëŸ¼ì—ì„œ is_anomaly ìë™ ìƒì„± ì™„ë£Œ ({df_gt['is_anomaly'].sum()}ê°œ ê³µê²© ìœˆë„ìš°)")

    # # âœ… Pred: ì§€ê¸ˆì€ CSVë§Œ ì‚¬ìš©
    df_pred = pd.read_csv(pred_path)

    # # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
    # for col in ["window_index", "is_anomaly"]:
    #     if col not in df_gt.columns:
    #         raise ValueError(f"GT íŒŒì¼ì— '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ({attack_path})")
    #     if col not in df_pred.columns:
    #         raise ValueError(f"Pred CSVì— '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ({pred_path})")

    # pred ìª½ì—ì„œ mse ì»¬ëŸ¼ë„ ê°™ì´ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìœ¼ë©´ AUC/MSE í†µê³„ìš©ìœ¼ë¡œ ì‚¬ìš©
    pred_cols = ["window_index", "is_anomaly"]
    has_mse = "mse" in df_pred.columns
    if has_mse:
        pred_cols.append("mse")

    # # window_index ê¸°ì¤€ inner join
    merged = pd.merge(
        df_gt[["window_index", "is_anomaly"]],
        df_pred[pred_cols],
        on="window_index",
        how="inner",
        suffixes=("_true", "_pred"),
    )

    # print(f"[INFO] join í›„ í–‰ ìˆ˜: {len(merged)}")

    # # íƒ€ì… ì •ë¦¬
    merged["is_anomaly_true"] = merged["is_anomaly_true"].astype(int)
    merged["is_anomaly_pred"] = merged["is_anomaly_pred"].astype(int)

    # # is_anomaly_pred == -1 (ë¯¸ë¼ë²¨) ì œê±° ì˜µì…˜
    # if args.ignore_pred_minus1:
    #     before = len(merged)
    #     merged = merged[merged["is_anomaly_pred"] != -1].copy()
    #     after = len(merged)
    #     print(f"[INFO] is_anomaly_pred == -1 ì œê±°: {before} -> {after}")

    y_true = merged["is_anomaly_true"].to_numpy()
    y_pred = merged["is_anomaly_pred"].to_numpy()

    # # í˜¼ë™í–‰ë ¬ ìš”ì†Œ ê³„ì‚°
    # tp = int(np.sum((y_true == 1) & (y_pred == 1)))
    # tn = int(np.sum((y_true == 0) & (y_pred == 0)))
    # fp = int(np.sum((y_true == 0) & (y_pred == 1)))
    # fn = int(np.sum((y_true == 1) & (y_pred == 0)))

    # total = tp + tn + fp + fn

    # accuracy = safe_div(tp + tn, total)
    # precision = safe_div(tp, tp + fp)
    # recall = safe_div(tp, tp + fn)  # TPR
    # f1 = safe_div(2 * precision * recall, precision + recall)

    # tpr = recall  # same
    # fpr = safe_div(fp, fp + tn)
    # tnr = safe_div(tn, tn + fp)
    # fnr = safe_div(fn, fn + tp)

    # # ì¶”ê°€ ì§€í‘œ: prevalence, predicted positive rate, balanced accuracy
    # prevalence = safe_div(tp + fn, total)          # ì‹¤ì œ ê³µê²© ë¹„ìœ¨
    # pred_positive_rate = safe_div(tp + fp, total)  # ëª¨ë¸ì´ ê³µê²©ì´ë¼ ë•Œë¦° ë¹„ìœ¨
    # balanced_accuracy = 0.5 * (tpr + tnr)

    # # AUC + MSE í†µê³„ ê³„ì‚° (mseê°€ ìˆì„ ë•Œë§Œ ì‹œë„)
    # auc_dict = {"roc_auc": None, "pr_auc": None}
    mse_stats: Optional[Dict[str, Any]] = None

    if has_mse:
        scores = merged["mse"].to_numpy(dtype=float)
    #     auc_dict = try_compute_auc(y_true, scores)
        mse_stats = compute_mse_stats(y_true, scores)

    #     # ì½˜ì†”ì—ë„ ê°„ë‹¨ ìš”ì•½ ì°ì–´ì£¼ê¸°
    #     print("===== MSE Stats by Class =====")
    #     atk = mse_stats["attack"]
    #     nor = mse_stats["normal"]
    #     if atk["count"] > 0:
    #         print(
    #             f"[ATTACK] count={atk['count']}, "
    #             f"mean={atk['mean']:.6f}, std={atk['std']:.6f}, "
    #             f"min={atk['min']:.6f}, max={atk['max']:.6f}"
    #         )
    #     else:
    #         print("[ATTACK] count=0")

    #     if nor["count"] > 0:
    #         print(
    #             f"[NORMAL] count={nor['count']}, "
    #             f"mean={nor['mean']:.6f}, std={nor['std']:.6f}, "
    #             f"min={nor['min']:.6f}, max={nor['max']:.6f}"
    #         )
    #     else:
    #         print("[NORMAL] count=0")
    # else:
    #     print("[INFO] pred CSVì— 'mse' ì»¬ëŸ¼ì´ ì—†ì–´ ROC-AUC / PR-AUC / MSE í†µê³„ ê³„ì‚°ì„ ìƒëµí•©ë‹ˆë‹¤.")

    # metrics: Dict[str, Any] = {
    #     "num_samples": int(total),
    #     "confusion_matrix": {
    #         "TP": tp,
    #         "TN": tn,
    #         "FP": fp,
    #         "FN": fn,
    #     },
    #     "accuracy": accuracy,
    #     "precision": precision,
    #     "recall": recall,
    #     "f1": f1,
    #     "tpr": tpr,
    #     "fpr": fpr,
    #     "tnr": tnr,
    #     "fnr": fnr,
    #     "prevalence": prevalence,
    #     "pred_positive_rate": pred_positive_rate,
    #     "balanced_accuracy": balanced_accuracy,
    #     "roc_auc": auc_dict["roc_auc"],
    #     "pr_auc": auc_dict["pr_auc"],
    # }

    # print("===== Detection Metrics =====")
    # print(f"Samples (windows)       : {total}")
    # print(f"TP={tp}, FP={fp}, FN={fn}, TN={tn}")
    # print(f"Accuracy                : {accuracy:.4f}")
    # print(f"Precision               : {precision:.4f}")
    # print(f"Recall (TPR)            : {recall:.4f}")
    # print(f"F1-score                : {f1:.4f}")
    # print(f"FPR                     : {fpr:.6f}")
    # print(f"TNR (Specificity)       : {tnr:.4f}")
    # print(f"FNR                     : {fnr:.4f}")
    # print(f"Prevalence (Attack rate): {prevalence:.6f}")
    # print(f"Pred Positive Rate      : {pred_positive_rate:.6f}")
    # print(f"Balanced Accuracy       : {balanced_accuracy:.4f}")
    # if metrics["roc_auc"] is not None:
    #     print(f"ROC-AUC                 : {metrics['roc_auc']:.4f}")
    # if metrics["pr_auc"] is not None:
    #     print(f"PR-AUC                  : {metrics['pr_auc']:.4f}")

    # # ë©”ì¸ metrics JSON ì €ì¥
    # out_path.parent.mkdir(parents=True, exist_ok=True)
    # with out_path.open("w", encoding="utf-8") as f:
    #     json.dump(metrics, f, indent=2, ensure_ascii=False)
    # print(f"[INFO] ì„±ëŠ¥ ì§€í‘œ JSON ì €ì¥ ì™„ë£Œ â†’ {out_path}")

    # MSE í†µê³„ JSON ë³„ë„ ì €ì¥
    if mse_stats is not None and mse_out_path is not None:
        mse_out_path.parent.mkdir(parents=True, exist_ok=True)
        with mse_out_path.open("w", encoding="utf-8") as f:
            json.dump(mse_stats, f, indent=2, ensure_ascii=False)
        print(f"[INFO] MSE í†µê³„ JSON ì €ì¥ ì™„ë£Œ â†’ {mse_out_path}")


if __name__ == "__main__":
    main()

"""
python eval_detection_metrics.py \
  --attack-csv ../result/attack_result.csv \
  --pred-csv ../result/benchmark/window_scores.csv \
  --ignore-pred-minus1
"""