#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
00.run_pipeline_pattern.py

ë‹¤ìŒ 4ë‹¨ê³„ë¥¼ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸.

1) 0.attack_result.py
   - attack.jsonl â†’ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° â†’ FC=6 í¬í•¨ ì—¬ë¶€ë¡œ GT ë¼ë²¨ ìƒì„±
   - ì¶œë ¥: ../result/attack_result.csv

2) 1.benchmark.py
   - attack.jsonl â†’ ì „ì²˜ë¦¬ + ìœˆë„ìš° feature + LSTM-AE MSE ê³„ì‚°
   - ì¶œë ¥:
       ../result/benchmark/X_windows.npy
       ../result/benchmark/windows_meta.jsonl
       ../result/benchmark/window_scores.csv

3) 2.eval_detection_metrics.py
   - GT CSV + ì˜ˆì¸¡ CSV â†’ detection metric ê³„ì‚°
   - ì¶œë ¥: ../result/eval_detection_metrics.json

4) 3.analyze_mse_dist.py
   - GT CSV + ì˜ˆì¸¡ CSV â†’ MSE ë¶„í¬/ìš”ì•½ í†µê³„
   - ì¶œë ¥: ../result/analyze_mse_dist.json

ì‚¬ìš©ìê°€ ì§€ì •í•˜ëŠ” ê²ƒ:
  --window-size  (0,1ë‹¨ê³„ ë‘˜ ë‹¤ ë™ì¼í•˜ê²Œ ì‚¬ìš©)
  --step-size    (0,1ë‹¨ê³„ ë‘˜ ë‹¤ ë™ì¼í•˜ê²Œ ì‚¬ìš©)
  --threshold    (1ë‹¨ê³„ benchmarkì—ì„œ MSE thresholdë¡œ ì‚¬ìš©; Noneì´ë©´ threshold.json í™œìš©)

ê²½ë¡œëŠ” ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •ë˜ì–´ ìˆìŒ (ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê¸°ì¤€):
  attack_jsonl      : ../data/attack.jsonl
  attack_result_csv : ../result/attack_result.csv
  benchmark_out_dir : ../result/benchmark
  window_scores_csv : ../result/benchmark/window_scores.csv
  eval_metrics_json : ../result/eval_detection_metrics.json
  analyze_json      : ../result/analyze_mse_dist.json
  pre_dir           : ../../preprocessing/result
  model_dir         : ../data
"""

import argparse
import subprocess
import sys
from pathlib import Path


def run_cmd(cmd, cwd: Path):
    """subprocess.run ë˜í¼ (ì‹¤íŒ¨ ì‹œ ë°”ë¡œ ì¢…ë£Œ)."""
    print("\n[RUN] ", " ".join(cmd))
    result = subprocess.run(cmd, cwd=str(cwd))
    if result.returncode != 0:
        print(f"[ERROR] ëª…ë ¹ ì‹¤íŒ¨ (exit code={result.returncode}): {' '.join(cmd)}")
        sys.exit(result.returncode)


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()

    p.add_argument(
        "--window-size", "-w",
        type=int,
        required=True,
        help="ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í¬ê¸° (ì˜ˆ: 8, 76, 80 ë“±)",
    )
    p.add_argument(
        "--step-size", "-s",
        type=int,
        default=None,
        help="ìŠ¬ë¼ì´ë”© stride (ê¸°ë³¸: window-sizeì™€ ë™ì¼ â†’ non-overlap)",
    )
    p.add_argument(
        "--threshold", "-t",
        type=float,
        default=None,
        help=(
            "benchmark ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  MSE threshold.\n"
            "ì§€ì •í•˜ë©´ 1.benchmark.pyì— --thresholdë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬.\n"
            "ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ threshold.json(í•™ìŠµ ì‹œ ì €ì¥í•œ ê°’)ì„ ì‚¬ìš©í•˜ê±°ë‚˜, "
            "threshold ì—†ëŠ” ìƒíƒœë¡œ window_scores.csv ìƒì„±."
        ),
    )

    return p.parse_args()


def main():
    args = parse_args()

    # ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ (0,1,2,3ë²ˆ ìŠ¤í¬ë¦½íŠ¸ë„ ì—¬ê¸° ìˆë‹¤ê³  ê°€ì •)
    here = Path(__file__).resolve().parent

    # ê³ ì • ê²½ë¡œ (ì—¬ê¸° ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
    attack_jsonl = here / "../data/attack.jsonl"
    attack_result_csv = here / "../result/attack_result.csv"
    pre_dir = here / "../../preprocessing/result"
    benchmark_out_dir = here / "../result/benchmark"
    window_scores_csv = benchmark_out_dir / "window_scores.csv"
    eval_metrics_json = here / "../result/eval_detection_metrics.json"
    analyze_json = here / "../result/analyze_mse_dist.json"
    model_dir = here / "../data"

    window_size = args.window_size
    step_size = args.step_size if args.step_size is not None else window_size
    threshold = args.threshold

    print("========== 00.run_pipeline_pattern.py ==========")
    print(f"[INFO] window_size = {window_size}")
    print(f"[INFO] step_size   = {step_size} (Noneì´ë©´ window_sizeì™€ ë™ì¼)")
    print(f"[INFO] threshold   = {threshold if threshold is not None else 'None (threshold.json ë˜ëŠ” -1 ì‚¬ìš©)'}")
    print("================================================")

    # 1) 0.attack_result.py
    #    attack.jsonl â†’ attack_result.csv (GT ë¼ë²¨)
    cmd0 = [
        sys.executable,
        "0.attack_result.py",
        "--input", str(attack_jsonl),
        "--window-size", str(window_size),
        "--step-size", str(step_size),
        "--output", str(attack_result_csv),
    ]
    run_cmd(cmd0, cwd=here)

    # 2) 1.benchmark.py
    #    attack.jsonl â†’ ìœˆë„ìš° feature + LSTM-AE MSE ê³„ì‚°
    cmd1 = [
        sys.executable,
        "1.benchmark.py",
        "--input", str(attack_jsonl),
        "--pre-dir", str(pre_dir),
        "--window-size", str(window_size),
        "--step-size", str(step_size),
        "--output-dir", str(benchmark_out_dir),
        "--model-dir", str(model_dir),
        "--batch-size", "128",          # ê³ ì • (ìš”ì²­ëŒ€ë¡œ CLIë¡œëŠ” expose ì•ˆ í•¨)
    ]
    if threshold is not None:
        cmd1.extend(["--threshold", str(threshold)])

    run_cmd(cmd1, cwd=here)

    # 3) 2.eval_detection_metrics.py
    #    attack_result.csv + window_scores.csv â†’ detection metrics
    cmd2 = [
        sys.executable,
        "2.eval_detection_metrics.py",
        "--attack-csv", str(attack_result_csv),
        "--pred-csv", str(window_scores_csv),
        "--output-json", str(eval_metrics_json),
        # í•„ìš”í•˜ë©´ --ignore-pred-minus1 ì˜µì…˜ì„ ì—¬ê¸° ì¶”ê°€í•´ì„œ ê³ ì •í•  ìˆ˜ë„ ìˆìŒ
        # "--ignore-pred-minus1",
    ]
    run_cmd(cmd2, cwd=here)

    # 4) 3.analyze_mse_dist.py
    #    attack_result.csv + window_scores.csv â†’ MSE í†µê³„
    cmd3 = [
        sys.executable,
        "3.analyze_mse_dist.py",
        "--attack-csv", str(attack_result_csv),
        "--pred-csv", str(window_scores_csv),
        "--output-json", str(analyze_json),
    ]
    run_cmd(cmd3, cwd=here)

    print("\n[INFO] íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ğŸ‰")
    print(f"  - GT CSV               : {attack_result_csv}")
    print(f"  - Benchmark dir        : {benchmark_out_dir}")
    print(f"  - window_scores.csv    : {window_scores_csv}")
    print(f"  - Detection metrics    : {eval_metrics_json}")
    print(f"  - MSE dist summary     : {analyze_json}")


if __name__ == "__main__":
    main()

"""
python 0.run_pipeline_pattern.py --window-size 8 --step-size 4 --threshold 100

"""