#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
train_lstm_ae_windows_keras.py

Keras/TensorFlow ë²„ì „ì˜ LSTM Autoencoder í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸.

ì…ë ¥:
  - window ë‹¨ìœ„ íŒ¨í„´ feature JSONL (pad_pattern_features_by_index.py ê²°ê³¼)
    ê° ë¼ì¸:
      {
        "window_id": ...,
        "pattern": "...",
        "index": [0, 1, ..., window_size-1],
        "sequence_group": [
          { feature_key1: float, feature_key2: float, ... },
          ...
        ]
      }

ì—­í• :
  - JSONL â†’ (N, T, D) numpy arrayë¡œ ë³€í™˜
  - LSTM Autoencoder í•™ìŠµ (íŒ¨ë”© ê°’ì— ëŒ€í•œ mask ì§€ì›)
  - ëª¨ë¸/ì„¤ì •/feature_key ë¦¬ìŠ¤íŠ¸ ì €ì¥

ì¶œë ¥ (output_dir):
  - model.h5           : í•™ìŠµëœ ëª¨ë¸ (ì „ì²´ Keras ëª¨ë¸)
  - config.json        : í•™ìŠµ ì„¤ì • ë° ë°ì´í„° ì°¨ì› ì •ë³´
  - feature_keys.txt   : feature key ìˆœì„œ (í•œ ì¤„ í•˜ë‚˜)
  - train_log.json     : epochë³„ train/val loss ê¸°ë¡
"""

import os
import json
import argparse
from pathlib import Path
from typing import Any, Dict, List, Tuple
import matplotlib.pyplot as plt
import random

import numpy as np
from tensorflow.keras.callbacks import EarlyStopping


# -------------------------------------------------------
# ê³µí†µ ìœ í‹¸
# -------------------------------------------------------
def compute_window_errors(X_true: np.ndarray,
                          X_pred: np.ndarray,
                          pad_value: float) -> np.ndarray:
    """
    X_true, X_pred: shape (N, T, D)
    pad_value    : íŒ¨ë”© ê°’ (í•´ë‹¹ timestepì€ ë§ˆìŠ¤í¬)

    ë°˜í™˜:
      errors: shape (N,), ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨
    """
    # íŒ¨ë”©ì´ ì•„ë‹Œ timestep ë§ˆìŠ¤í¬ (N, T)
    not_pad = np.any(np.not_equal(X_true, pad_value), axis=-1)
    mask = not_pad.astype(np.float32)

    # íƒ€ì„ìŠ¤í…ë³„ MSE (N, T)
    se = np.mean((X_pred - X_true) ** 2, axis=-1)
    se_masked = se * mask

    denom = np.sum(mask, axis=-1) + 1e-8
    errors = np.sum(se_masked, axis=-1) / denom
    return errors


def set_global_seed(seed: int):
    """Python, NumPy, TensorFlow ì‹œë“œ ê³ ì •"""
    random.seed(seed)
    np.random.seed(seed)

    # TensorFlow seeding (ì§€ì—° import)
    import tensorflow as tf
    tf.random.set_seed(seed)


# -------------------------------------------------------
# JSONL â†’ (N, T, D) ë³€í™˜ + feature ì„ íƒ
# -------------------------------------------------------
def load_windows_to_array(
    jsonl_path: Path,
    exclude_features: List[str] | None = None,
) -> Tuple[np.ndarray, List[str], List[int], List[str]]:
    """
    JSONL íŒŒì¼ â†’ (N, T, D) numpy arrayë¡œ ë³€í™˜

    exclude_features:
      - í•™ìŠµì—ì„œ ì œì™¸í•  feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
      - seq[0].keys() ì¤‘ í•´ë‹¹ ì´ë¦„ì´ ìˆìœ¼ë©´ ì œê±°

    ë°˜í™˜:
      X           : shape (N, T, D), float32
      feature_keys: feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ê¸¸ì´ D, ìˆœì„œ ê³ ì •)
      window_ids  : ê° ìœˆë„ìš°ì˜ window_id ë¦¬ìŠ¤íŠ¸
      patterns    : ê° ìœˆë„ìš°ì˜ pattern ë¦¬ìŠ¤íŠ¸
    """
    X_list: List[np.ndarray] = []
    window_ids: List[int] = []
    patterns: List[str] = []

    feature_keys: List[str] = []

    exclude_set = set(exclude_features) if exclude_features else set()

    with jsonl_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            obj = json.loads(line)

            seq = obj.get("sequence_group", [])
            if not seq:
                continue

            # feature_keysë¥¼ ì²« windowì—ì„œ í•œ ë²ˆë§Œ ê²°ì •
            if not feature_keys:
                all_keys = sorted(list(seq[0].keys()))

                if exclude_set:
                    actually_excluded = sorted(set(all_keys) & exclude_set)
                    if actually_excluded:
                        print(f"[INFO] load_windows_to_array: ì‹¤ì œë¡œ ì œì™¸ë˜ëŠ” feature = {actually_excluded}")
                    not_found = sorted(exclude_set - set(all_keys))
                    if not_found:
                        print(f"[WARN] load_windows_to_array: JSONLì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” feature (ë¬´ì‹œë¨) = {not_found}")

                    feature_keys = [k for k in all_keys if k not in exclude_set]
                    if not feature_keys:
                        raise RuntimeError("âŒ ëª¨ë“  featureê°€ excludeë˜ì–´ ë‚¨ëŠ” featureê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    feature_keys = all_keys

                print(f"[INFO] ìµœì¢… ì‚¬ìš© feature ìˆ˜ = {len(feature_keys)}")
                print(f"[INFO] ì˜ˆì‹œ feature ëª©ë¡ (ì• 10ê°œ): {feature_keys[:10]}")

            T = len(seq)
            D = len(feature_keys)
            arr = np.zeros((T, D), dtype=np.float32)

            for t, pkt in enumerate(seq):
                for d, k in enumerate(feature_keys):
                    arr[t, d] = float(pkt.get(k, 0.0))

            X_list.append(arr)
            window_ids.append(int(obj.get("window_id", -1)))
            patterns.append(str(obj.get("pattern", "")))

    if not X_list:
        raise RuntimeError("âŒ JSONLì—ì„œ ìœ íš¨í•œ windowë¥¼ í•˜ë‚˜ë„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    X = np.stack(X_list, axis=0)  # (N, T, D)
    return X, feature_keys, window_ids, patterns


# -------------------------------------------------------
# ë°ì´í„° ì¸ìŠ¤í™ì…˜ ìœ í‹¸
# -------------------------------------------------------
def inspect_data(
    X: np.ndarray,
    feature_keys: List[str],
    window_ids: List[int],
    patterns: List[str],
    pad_value: float = 0.0,
    n_samples: int = 3,
):
    """
    í•™ìŠµ ì „ì— X / feature / window ëª‡ ê°œë¥¼ ëˆˆìœ¼ë¡œ í™•ì¸í•˜ëŠ” ë””ë²„ê·¸ìš© í•¨ìˆ˜.
    """
    N, T, D = X.shape
    print("\n================= [INSPECT DATA] =================")
    print(f"N (windows) = {N}, T (time steps) = {T}, D (features) = {D}")
    print(f"pad_value = {pad_value}")
    print(f"feature_keys (ì• 10ê°œ): {feature_keys[:10]}")
    print("===================================================\n")

    # ì „ì²´ ë°ì´í„° flatten í•´ì„œ featureë³„ í†µê³„
    X_flat = X.reshape(-1, D)  # (N*T, D)

    print(">>> Feature-wise í†µê³„ (pad_value ì œì™¸):")
    for i, k in enumerate(feature_keys):
        col = X_flat[:, i]
        # pad_valueë¡œë§Œ ê°€ë“í•œ featureë©´ ì œì™¸
        mask = col != pad_value
        if not np.any(mask):
            print(f"  - {k}: (ëª¨ë“  ê°’ì´ pad_value={pad_value})")
            continue
        vals = col[mask]
        print(
            f"  - {k:25s} | "
            f"min={vals.min():.4f}, max={vals.max():.4f}, "
            f"mean={vals.mean():.4f}, std={vals.std():.4f}, "
            f"non_pad_ratio={len(vals)/len(col):.3f}"
        )

    # ëª‡ ê°œ ìœˆë„ìš° ìƒ˜í”Œ ì¶œë ¥
    print("\n>>> ìƒ˜í”Œ ìœˆë„ìš° ëª‡ ê°œ ë³´ê¸°:")
    n_samples = min(n_samples, N)
    for idx in range(n_samples):
        print(f"\n--- Window #{idx} (global index) ---")
        print(f"window_id = {window_ids[idx]}, pattern = {patterns[idx]}")
        # ì• 5 timestepë§Œ
        steps = min(5, T)
        for t in range(steps):
            row = X[idx, t]
            # ì´ timestepì´ íŒ¨ë”©ë§Œ ìˆëŠ”ì§€ ì—¬ë¶€
            if np.all(row == pad_value):
                print(f"  t={t:2d}: [PAD ROW]")
            else:
                # ì• ëª‡ featureë§Œ ë³´ê¸°
                feat_preview_cnt = min(8, D)
                preview = ", ".join(
                    f"{feature_keys[j]}={row[j]:.4f}"
                    for j in range(feat_preview_cnt)
                )
                print(f"  t={t:2d}: {preview}")
    print("\n===================================================\n")


# -------------------------------------------------------
# main
# -------------------------------------------------------

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i", "--input_jsonl",
        required=True,
        help="pad_pattern_features_by_index.py ê²°ê³¼ JSONL ê²½ë¡œ",
    )
    parser.add_argument(
        "-o", "--output_dir",
        required=True,
        help="ëª¨ë¸ ë° ë¡œê·¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=50,
        help="í•™ìŠµ epoch ìˆ˜ (default: 50)",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=64,
        help="batch size (default: 64)",
    )
    parser.add_argument(
        "--hidden_dim",
        type=int,
        default=128,
        help="LSTM hidden dim (default: 128)",
    )
    parser.add_argument(
        "--latent_dim",
        type=int,
        default=64,
        help="latent dim (default: 64)",
    )
    parser.add_argument(
        "--num_layers",
        type=int,
        default=1,
        help="LSTM layer ìˆ˜ (encoderì—ë§Œ ì ìš©, default: 1; í˜„ì¬ëŠ” 1ë§Œ ê¶Œì¥)",
    )
    parser.add_argument(
        "--bidirectional",
        action="store_true",
        help="encoder LSTMì„ bidirectionalë¡œ ì‚¬ìš©í• ì§€ ì—¬ë¶€",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=1e-3,
        help="learning rate (default: 1e-3)",
    )
    parser.add_argument(
        "--val_ratio",
        type=float,
        default=0.2,
        help="validation ë¹„ìœ¨ (default: 0.2)",
    )
    parser.add_argument(
        "--pad_value",
        type=float,
        default=0.0,
        help="íŒ¨ë”© ê°’ (loss ê³„ì‚° ì‹œ maskìš©, default: 0.0)",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="cuda or cpu (Keras/TensorFlowëŠ” ìë™ ì„ íƒ; ì´ ê°’ì€ ë¡œê·¸ìš©)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="ëœë¤ ì‹œë“œ (default: 42)",
    )
    # ğŸ”¥ ê¸°ì¡´: CLIì—ì„œ ì§ì ‘ feature ë‚˜ì—´
    parser.add_argument(
        "--exclude-features",
        nargs="+",
        default=None,
        help=(
            "í•™ìŠµì—ì„œ ì œì™¸í•  feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„). "
            "ì˜ˆ: --exclude-features protocol delta_t modbus_regs_val_std"
        ),
    )
    # ğŸ”¥ ì¶”ê°€: TXT íŒŒì¼ë¡œ feature ì œì™¸ ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬
    parser.add_argument(
        "--exclude-file",
        type=str,
        default=None,
        help=(
            "í•™ìŠµì—ì„œ ì œì™¸í•  feature ì´ë¦„ì„ ì¤„ ë‹¨ìœ„ë¡œ ì ì–´ë‘” txt íŒŒì¼ ê²½ë¡œ. "
            "ë¹ˆ ì¤„ / #ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì€ ë¬´ì‹œë¨. "
            "ì˜ˆ: --exclude-file ../config/exclude_features.txt"
        ),
    )
    # ğŸ‘€ ë°ì´í„°ë§Œ ë³´ê³  ì‹¶ì€ ì˜µì…˜
    parser.add_argument(
        "--inspect-only",
        action="store_true",
        help="ë°ì´í„°ë¥¼ ë¡œë“œ/ìš”ì•½ ì¶œë ¥ë§Œ í•˜ê³  í•™ìŠµì€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ",
    )

    args = parser.parse_args()

    input_path = Path(args.input_jsonl)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    set_global_seed(args.seed)
    print(f"[INFO] Random seed = {args.seed}")

    # TensorFlow / Keras import
    import tensorflow as tf
    from tensorflow.keras import layers, models, optimizers

    print(f"[INFO] TensorFlow version: {tf.__version__}")
    print(f"[INFO] device flag = {args.device} (ì‹¤ì œ ì‚¬ìš© ë””ë°”ì´ìŠ¤ëŠ” TensorFlowê°€ ìë™ ì„ íƒ)")

    # -----------------------------
    # ì œì™¸ feature ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (CLI + TXT í•©ì¹˜ê¸°)
    # -----------------------------
    exclude_from_cli: List[str] = args.exclude_features or []
    exclude_from_file: List[str] = []

    if args.exclude_file:
        excl_path = Path(args.exclude_file)
        if not excl_path.exists():
            print(f"[WARN] exclude-file ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {excl_path}")
        else:
            print(f"[INFO] exclude-file ë¡œë“œ: {excl_path}")
            with excl_path.open("r", encoding="utf-8") as f:
                for line in f:
                    name = line.strip()
                    if not name:
                        continue
                    if name.startswith("#"):
                        continue
                    exclude_from_file.append(name)

    # ë‘ ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê³  ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
    merged_exclude: List[str] = []
    for name in exclude_from_cli + exclude_from_file:
        if name not in merged_exclude:
            merged_exclude.append(name)

    if merged_exclude:
        print(f"[INFO] ìµœì¢… ì œì™¸ feature ëª©ë¡ = {merged_exclude}")
    else:
        print("[INFO] ì œì™¸í•  feature ì—†ìŒ (ì „ì²´ feature ì‚¬ìš©)")

    # 1) ë°ì´í„° ë¡œë“œ
    print(f"[INFO] JSONL ë¡œë“œ: {input_path}")
    X, feature_keys, window_ids, patterns = load_windows_to_array(
        input_path,
        exclude_features=merged_exclude,
    )
    N, T, D = X.shape
    print(f"[INFO] ë°ì´í„° shape: N={N}, T={T}, D={D}")
    print(f"[INFO] ìµœì¢… feature ìˆ˜: {len(feature_keys)}")

    # ğŸ‘€ inspect-only ëª¨ë“œë©´ ì—¬ê¸°ì„œ ë°ì´í„°ë§Œ ë³´ê³  ì¢…ë£Œ
    if args.inspect_only:
        inspect_data(
            X,
            feature_keys,
            window_ids,
            patterns,
            pad_value=float(args.pad_value),
            n_samples=3,
        )
        print("[INFO] --inspect-only í”Œë˜ê·¸ë¡œ ì¸í•´ í•™ìŠµ ì—†ì´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # feature key ìˆœì„œ ì €ì¥
    feat_path = output_dir / "feature_keys.txt"
    with feat_path.open("w", encoding="utf-8") as f:
        for k in feature_keys:
            f.write(k + "\n")
    print(f"[INFO] feature_keys.txt ì €ì¥ â†’ {feat_path}")

    # 2) Train/Val split
    val_ratio = args.val_ratio
    indices = np.arange(N)
    np.random.shuffle(indices)
    split = int(N * (1.0 - val_ratio))
    train_idx = indices[:split]
    val_idx = indices[split:]

    X_train = X[train_idx]
    X_val = X[val_idx]

    print(f"[INFO] train N = {X_train.shape[0]}, val N = {X_val.shape[0]}")

    # 3) LSTM Autoencoder ëª¨ë¸ ì •ì˜ (Keras)
    input_dim = D
    hidden_dim = args.hidden_dim
    latent_dim = args.latent_dim
    bidirectional = args.bidirectional

    # Encoder
    encoder_inputs = layers.Input(shape=(T, input_dim), name="encoder_input")

    if bidirectional:
        # ì–‘ë°©í–¥ LSTM: ì¶œë ¥ ì°¨ì›ì€ hidden_dim * 2
        lstm_layer = layers.Bidirectional(
            layers.LSTM(hidden_dim, return_sequences=False),
            name="encoder_bi_lstm",
        )
        encoder_output = lstm_layer(encoder_inputs)  # (B, hidden_dim * 2)
    else:
        lstm_layer = layers.LSTM(
            hidden_dim, return_sequences=False, name="encoder_lstm"
        )
        encoder_output = lstm_layer(encoder_inputs)  # (B, hidden_dim)

    latent = layers.Dense(latent_dim, name="latent_dense")(encoder_output)  # (B, latent_dim)

    # Decoder: latentë¥¼ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ë°˜ë³µ
    def repeat_latent(x):
        # x: (B, latent_dim)
        x = tf.expand_dims(x, axis=1)  # (B, 1, latent_dim)
        x = tf.tile(x, [1, T, 1])      # (B, T, latent_dim)
        return x

    repeated_latent = layers.Lambda(repeat_latent, name="repeat_latent")(latent)
    decoder_lstm = layers.LSTM(
        hidden_dim,
        return_sequences=True,
        name="decoder_lstm",
    )
    decoder_output = decoder_lstm(repeated_latent)          # (B, T, hidden_dim)
    decoder_dense = layers.TimeDistributed(
        layers.Dense(input_dim), name="decoder_output_dense"
    )
    outputs = decoder_dense(decoder_output)                 # (B, T, D)

    model = models.Model(inputs=encoder_inputs, outputs=outputs, name="lstm_autoencoder")
    model.summary()

    # 4) ì†ì‹¤ í•¨ìˆ˜ (pad_value ë§ˆìŠ¤í‚¹)
    pad_value = float(args.pad_value)

    def make_masked_mse(pad_val: float):
        def masked_mse(y_true, y_pred):
            # y_true, y_pred: (B, T, D)
            # ëª¨ë“  featureê°€ pad_valì¸ timestepì€ ë§ˆìŠ¤í¬ 0
            # (ì›ë³¸ PyTorch êµ¬í˜„: (batch != pad_value).any(dim=-1))
            not_pad = tf.reduceAny(tf.not_equal(y_true, pad_val), axis=-1)  # (B, T) bool
            mask = tf.cast(not_pad, tf.float32)                              # (B, T)

            se = tf.reduceMean(tf.square(y_pred - y_true), axis=-1)        # (B, T)
            se_masked = se * mask

            # epsë¡œ 0 ë‚˜ëˆ„ê¸° ë°©ì§€
            loss = tf.reduceSum(se_masked) / (tf.reduceSum(mask) + 1e-8)
            return loss
        return masked_mse

    # ìœ„ reduceAny / reduceMean / reduceSum ì˜¤íƒ€ ì£¼ì˜:
    import tensorflow as tf  # ì´ë¯¸ ìœ„ì—ì„œ í–ˆì§€ë§Œ ì•ˆì „í•˜ê²Œ
    def make_masked_mse(pad_val: float):
        def masked_mse(y_true, y_pred):
            not_pad = tf.reduce_any(tf.not_equal(y_true, pad_val), axis=-1)  # (B, T) bool
            mask = tf.cast(not_pad, tf.float32)                              # (B, T)

            se = tf.reduce_mean(tf.square(y_pred - y_true), axis=-1)        # (B, T)
            se_masked = se * mask

            loss = tf.reduce_sum(se_masked) / (tf.reduce_sum(mask) + 1e-8)
            return loss
        return masked_mse

    loss_fn = make_masked_mse(pad_value)

    optimizer = optimizers.Adam(learning_rate=args.lr)
    model.compile(optimizer=optimizer, loss=loss_fn)

    # 5) í•™ìŠµ
    es = EarlyStopping(
        monitor="val_loss",
        patience=5,       # 5 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ë©ˆì¶¤
        restore_best_weights=True,
        verbose=1,
    )

    print("[INFO] Keras model.fit() ì‹œì‘")
    history_obj = model.fit(
        X_train,
        X_train,
        validation_data=(X_val, X_val),
        epochs=args.epochs,
        batch_size=args.batch_size,
        shuffle=True,
        callbacks=[es],
        verbose=1,
    )

    history = {
        "train_loss": list(map(float, history_obj.history.get("loss", []))),
        "val_loss": list(map(float, history_obj.history.get("val_loss", []))),
    }

    # 6) train set reconstruction error ê¸°ë°˜ threshold ê³„ì‚°
    print("[INFO] train set reconstruction error ê³„ì‚°...")
    X_train_pred = model.predict(X_train,
                                 batch_size=args.batch_size,
                                 verbose=1)

    errors_train = compute_window_errors(X_train,
                                         X_train_pred,
                                         pad_value)

    print(f"[INFO] train error í†µê³„: "
          f"mean={errors_train.mean():.4f}, "
          f"std={errors_train.std():.4f}, "
          f"min={errors_train.min():.4f}, "
          f"max={errors_train.max():.4f}")

    # ëŒ€í‘œì ì¸ ë‘ ì¢…ë¥˜ threshold
    threshold_p99 = float(np.percentile(errors_train, 99.0))
    threshold_mu3 = float(errors_train.mean() + 3.0 * errors_train.std())

    print(f"[INFO] 99th percentile threshold = {threshold_p99:.4f}")
    print(f"[INFO] mean + 3*std threshold    = {threshold_mu3:.4f}")

    threshold_info = {
        "threshold_p99": threshold_p99,
        "threshold_mu3": threshold_mu3,
        "stats": {
            "mean": float(errors_train.mean()),
            "std": float(errors_train.std()),
            "min": float(errors_train.min()),
            "max": float(errors_train.max()),
        }
    }

    thr_path = output_dir / "threshold.json"
    with thr_path.open("w", encoding="utf-8") as f:
        json.dump(threshold_info, f, indent=2, ensure_ascii=False)
    print(f"[INFO] threshold.json ì €ì¥ â†’ {thr_path}")

    # 7) ëª¨ë¸/ì„¤ì •/ë¡œê·¸ ì €ì¥
    model_path = output_dir / "model.h5"
    model.save(model_path)
    print(f"[INFO] ëª¨ë¸ ì €ì¥ â†’ {model_path}")

    config = {
        "input_jsonl": str(input_path),
        "N": int(N),
        "T": int(T),
        "D": int(D),
        "hidden_dim": hidden_dim,
        "latent_dim": latent_dim,
        "num_layers": args.num_layers,
        "bidirectional": bidirectional,
        "epochs": args.epochs,
        "batch_size": args.batch_size,
        "lr": args.lr,
        "val_ratio": args.val_ratio,
        "pad_value": pad_value,
        "device_flag": args.device,
        "framework": "tensorflow.keras",
        "seed": args.seed,
        "exclude_features": merged_exclude,
    }
    config_path = output_dir / "config.json"
    with config_path.open("w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print(f"[INFO] config ì €ì¥ â†’ {config_path}")

    log_path = output_dir / "train_log.json"
    with log_path.open("w", encoding="utf-8") as f:
        json.dump(history, f, indent=2, ensure_ascii=False)
    print(f"[INFO] train_log ì €ì¥ â†’ {log_path}")

    # 8) loss / val_loss ê³¡ì„  ê·¸ë¦¼ ì €ì¥
    try:
        epochs_range = range(1, len(history["train_loss"]) + 1)

        plt.figure(figsize=(8, 5))
        plt.plot(epochs_range, history["train_loss"], label="train_loss")
        plt.plot(epochs_range, history["val_loss"], label="val_loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("LSTM-AE Training / Validation Loss")
        plt.legend()
        plt.grid(True)

        plot_path = output_dir / "loss_curve.png"
        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()
        print(f"[INFO] loss_curve.png ì €ì¥ â†’ {plot_path}")
    except Exception as e:
        print(f"[WARN] loss ê·¸ë˜í”„ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()


"""
python 2.LSTM_AE.py -i "../result/pattern_features_padded_0.jsonl" -o "../../result_train/data" --epochs 400 --batch_size 64 --hidden_dim 64 --latent_dim 64 --pad_value 0.0 --device cuda --seed 42 --exclude-file "../data/exclude.txt"

inspect ëª¨ë“œ:
python 2.LSTM_AE.py -i "../result/pattern_features_padded_0.jsonl" -o "../../result_train/inspect" --pad_value 0.0 --exclude-file "../data/exclude.txt" --inspect-only
"""
