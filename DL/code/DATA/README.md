# `DATA` 폴더

## 1. 폴더 개요 (Folder Overview)

이 폴더는 데이터 파이프라인의 초기 단계에서 사용되는 스크립트를 포함하고 있습니다. 주요 역할은 분산된 로그 데이터를 하나의 파일로 병합하는 것입니다. `modbus` 및 `xgt_fen` 프로토콜과 같이 여러 패킷으로 나뉘어 수집될 수 있는 로그들을 연속성(동일 타임스탬프 및 시퀀스 번호)을 기준으로 그룹화하고, 관련 필드를 통합하여 단일 JSONL 객체로 만듭니다.

## 2. 파일별 설명 (File Descriptions)

| 파일명 | 설명 |
| --- | --- |
| `data_merge.py` | JSONL 형식의 입력 파일을 읽어 특정 프로토콜(`modbus`, `xgt_fen`)의 연속적인 패킷들을 병합하는 메인 스크립트입니다. 설정 파일(`data_merge.yaml`)에 정의된 작업 목록에 따라 동작합니다. |
| `data_merge.yaml` | `data_merge.py` 스크립트의 설정 파일입니다. 병합할 데이터의 입력 경로와 병합된 결과를 저장할 출력 경로, 인코딩 방식 등을 지정하는 'job' 목록을 정의합니다. |

## 3. 작업 흐름 (Workflow)

1.  **설정 정의**: `data_merge.yaml` 파일에 병합할 데이터의 위치(`input`)와 결과를 저장할 위치(`output`)를 포함한 하나 이상의 작업을 정의합니다.
2.  **스크립트 실행**: `data_merge.py`를 실행합니다.
    ```bash
    python data_merge.py [data_merge.yaml 경로]
    ```
    *   YAML 파일 경로를 지정하지 않으면 스크립트와 동일한 위치에 있는 `data_merge.yaml`을 기본값으로 사용합니다.
3.  **병합 처리**: 스크립트는 설정 파일에 명시된 각 작업에 대해 다음을 수행합니다.
    *   입력 JSONL 파일을 한 줄씩 읽습니다.
    *   `protocol`, `@timestamp`, `sq` 필드를 기준으로 연속적인 패킷들을 식별하고 그룹화합니다.
    *   그룹화된 패킷들을 하나의 JSON 객체로 병합합니다.
    *   병합된 결과를 출력 JSONL 파일에 씁니다.
4.  **결과 확인**: 작업이 완료되면 `data_merge.yaml`에 지정된 `output` 경로에서 병합된 파일을 확인할 수 있습니다.
